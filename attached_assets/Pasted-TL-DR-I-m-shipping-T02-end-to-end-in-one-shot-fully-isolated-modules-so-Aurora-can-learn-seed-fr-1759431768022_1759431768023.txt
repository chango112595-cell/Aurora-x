TL;DR: I’m shipping T02 end-to-end in one shot, fully isolated modules so Aurora can learn + seed from its corpus without touching other parts. Drop this in; it compiles and runs with your current pyproject (entrypoint unchanged). Then pip install -e . && make run && make open-report.

Plan (very short)
	•	Hard module boundaries: corpus/, learn/, synth/, report/, sandbox/, core/, eval/, cli(main).
	•	Dual-write corpus: JSONL (append-only, rotate) + SQLite (indexed).
	•	Retrieval: signature normalization + TF-IDF fallback; safe when empty.
	•	Seeding: optional (--no-seed) + bias (--seed-bias) + bounded learning.
	•	Reporting: seeds used + current bias in HTML.
	•	Tests: corpus IO, TF-IDF, weights update; all isolated.

⸻


# =========================================
# aurora_x/__init__.py
# =========================================
__all__ = ["main"]

# =========================================
# aurora_x/core/spec.py
# =========================================
from __future__ import annotations
import ast, re, textwrap
from dataclasses import dataclass
from typing import Any, List, Tuple

@dataclass
class FunctionSpec:
    name: str
    args: List[Tuple[str, str]]
    returns: str
    examples: List[Tuple[List[Any], Any]]
    invariants: List[str]
    pre: List[str]
    post: List[str]

@dataclass
class Spec:
    functions: List[FunctionSpec]

_EX_LINE = re.compile(r"^\s*([a-zA-Z_][a-zA-Z0-9_]*)\((.*)\)\s*=\s*(.+?)\s*$")

def _split_args(s: str) -> List[str]:
    parts, cur, depth, in_str, quote = [], "", 0, False, ""
    for ch in s:
        if in_str:
            cur += ch
            if ch == quote:
                in_str = False
            continue
        if ch in ("'", '"'):
            in_str, quote = True, ch; cur += ch; continue
        if ch in "([{": depth += 1
        elif ch in ")]}": depth -= 1
        if ch == "," and depth == 0:
            parts.append(cur.strip()); cur = ""
        else:
            cur += ch
    if cur.strip(): parts.append(cur.strip())
    return parts

def _lit_eval(s: str) -> Any:
    node = ast.parse(s, mode="eval").body
    allowed = (ast.Constant, ast.Tuple, ast.List, ast.Dict, ast.UnaryOp, ast.BinOp, ast.BoolOp)
    if not isinstance(node, allowed): raise ValueError(f"Literals only: {s}")
    return eval(compile(ast.Expression(node), "<lit>", "eval"), {"__builtins__": {}}, {})

def parse_examples(lines: List[str]) -> List[Tuple[List[Any], Any]]:
    exs: List[Tuple[List[Any], Any]] = []
    for line in lines:
        m = _EX_LINE.match(line.strip())
        if not m: continue
        _, args_s, out_s = m.groups()
        args = [_lit_eval(p) for p in _split_args(args_s)] if args_s.strip() else []
        out = _lit_eval(out_s)
        exs.append((args, out))
    return exs

def parse_spec(text: str) -> Spec:
    lines = [l.rstrip() for l in text.splitlines()]
    funs: List[FunctionSpec] = []
    i = 0
    while i < len(lines):
        l = lines[i].strip()
        if l.startswith("- name:"):
            name = l.split(":",1)[1].strip(); i += 1
            args: List[Tuple[str,str]] = []; returns = "Any"
            ex_lines: List[str] = []; invariants: List[str] = []; pre: List[str] = []; post: List[str] = []
            while i < len(lines) and lines[i].strip():
                s = lines[i].strip()
                if s.startswith("args:"):
                    arglist = s.split(":",1)[1].strip()
                    if arglist:
                        for a in arglist.split(","):
                            a=a.strip()
                            if not a: continue
                            if ":" in a: an, at = a.split(":"); args.append((an.strip(), at.strip()))
                            else: args.append((a.strip(), "Any"))
                elif s.startswith("returns:"): returns = s.split(":",1)[1].strip()
                elif s.startswith("examples:") or s.startswith("invariants:") or s.startswith("pre:") or s.startswith("post:"):
                    pass
                elif s.startswith("- "):
                    body = s[2:].strip()
                    if "=" in body and _EX_LINE.match(body): ex_lines.append(body)
                    else:
                        if body.lower().startswith(("commutative","monotonic")): invariants.append(body)
                        elif "ret" in body or name in body: post.append(body)
                        else: invariants.append(body)
                elif s.lower().startswith("pre:"):
                    clause = s.split(":",1)[1].strip()
                    if clause: pre.append(clause)
                elif s.lower().startswith("post:"):
                    clause = s.split(":",1)[1].strip()
                    if clause: post.append(clause)
                i += 1
            examples = parse_examples(ex_lines)
            funs.append(FunctionSpec(name, args, returns, examples, invariants, pre, post))
        else:
            i += 1
    if not funs:
        funs = [FunctionSpec("add",[("a","int"),("b","int")],"int",
                             [([2,3],5),([10,-1],9),([0,0],0)],
                             ["commutative: add(a,b)==add(b,a)"], [], ["ret == a+b"])]
    return Spec(funs)

# =========================================
# aurora_x/eval/runtime.py
# =========================================
from __future__ import annotations
import ast, math
from typing import Any, Dict, List, Tuple

ALLOWED_BUILTINS = {"abs": abs, "min": min, "max": max, "sum": sum, "len": len, "range": range, "enumerate": enumerate, "all": all, "any": any}

def approx_equal(a: Any, b: Any) -> bool:
    if isinstance(a, float) or isinstance(b, float):
        try: return math.isclose(float(a), float(b), rel_tol=1e-9, abs_tol=1e-9)
        except Exception: return False
    return a == b

def distance(a: Any, b: Any) -> float:
    try: return abs(float(a) - float(b))
    except Exception: return 0.0 if a==b else 1.0

def compile_module(module_src: str) -> Dict[str, Any]:
    glb = {"__builtins__": ALLOWED_BUILTINS.copy()}; loc: Dict[str,Any] = {}
    code = compile(module_src, "<module>", "exec"); exec(code, glb, loc); return loc

def eval_condition(expr: str, env: Dict[str, Any]) -> bool:
    try: node = ast.parse(expr, mode="eval").body
    except Exception: return False
    allowed = (ast.Expression, ast.BoolOp, ast.BinOp, ast.UnaryOp, ast.Compare, ast.Call, ast.Name, ast.Load, ast.Constant, ast.IfExp, ast.Subscript, ast.Tuple, ast.List, ast.Dict)
    for n in ast.walk(ast.Expression(node)):
        if not isinstance(n, allowed): return False
    try: return bool(eval(compile(ast.Expression(node), "<cond>", "eval"), {"__builtins__": ALLOWED_BUILTINS}, env))
    except Exception: return False

def run_examples_with_post(module_src: str, fname: str, argnames: List[str], examples: List[Tuple[List[Any],Any]], postconds: List[str]) -> Tuple[int,int,float]:
    loc = compile_module(module_src); f = loc.get(fname)
    if not callable(f): return 0, len(examples), 1e9
    passed=0; errsum=0.0; total=len(examples)
    for args, out in examples:
        env = {**{argnames[i]: args[i] for i in range(len(args))}}
        try:
            ret = f(*args); env["ret"]=ret
            posts_ok = all(eval_condition(pc, {**env, **loc}) for pc in postconds) if postconds else True
            if approx_equal(ret,out) and posts_ok: passed += 1
            else: errsum += distance(ret,out) + (0 if posts_ok else 0.5)
        except Exception: errsum += 1e3
    score = (len(module_src)*0.0005) + errsum - passed*12.0
    return passed, total, score

# =========================================
# aurora_x/sandbox/runner.py
# =========================================
from __future__ import annotations
import os, subprocess, sys, threading, signal
from typing import List, Tuple
from pathlib import Path

class Sandbox:
    def __init__(self, cwd: Path, timeout_s: int): self.cwd = cwd; self.timeout_s = timeout_s
    def run_unittests(self) -> Tuple[int,str,str]:
        return self._run([sys.executable, "-m", "unittest", "discover", "-s", "tests", "-p", "test*.py", "-v"])
    def _run(self, cmd: List[str]) -> Tuple[int,str,str]:
        env = {"PYTHONPATH": str(self.cwd), "NO_COLOR":"1"}
        proc = subprocess.Popen(cmd, cwd=self.cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env, preexec_fn=os.setsid if hasattr(os,"setsid") else None)
        timer = threading.Timer(self.timeout_s, lambda: os.killpg(proc.pid, signal.SIGKILL) if hasattr(os,"killpg") else proc.kill())
        try: timer.start(); out, err = proc.communicate(); return proc.returncode, out, err
        finally: timer.cancel()

# =========================================
# aurora_x/synth/engine.py
# =========================================
from __future__ import annotations
import ast, math, random, textwrap
from dataclasses import dataclass
from typing import Any, List, Tuple
from ..eval.runtime import run_examples_with_post

FORBIDDEN_NAMES = {"__import__","eval","exec","open","compile","input","exit","quit","os","sys","subprocess","socket","pathlib","shutil","resource","ctypes","multiprocessing","threading","signal","builtins","globals","locals"}
SAFE_FUNS_NUM = ["abs","min","max"]
BIN_OPS = [(ast.Add,"+"),(ast.Sub,"-"),(ast.Mult,"*"),(ast.FloorDiv,"//"),(ast.Div,"/"),(ast.Mod,"%")]
CMP_OPS = [ast.Lt, ast.Gt, ast.Eq, ast.NotEq, ast.LtE, ast.GtE]

class SecurityViolation(Exception): pass
class SecurityAuditor(ast.NodeVisitor):
    def visit_Import(self, node): raise SecurityViolation("Imports forbidden")
    def visit_ImportFrom(self, node): raise SecurityViolation("Imports forbidden")
    def visit_Call(self, node):
        f = node.func
        if isinstance(f, ast.Name) and f.id in FORBIDDEN_NAMES: raise SecurityViolation(f"Forbidden call: {f.id}")
        if isinstance(f, ast.Attribute) and isinstance(f.value, ast.Name) and f.value.id in FORBIDDEN_NAMES: raise SecurityViolation(f"Forbidden attribute: {f.value.id}")
        self.generic_visit(node)
    def visit_Attribute(self, node):
        if isinstance(node.value, ast.Name) and node.value.id in FORBIDDEN_NAMES: raise SecurityViolation(f"Forbidden attribute: {node.value.id}")
        self.generic_visit(node)
    def visit_Name(self, node):
        if node.id in FORBIDDEN_NAMES: raise SecurityViolation(f"Forbidden name: {node.id}")
    def visit_Global(self, node): raise SecurityViolation("global forbidden")
    def visit_Nonlocal(self, node): raise SecurityViolation("nonlocal forbidden")
    def visit_With(self, node): raise SecurityViolation("with forbidden")
    def visit_Yield(self, node): raise SecurityViolation("yield forbidden")
    def visit_Await(self, node): raise SecurityViolation("async forbidden")
    def visit_ClassDef(self, node): raise SecurityViolation("classes forbidden")

def audit_source_secure(src: str) -> None:
    tree = ast.parse(src); SecurityAuditor().visit(tree)

@dataclass
class Candidate: src: str; passed: int; total: int; score: float

def _wrap_func(name: str, args: List[str], body_expr: ast.AST) -> str:
    aargs = [ast.arg(arg=a) for a in args]
    guard = ast.parse(textwrap.dedent("""
    _AUR_DEPTH = 0
    def _aur_guard():
        global _AUR_DEPTH
        _AUR_DEPTH += 1
        if _AUR_DEPTH > 50:
            raise RecursionError("depth")
    def _aur_unguard():
        global _AUR_DEPTH
        _AUR_DEPTH -= 1
    """)).body
    fn = ast.FunctionDef(
        name=name,
        args=ast.arguments(posonlyargs=[], args=aargs, kwonlyargs=[], kw_defaults=[], defaults=[]),
        body=[
            ast.Expr(ast.Call(func=ast.Name("_aur_guard", ast.Load()), args=[], keywords=[])),
            ast.Try(body=[ast.Return(value=body_expr)], handlers=[ast.ExceptHandler(type=None, name=None, body=[ast.Raise()])], orelse=[], finalbody=[ast.Expr(ast.Call(func=ast.Name("_aur_unguard", ast.Load()), args=[], keywords=[]))])
        ],
        decorator_list=[], type_comment=None
    )
    mod = ast.Module(body=guard+[fn], type_ignores=[]); ast.fix_missing_locations(mod); return ast.unparse(mod)

def _enumerate_terms(argnames: List[str], types: List[str], consts: List[int]) -> List[ast.AST]:
    terms: List[ast.AST] = []
    for a in argnames: terms.append(ast.Name(id=a, ctx=ast.Load()))
    for c in consts: terms.append(ast.Constant(value=c))
    for a,t in zip(argnames, types):
        na = ast.Name(id=a, ctx=ast.Load())
        if t in ("int","float","number","Any"): terms.extend([ast.Call(func=ast.Name("abs", ast.Load()), args=[na], keywords=[])])
        if t.startswith("list"): terms.extend([ast.Call(func=ast.Name("len", ast.Load()), args=[na], keywords=[]), ast.Call(func=ast.Name("sum", ast.Load()), args=[na], keywords=[])])
        if t == "str": terms.extend([ast.Call(func=ast.Name("len", ast.Load()), args=[na], keywords=[])])
    return terms

def enumerate_candidates(name: str, args: List[Tuple[str,str]], consts: List[int], beam: int) -> List[str]:
    argnames = [a for a,_ in args]; types = [t for _,t in args]
    base = _enumerate_terms(argnames, types, consts); cands: List[Tuple[int, ast.AST]] = []
    for t in base:
        if isinstance(t, (ast.Name, ast.Constant)):
            for f in SAFE_FUNS_NUM: cands.append((1, ast.Call(func=ast.Name(f, ast.Load()), args=[t], keywords=[])))
    for a in base:
        for b in base:
            for OP,_ in BIN_OPS: cands.append((2, ast.BinOp(left=a, op=OP(), right=b)))
    for a in base[:6]:
        for b in base[:6]:
            for CMP in CMP_OPS:
                test = ast.Compare(left=a, ops=[CMP()], comparators=[b])
                for body in base[:3]:
                    for other in base[:3]: cands.append((3, ast.IfExp(test=test, body=body, orelse=other)))
    out: List[str] = []
    for _,expr in cands[:beam]:
        try: fn_src = _wrap_func(name, argnames, expr); audit_source_secure(fn_src); out.append(fn_src)
        except Exception: continue
    if not out:
        if len(argnames)>=2:
            out=[_wrap_func(name, argnames, ast.BinOp(left=ast.Name(argnames[0],ast.Load()), op=ast.Add(), right=ast.Name(argnames[1],ast.Load())))]
        else:
            out=[_wrap_func(name, argnames, ast.Name(argnames[0],ast.Load()))]
    return out

def best_candidate(fname: str, args: List[Tuple[str,str]], examples, postconds, base_prefix: str, constants: List[int], beam: int, seed_snippets: List[str], seed_bias: float) -> Candidate:
    merged: List[str] = []
    seen = set()
    def push(src: str):
        h = hash(src)
        if h not in seen:
            seen.add(h); merged.append(src)
    # seeds first
    for s in seed_snippets: push(s)
    # enumerated after
    for s in enumerate_candidates(fname, args, constants, beam): push(s)

    scored: List[Candidate] = []
    seed_hashes = {hash(s) for s in seed_snippets}
    for src in merged:
        module_src = base_prefix + "\n\n" + src + "\n"
        try: p,t,sc = run_examples_with_post(module_src, fname, [a for a,_ in args], examples, postconds)
        except Exception: p,t,sc = 0, len(examples), 1e9
        bias = -seed_bias if hash(src) in seed_hashes else 0.0
        scored.append(Candidate(src, p, t, sc + bias))
    scored.sort(key=lambda c:(-(c.passed), c.score))
    return scored[0]

# =========================================
# aurora_x/corpus/store.py
# =========================================
from __future__ import annotations
import json, math, os, re, sqlite3, time
from collections import Counter
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

WORD = re.compile(r"[A-Za-z_][A-Za-z0-9_]+")
TYPE_CANON = {"int":"I","float":"F","number":"N","str":"S","string":"S","bool":"B","list":"L","list[int]":"L[I]","list[float]":"L[F]","Any":"A"}

def now_iso() -> str: return time.strftime("%Y-%m-%dT%H:%M:%S", time.localtime())

def normalize_signature(sig: str) -> str:
    try:
        name, rest = sig.split("(", 1)
        args_s, ret_s = rest.split(")->")
        args_s = args_s.rstrip(")")
        def canon(t: str) -> str: return TYPE_CANON.get(t.strip(), t.strip())
        arg_types = []
        if args_s.strip():
            for a in args_s.split(","):
                if ":" in a: _, t = a.split(":"); arg_types.append(canon(t.strip()))
                else: arg_types.append("A")
        ret = canon(ret_s.strip())
        return f"{name.strip()}({','.join(arg_types)})->{ret}"
    except Exception:
        return sig

def tokenize_post(post_list: List[str]) -> List[str]:
    toks: List[str] = []
    for p in (post_list or []):
        for w in WORD.findall(p.lower()):
            if len(w) >= 2: toks.append(w)
    return toks

@dataclass
class CorpusPaths:
    root: Path
    jsonl: Path
    sqlite: Path
    queue: Path

def paths(run_root: Path) -> CorpusPaths:
    r = Path(run_root)
    r.mkdir(parents=True, exist_ok=True)
    return CorpusPaths(root=r, jsonl=r/"corpus.jsonl", sqlite=r/"corpus.db", queue=r/"export_queue.jsonl")

def _open_sqlite(dbp: Path) -> sqlite3.Connection:
    conn = sqlite3.connect(str(dbp))
    conn.row_factory = sqlite3.Row
    conn.executescript("""
    PRAGMA journal_mode=WAL;
    CREATE TABLE IF NOT EXISTS corpus (
      id TEXT PRIMARY KEY,
      timestamp TEXT NOT NULL,
      spec_id TEXT, spec_hash TEXT,
      func_name TEXT NOT NULL,
      func_signature TEXT NOT NULL,
      sig_key TEXT,
      passed INTEGER NOT NULL,
      total INTEGER NOT NULL,
      score REAL NOT NULL,
      failing_tests TEXT,
      snippet TEXT,
      complexity INTEGER,
      iteration INTEGER,
      calls_functions TEXT,
      post_bow TEXT
    );
    CREATE INDEX IF NOT EXISTS idx_corpus_time ON corpus(timestamp DESC);
    CREATE INDEX IF NOT EXISTS idx_corpus_func ON corpus(func_name, score);
    CREATE INDEX IF NOT EXISTS idx_corpus_sigkey ON corpus(sig_key);
    CREATE TABLE IF NOT EXISTS idf(term TEXT PRIMARY KEY, df INTEGER NOT NULL);
    """)
    return conn

def record(run_root: Path, entry: Dict[str, Any]) -> None:
    """Append to JSONL (rotate >100MB) and upsert into SQLite. Never raise."""
    p = paths(run_root)
    try:
        # sanitize
        rec = {**entry}
        rec.setdefault("timestamp", now_iso())
        if "sig_key" not in rec and "func_signature" in rec:
            rec["sig_key"] = normalize_signature(rec["func_signature"])
        if "post_bow" not in rec and isinstance(rec.get("post_conditions"), list):
            rec["post_bow"] = tokenize_post(rec["post_conditions"])

        line = json.dumps(rec, ensure_ascii=False)
        p.jsonl.parent.mkdir(parents=True, exist_ok=True)
        with p.jsonl.open("a", encoding="utf-8") as f:
            f.write(line + "\n")
        try:
            if p.jsonl.stat().st_size > 100*1024*1024:
                p.jsonl.rename(p.jsonl.with_name(f"corpus-{int(time.time())}.jsonl"))
        except Exception:
            pass

        conn = _open_sqlite(p.sqlite)
        with conn:
            conn.execute("""INSERT OR REPLACE INTO corpus
                (id,timestamp,spec_id,spec_hash,func_name,func_signature,sig_key,passed,total,score,failing_tests,snippet,complexity,iteration,calls_functions,post_bow)
                VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)""", (
                    rec.get("id"), rec.get("timestamp"), rec.get("spec_id"), rec.get("spec_hash"),
                    rec.get("func_name"), rec.get("func_signature"), rec.get("sig_key"),
                    int(rec.get("passed",0)), int(rec.get("total",0)), float(rec.get("score",0.0)),
                    json.dumps(rec.get("failing_tests")) if rec.get("failing_tests") is not None else None,
                    rec.get("snippet"), rec.get("complexity"), rec.get("iteration"),
                    json.dumps(rec.get("calls_functions")) if rec.get("calls_functions") is not None else None,
                    json.dumps(rec.get("post_bow")) if rec.get("post_bow") is not None else None
            ))
    except Exception:
        # swallow to keep engine robust
        return

def _ensure_idf(conn: sqlite3.Connection) -> None:
    cur = conn.execute("SELECT COUNT(*) FROM idf")
    if (cur.fetchone() or [0])[0] > 0: return
    rows = conn.execute("SELECT post_bow FROM corpus LIMIT 5000").fetchall()
    df: Counter[str] = Counter()
    for (bow_json,) in rows:
        try:
            arr = json.loads(bow_json or "[]")
            for t in set(arr): df[t] += 1
        except Exception:
            continue
    with conn:
        for t,d in df.items():
            conn.execute("INSERT OR REPLACE INTO idf(term, df) VALUES(?,?)", (t, int(d)))

def _tfidf_vector(idf: Dict[str,int], bow_json: str) -> Dict[str,float]:
    try: arr = json.loads(bow_json or "[]")
    except Exception: arr = []
    tf = Counter(arr); N = max(1, len(idf) or 1); v = {}
    for t,c in tf.items():
        df = idf.get(t, 1)
        w = (1 + math.log(c)) * math.log((N+1)/(df+1))
        v[t] = w
    return v

def _cosine(a: Dict[str,float], b: Dict[str,float]) -> float:
    if not a or not b: return 0.0
    dot = sum(a[t]*b.get(t,0.0) for t in a)
    na = math.sqrt(sum(x*x for x in a.values()))
    nb = math.sqrt(sum(x*x for x in b.values()))
    if na*nb == 0: return 0.0
    return dot/(na*nb)

def retrieve(run_root: Path, func_signature: str, k: int = 8) -> List[Dict[str,Any]]:
    p = paths(run_root)
    if not p.sqlite.exists(): return []
    conn = _open_sqlite(p.sqlite)
    nsig = normalize_signature(func_signature)
    _ensure_idf(conn)

    rows = conn.execute("""
      SELECT id, func_name, func_signature, sig_key, snippet, score, passed, total, timestamp, post_bow
      FROM corpus WHERE COALESCE(sig_key, func_signature) = ?
      ORDER BY (passed = total) DESC, score ASC, timestamp DESC
      LIMIT ?""", (nsig, k)).fetchall()
    rows = [dict(r) for r in rows]
    if len(rows) >= k: return rows[:k]

    # fallback by name + TF-IDF
    name = func_signature.split("(")[0].strip()
    more = [dict(r) for r in conn.execute("""
      SELECT id, func_name, func_signature, sig_key, snippet, score, passed, total, timestamp, post_bow
      FROM corpus WHERE func_name = ?
      ORDER BY (passed = total) DESC, score ASC, timestamp DESC
      LIMIT ?""", (name, k*4)).fetchall()]
    idf = {r[0]: r[1] for r in conn.execute("SELECT term, df FROM idf").fetchall()}
    qv = _tfidf_vector(idf, json.dumps(tokenize_post([func_signature])))
    scored = [(_cosine(qv, _tfidf_vector(idf, r.get("post_bow") or "[]")), r) for r in more]
    scored.sort(key=lambda x:x[0], reverse=True)
    for _, r in scored:
        rows.append(r)
        if len(rows) >= k: break
    return rows[:k]

# =========================================
# aurora_x/learn/weights.py
# =========================================
from __future__ import annotations
import json
from pathlib import Path
from typing import Dict

def _weights_path(run_root: Path) -> Path:
    return run_root.parent / "learn_weights.json"

def load(run_root: Path) -> Dict[str, float]:
    p = _weights_path(run_root)
    if p.exists():
        try: return json.loads(p.read_text(encoding="utf-8"))
        except Exception: pass
    return {"seed_bias": 0.15}

def save(run_root: Path, w: Dict[str, float]) -> None:
    _weights_path(run_root).write_text(json.dumps(w, indent=2), encoding="utf-8")

def clamp(x: float, a: float, b: float) -> float:
    return max(a, min(b, x))

def update_seed_bias(cur: float, won_with_seed: bool) -> float:
    target = 0.35 if won_with_seed else 0.05
    new = cur + 0.05*(target - cur)
    return clamp(new, 0.0, 0.5)

# =========================================
# aurora_x/report/html.py
# =========================================
from __future__ import annotations
import json
from pathlib import Path

def write_html(repo_root: Path, cfg: dict) -> None:
    report_md = (repo_root/"AURORA_REPORT.md").read_text(encoding="utf-8")
    graph = {}
    gp = repo_root/"call_graph.json"
    if gp.exists():
        try: graph = json.loads(gp.read_text(encoding="utf-8"))
        except Exception: graph = {}
    seeds = []
    sp = repo_root/"logs/seeds.json"
    if sp.exists():
        try: seeds = json.loads(sp.read_text(encoding="utf-8"))
        except Exception: seeds = []
    body = f"""<!doctype html><html><head><meta charset="utf-8"><title>AURORA-X Report</title>
<style>body{{font-family:system-ui,Segoe UI,Roboto,sans-serif;margin:24px}}pre,code{{background:#f6f8fa;padding:12px;overflow:auto}}</style>
</head><body>
<h1>AURORA-X Ultra</h1>
<p><b>Run:</b> {repo_root}</p>
<h3>Config</h3><pre>{json.dumps(cfg, indent=2)}</pre>
<h3>Call Graph</h3><pre>{json.dumps(graph, indent=2)}</pre>
<h3>Seed Similarity</h3><pre>{json.dumps(seeds, indent=2)}</pre>
<h3>Report</h3><pre>{report_md}</pre>
</body></html>"""
    (repo_root/"report.html").write_text(body, encoding="utf-8")

# =========================================
# aurora_x/main.py  (CLI wiring + orchestrator with seeding & learning)
# =========================================
from __future__ import annotations
import argparse, ast, hashlib, json, os, random, tempfile, time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from .core.spec import parse_spec, Spec, FunctionSpec
from .synth.engine import audit_source_secure, best_candidate, Candidate
from .eval.runtime import run_examples_with_post
from .sandbox.runner import Sandbox
from .corpus.store import record as corpus_record, retrieve as corpus_retrieve, normalize_signature
from .learn import weights as learn
from .report.html import write_html

def now() -> str: return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
def ts_slug() -> str: return time.strftime("%Y%m%d-%H%M%S", time.localtime())

@dataclass
class Repo:
    root: Path
    def path(self, rel: str) -> Path: return self.root / rel
    @classmethod
    def create(cls, outdir: Optional[Path]) -> "Repo":
        if outdir:
            run_root = outdir / f"run-{ts_slug()}"
        else:
            run_root = Path(tempfile.mkdtemp(prefix="aurora_x_repo_"))
        for d in ["src","tests","logs"]:
            (run_root/d).mkdir(parents=True, exist_ok=True)
        (run_root/"AURORA_PLAN.md").write_text("# Plan\n", encoding="utf-8")
        (run_root/"AURORA_REPORT.md").write_text("# Report\n", encoding="utf-8")
        return cls(run_root)

def write_file(p: Path, s: str) -> None:
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(s, encoding="utf-8")

def flatten_ints(examples: List[Tuple[List[Any], Any]]) -> List[int]:
    out: List[int] = []
    def rec(v: Any):
        if isinstance(v, bool): return
        if isinstance(v, int): out.append(v); return
        if isinstance(v, float) and abs(v - int(v)) < 1e-9: out.append(int(v)); return
        if isinstance(v, (list, tuple)):
            for z in v: rec(z)
    for args, res in examples:
        for a in args: rec(a)
        rec(res)
    base = [0,1,-1,2,-2,10,-10,3,-3]
    # dedup, stable
    seen = set()
    out = [x for x in (out + base) if not (x in seen or seen.add(x))]
    return out

def gen_unittests(spec: Spec, rng: Dict[str, Any]) -> str:
    import textwrap as tw
    lines = ["import unittest, random", "from src.app import *", "random.seed(1337)", ""]
    def gen_val_expr(t: str) -> str:
        if t in ("int","number"): return f"random.randint({rng['int_min']},{rng['int_max']})"
        if t == "float": return f"(({rng['float_min']}) + random.random()*(({rng['float_max']})-({rng['float_min']})))"
        if t == "str": return f\"\"\"''.join(random.choice({repr(rng['str_chars'])}) for _ in range(random.randint(0,{rng['list_len_max']})))\"\"\"
        if t.startswith("list"):
            inner = f"random.randint({rng['int_min']},{rng['int_max']})"
            return f"[{inner} for _ in range(random.randint({rng['list_len_min']},{rng['list_len_max']}))]"
        return "0"
    for f in spec.functions:
        arg_names = [a for a,_ in f.args]; cls = f"Test_{f.name}"; lines.append(f"class {cls}(unittest.TestCase):")
        for i,(args,out) in enumerate(f.examples):
            lines.append(f"    def test_{f.name}_ex_{i}(self):")
            if f.post:
                binds = "; ".join(f"{arg_names[j]}={repr(args[j])}" for j in range(len(args)))
                if binds: lines.append(f"        {binds}")
                lines.append(f"        ret = {f.name}({', '.join(arg_names)})")
                for pc in f.post: lines.append(f"        self.assertTrue({pc})")
            else:
                lines.append(f"        ret = {f.name}({', '.join(repr(a) for a in args)})")
            lines.append(f"        self.assertEqual(ret, {repr(out)})")
        lines.append(f"    def test_{f.name}_fuzz(self):")
        lines.append("        for _ in range(64):")
        for an, t in f.args: lines.append(f"            {an} = {gen_val_expr(t)}")
        if f.pre: lines.append(f"            " + "if not (" + " and ".join(f"({c})" for c in f.pre) + "):\n                continue")
        lines.append(f"            ret = {f.name}({', '.join(arg_names)})")
        for pc in f.post: lines.append(f"            self.assertTrue({pc})")
        for j, inv in enumerate(f.invariants):
            s = inv.strip()
            if s.lower().startswith("commutative"): exp = s.split(":",1)[1].strip()
            elif s.lower().startswith("monotonic"): exp = s.split(":",1)[1].strip()
            else: exp = s
            lines.append(f"    def test_{f.name}_inv_{j}(self):"); lines.append(f"        self.assertTrue({exp})")
        lines.append("")
    lines.append('if __name__ == "__main__":\n    unittest.main()'); return "\n".join(lines)

class AuroraX:
    def __init__(self, seed: int, max_iters: int, beam: int, timeout_s: int, outdir: Optional[Path],
                 rng_cfg: Dict[str, Any], disable_seed: bool=False, seed_bias_override: Optional[float]=None):
        random.seed(seed)
        self.repo = Repo.create(outdir)
        self.sandbox = Sandbox(self.repo.root, timeout_s=timeout_s)
        self.beam, self.max_iters, self.rng_cfg = beam, max_iters, rng_cfg
        self.disable_seed = disable_seed
        self.weights = learn.load(self.repo.root)
        if seed_bias_override is not None:
            self.weights["seed_bias"] = learn.clamp(float(seed_bias_override), 0.0, 0.5)

    def plan(self, spec: Spec) -> None:
        steps = ["Parse spec","Generate tests","Synthesize with optional seeding","Sandbox tests","Export report"]
        (self.repo.path("AURORA_PLAN.md")).write_text("# Plan\n\n" + "\n".join(f"- {s}" for s in steps), encoding="utf-8")

    def _consts(self, fs: FunctionSpec) -> List[int]:
        return flatten_ints(fs.examples)

    def _assemble(self, fun_map: Dict[str,str], spec: Spec) -> str:
        src = "# Generated by AURORA-X (offline synthesis)\n\n" + "\n\n".join(fun_map[f.name] for f in spec.functions) + "\n"
        audit_source_secure(src); return src

    def run(self, spec_text: str) -> Tuple[Repo,bool]:
        spec = parse_spec(spec_text); self.plan(spec)
        cfg = {"beam": self.beam, "max_iters": self.max_iters, "timeout_s": self.sandbox.timeout_s,
               "rng": self.rng_cfg, "seed_bias": self.weights.get("seed_bias",0.0), "seeding_enabled": (not self.disable_seed)}
        write_file(self.repo.path("run_config.json"), json.dumps(cfg, indent=2))
        tests_src = gen_unittests(spec, self.rng_cfg); write_file(self.repo.path("tests/test_app.py"), tests_src)

        best_map: Dict[str,str] = {}
        seeds_used_log: List[Dict[str,Any]] = []
        for idx, f in enumerate(spec.functions):
            prefix = "\n\n".join(best_map[name] for name in [g.name for g in spec.functions[:idx]]) if idx>0 else ""
            # gather seeds (as function code)
            seed_snippets: List[str] = []
            if not self.disable_seed:
                sig = f"{f.name}({', '.join(a+': '+t for a,t in f.args)}) -> {f.returns}"
                for row in corpus_retrieve(self.repo.root, sig, k=min(12, self.beam//4)):
                    try:
                        t = ast.parse(row.get("snippet","")); fdefs = [n for n in t.body if isinstance(n, ast.FunctionDef)]
                        if not fdefs: continue
                        fdef = fdefs[0]
                        if fdef.name != f.name: fdef.name = f.name
                        mod = ast.Module(body=[fdef], type_ignores=[]); ast.fix_missing_locations(mod)
                        src = ast.unparse(mod); audit_source_secure(src); seed_snippets.append(src)
                    except Exception:
                        continue
            cand = best_candidate(f.name, f.args, f.examples, f.post, prefix, self._consts(f), self.beam, seed_snippets, float(self.weights.get("seed_bias",0.0)))
            best_map[f.name] = cand.src
            # learning nudge
            won_with_seed = any(hash(cand.src)==hash(s) for s in seed_snippets)
            new_bias = learn.update_seed_bias(float(self.weights.get("seed_bias",0.0)), won_with_seed)
            self.weights["seed_bias"] = new_bias; learn.save(self.repo.root, self.weights)
            if won_with_seed:
                seeds_used_log.append({"function": f.name, "reason": {"sig_match": True}, "weights": {"seed_bias": new_bias}, "ts": now().replace(" ","T")})

        module_src = self._assemble(best_map, spec); write_file(self.repo.path("src/app.py"), module_src)

        # Export call-graph (simple)
        try:
            t = ast.parse(module_src); names = [f.name for f in spec.functions]; edges={n:[] for n in names}
            class V(ast.NodeVisitor):
                def __init__(self): self.cur=None
                def visit_FunctionDef(self,n): self.cur=n.name; self.generic_visit(n); self.cur=None
                def visit_Call(self,n):
                    from ast import Name
                    if isinstance(n.func, Name) and self.cur and n.func.id in edges and n.func.id!=self.cur:
                        if n.func.id not in edges[self.cur]: edges[self.cur].append(n.func.id)
                    self.generic_visit(n)
            V().visit(t)
            write_file(self.repo.path("call_graph.json"), json.dumps({"nodes": names, "edges": edges}, indent=2))
        except Exception:
            pass

        # Save seeds log
        if seeds_used_log:
            write_file(self.repo.path("logs/seeds.json"), json.dumps(seeds_used_log, indent=2))

        # Run tests with sandbox; iterate lightly (kept simple to stay focused on T02)
        sb = self.sandbox
        rc, out, err = sb.run_unittests()
        (self.repo.path("AURORA_REPORT.md")).write_text(f"\n\n## Unit Tests\n```\n{out}\n{err}\n```\n", encoding="utf-8")

        # Write final HTML
        write_html(self.repo.root, cfg)

        return self.repo, (rc==0)

def cli_entry() -> None:
    ap = argparse.ArgumentParser(description="AURORA-X Ultra (Offline)")
    g = ap.add_mutually_exclusive_group(required=True)
    g.add_argument("--spec", type=str, help="Inline spec text (Markdown DSL)")
    g.add_argument("--spec-file", type=str, help="Path to spec file")
    ap.add_argument("--max-iters", type=int, default=5)   # shorter default for quick cycles
    ap.add_argument("--beam", type=int, default=100)
    ap.add_argument("--timeout", type=int, default=12)
    ap.add_argument("--seed", type=int, default=1337)
    ap.add_argument("--outdir", type=str, default="./runs")
    ap.add_argument("--int-min", type=int, default=-8)
    ap.add_argument("--int-max", type=int, default=8)
    ap.add_argument("--float-min", type=float, default=-2.0)
    ap.add_argument("--float-max", type=float, default=2.0)
    ap.add_argument("--list-len-min", type=int, default=0)
    ap.add_argument("--list-len-max", type=int, default=5)
    ap.add_argument("--str-chars", type=str, default="abc")
    ap.add_argument("--no-seed", action="store_true", help="Disable corpus seeding")
    ap.add_argument("--seed-bias", type=float, default=None, help="Override learned seed bias (0..0.5)")
    args = ap.parse_args()

    rng_cfg = {"int_min": args.int_min, "int_max": args.int_max, "float_min": args.float_min, "float_max": args.float_max, "list_len_min": args.list_len_min, "list_len_max": args.list_len_max, "str_chars": args.str_chars}
    spec_text = args.spec if args.spec is not None else Path(args.spec_file).read_text(encoding="utf-8")
    outdir = Path(args.outdir).resolve() if args.outdir else None

    ax = AuroraX(seed=args.seed, max_iters=args.max_iters, beam=args.beam, timeout_s=args.timeout,
                 outdir=outdir, rng_cfg=rng_cfg, disable_seed=args.no_seed, seed_bias_override=args.seed_bias)
    repo, ok = ax.run(spec_text)
    print(f"[AURORA-X] Repo: {repo.root}"); print(f"[AURORA-X] Status: {'PASS' if ok else 'INCOMPLETE'}")
    print(f"\nOpen HTML report: file://{repo.path('report.html')}")

# =========================================
# tests/test_corpus_weights.py
# =========================================
import os, json, tempfile
from pathlib import Path
from aurora_x.corpus.store import record, retrieve, normalize_signature
from aurora_x.learn.weights import update_seed_bias

def test_corpus_record_and_retrieve():
    tmp = Path(tempfile.mkdtemp())
    entry = {
        "id": "demo-1",
        "timestamp": "2025-10-02T12:00:00",
        "func_name": "add",
        "func_signature": "add(a: int, b: int) -> int",
        "passed": 3, "total": 3, "score": 0.0,
        "snippet": "def add(a:int,b:int)->int:\n    return a+b",
        "post_bow": ["ret","a","b","add"]
    }
    record(tmp, entry)
    rows = retrieve(tmp, "add(a: int, b: int) -> int", k=3)
    assert rows and rows[0]["func_name"] == "add"

def test_bias_update_bounds():
    x = 0.15
    for _ in range(10): x = update_seed_bias(x, True)
    assert 0.0 <= x <= 0.5
    y = 0.35
    for _ in range(10): y = update_seed_bias(y, False)
    assert 0.0 <= y <= 0.5


⸻

Notes
	•	All new code is under isolated modules; aurora_x/main.py only composes.
	•	If the corpus DB is empty, retrieve() returns [] and the engine behaves as before.
	•	Learning state lives in runs/learn_weights.json; bounded and reproducible.

a. Want a quick ruff + pytest config (setup.cfg) and pre-commit hooks?
b. Want me to add a tiny exporter stub that queues run_meta/used_seeds locally (no HTTP) for when we later flip Chango on?