TL;DR: Do both.
	•	(a) Function-level regression summary in the report catches real quality drops (not just call-graph changes).
	•	(b) make compare-latest RUN=... is a fast, explicit tool to diff any two runs on demand.

Below are drop-in patches (compilable) to add both.

⸻

Plan (pseudocode)
	•	Parse logs/scores.jsonl → build {function: (passed,total,iter)} maps.
	•	Compute deltas vs runs/latest.
	•	Render Regression Summary in report.html with per-function PASS/FAIL deltas.
	•	Write a scores_diff.json + scores_diff.html.
	•	Add make compare-latest RUN=<other-run> target: regenerates diffs (graph + scores) for arbitrary pairs.

⸻


# =========================================
# PATCH 1/2: aurora_x/main.py  (helpers + integrate into report)
# =========================================
from typing import Any, Dict, List, Tuple
import json, os

# --- add near other small helpers ---
def load_scores_map(run_root: Path) -> Dict[str, Dict[str, Any]]:
    """
    Return {function: {'passed':int,'total':int,'iter':int}} using the latest
    entry per function from logs/scores.jsonl. Missing file => {}.
    """
    p = run_root / "logs" / "scores.jsonl"
    out: Dict[str, Dict[str, Any]] = {}
    if not p.exists():
        return out
    for line in p.read_text(encoding="utf-8").splitlines():
        if not line.strip():
            continue
        try:
            obj = json.loads(line)
            fn = obj.get("function")
            it = int(obj.get("iter", -1))
            if fn is None:
                continue
            prev = out.get(fn)
            if prev is None or it >= prev.get("iter", -1):
                out[fn] = {"passed": int(obj.get("passed", 0)),
                           "total": int(obj.get("total", 0)),
                           "iter": it}
        except Exception:
            continue
    return out

def diff_scores(old: Dict[str, Dict[str, Any]], new: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
    """
    Compute per-function deltas: {fn: {'old': (p,t), 'new': (p,t), 'delta': dp}}
    Only delta in 'passed' is shown; 'total' shown for context.
    """
    keys = sorted(set(old.keys()) | set(new.keys()))
    rows = []
    regressions = 0
    improvements = 0
    for fn in keys:
        o = old.get(fn, {"passed": 0, "total": 0})
        n = new.get(fn, {"passed": 0, "total": 0})
        dp = int(n["passed"]) - int(o["passed"])
        if dp < 0:
            regressions += 1
        elif dp > 0:
            improvements += 1
        rows.append({
            "function": fn,
            "old": [int(o["passed"]), int(o["total"])],
            "new": [int(n["passed"]), int(n["total"])],
            "delta_passed": dp
        })
    return {"summary": {"regressions": regressions, "improvements": improvements, "count": len(keys)},
            "rows": rows}

# --- extend write_html_report(...) to embed score regression + write scores_diff.html/json ---

# locate inside write_html_report after we computed latest status and maybe graph diff.
# Existing variables available: repo, latest_link, is_latest, edges (current graph edges), etc.

    # ----- Scores regression vs latest -----
    scores_html = ""
    try:
        if latest_link.exists():
            latest_scores = load_scores_map(latest_link)
            current_scores = load_scores_map(repo.root)
            sd = diff_scores(latest_scores, current_scores)
            write_file(repo.path("scores_diff.json"), json.dumps(sd, indent=2))

            # Build small HTML table
            def _cell(s: str) -> str: return f"<td style='padding:4px 8px;border-bottom:1px solid #eee'>{s}</td>"
            rows_html = []
            for r in sd["rows"]:
                fn = r["function"]
                o = f"{r['old'][0]}/{r['old'][1]}"
                n = f"{r['new'][0]}/{r['new'][1]}"
                dp = r["delta_passed"]
                color = "#dc2626" if dp < 0 else ("#16a34a" if dp > 0 else "#6b7280")
                rows_html.append(
                    f"<tr>{_cell(fn)}{_cell(o)}{_cell(n)}"
                    f"<td style='padding:4px 8px;border-bottom:1px solid #eee;color:{color};font-weight:600'>{dp:+d}</td></tr>"
                )
            table = (
                "<table style='border-collapse:collapse'>"
                "<thead><tr>"
                "<th style='text-align:left;padding:4px 8px;border-bottom:2px solid #ddd'>Function</th>"
                "<th style='text-align:left;padding:4px 8px;border-bottom:2px solid #ddd'>Old (pass/total)</th>"
                "<th style='text-align:left;padding:4px 8px;border-bottom:2px solid #ddd'>New (pass/total)</th>"
                "<th style='text-align:left;padding:4px 8px;border-bottom:2px solid #ddd'>Δ passed</th>"
                "</tr></thead>"
                f"<tbody>{''.join(rows_html)}</tbody></table>"
            )
            header = sd["summary"]
            summary_bar = (f"<p>Regressions: <b style='color:#dc2626'>{header['regressions']}</b> · "
                           f"Improvements: <b style='color:#16a34a'>{header['improvements']}</b> · "
                           f"Functions: {header['count']}</p>")
            scores_html = f"<h3>Regression Summary (vs latest)</h3>{summary_bar}{table}"

            # a small standalone html too:
            diff_page = f"""<!doctype html><html><head><meta charset="utf-8"><title>Scores Diff</title>
<style>body{{font-family:system-ui;margin:24px}}</style></head><body>
<h2>Function Score Diff vs latest</h2>
{summary_bar}
{table}
</body></html>"""
            write_file(repo.path("scores_diff.html"), diff_page)
    except Exception:
        scores_html = ""

# Then place `{scores_html}` into the final HTML body (e.g., after Call Graph):

    body = f"""<!doctype html><html><head><meta charset="utf-8"><title>AURORA-X Report</title>
<style>
  body{{font-family:system-ui,Segoe UI,Roboto,sans-serif;margin:24px}}
  pre,code{{background:#f6f8fa;padding:12px;overflow:auto}}
  .hdr{{display:flex;align-items:center;gap:12px;margin-bottom:8px}}
  .sub{{color:#555;margin:0 0 6px 0}}
  .meta{{color:#333;margin:0 0 16px 0}}
</style>
</head><body>
<div class="hdr">
  <h1 style="margin:0;">AURORA-X Ultra</h1>
  {latest_badge}
</div>
<p class="sub"><b>Run:</b> {repo.root}</p>
<p class="meta">{ts_html}{dur_html}</p>

<h3>Learning</h3>
<pre>{json.dumps({{"seed_bias": round(seed_bias, 4), "weights_file": str(weights_path)}}, indent=2)}</pre>

<h3>Quick Links</h3>
<p>{links_html}</p>

<h3>Config</h3>
<pre>{json.dumps(cfg, indent=2)}</pre>

<h3>Call Graph</h3>
<pre>{json.dumps(graph, indent=2)}</pre>

{scores_html}

<h3>Report</h3>
<pre>{md}</pre>

</body></html>"""
    write_file(repo.path("report.html"), body)

# =========================================
# PATCH 2/2: Makefile  (compare-latest target)
# =========================================
# Usage:
#   make compare-latest RUN=runs/run-YYYYMMDD-HHMMSS
# Generates graph_diff + scores_diff for RUN (vs runs/latest) and prints paths.

.PHONY: compare-latest
compare-latest:
	@if [ -z "$(RUN)" ]; then echo "Usage: make compare-latest RUN=runs/run-YYYYMMDD-HHMMSS"; exit 2; fi; \
	if [ ! -d "$(RUN)" ]; then echo "Run directory not found: $(RUN)"; exit 3; fi; \
	if [ ! -L "runs/latest" ] && [ ! -d "runs/latest" ]; then echo "No 'runs/latest' symlink or dir."; exit 4; fi; \
	echo "[compare] latest: runs/latest  vs  target: $(RUN)"; \
	python - <<'PY'
import json, sys, os
from pathlib import Path

target = Path(os.environ.get("RUN"))
latest = Path("runs/latest")
def read_json(p):
    try:
        return json.loads(Path(p).read_text(encoding="utf-8"))
    except Exception:
        return {}

def edges_of(g):
    e = g.get("edges", {})
    return {(u, v) for u, vs in e.items() for v in vs}

# graph diff
tg = read_json(target/"call_graph.json")
lg = read_json(latest/"call_graph.json")
added = sorted(list(edges_of(tg) - edges_of(lg)))
removed = sorted(list(edges_of(lg) - edges_of(tg)))
gd = {"added": added, "removed": removed, "old_edges": len(edges_of(lg)), "new_edges": len(edges_of(tg))}
Path(target/"graph_diff.json").write_text(json.dumps(gd, indent=2), encoding="utf-8")
print("[compare] graph diff written:", target/"graph_diff.json")

# scores diff
def load_scores(run):
    p = Path(run)/"logs"/"scores.jsonl"
    out = {}
    if not p.exists():
        return out
    for line in p.read_text(encoding="utf-8").splitlines():
        if not line.strip(): continue
        try:
            o = json.loads(line); fn=o.get("function"); it=int(o.get("iter",-1))
            if fn is None: continue
            prev = out.get(fn)
            if prev is None or it >= prev.get("iter",-1):
                out[fn] = {"passed": int(o.get("passed",0)), "total": int(o.get("total",0)), "iter": it}
        except Exception: pass
    return out

ts = load_scores(target)
ls = load_scores(latest)
allf = sorted(set(ts.keys())|set(ls.keys()))
rows=[]; reg=imp=0
for fn in allf:
    o=ls.get(fn,{"passed":0,"total":0}); n=ts.get(fn,{"passed":0,"total":0})
    dp = int(n["passed"])-int(o["passed"])
    reg += (dp<0); imp += (dp>0)
    rows.append({"function":fn,"old":[o["passed"],o["total"]],"new":[n["passed"],n["total"]],"delta_passed":dp})
sd={"summary":{"regressions":reg,"improvements":imp,"count":len(allf)},"rows":rows}
Path(target/"scores_diff.json").write_text(json.dumps(sd, indent=2), encoding="utf-8")
print("[compare] scores diff written:", target/"scores_diff.json")
PY
	@echo "[compare] open:"
	@echo "  - $(RUN)/graph_diff.json"
	@echo "  - $(RUN)/scores_diff.json"


⸻

Why this helps
	•	Immediate signal when a change hurts correctness (regressions go red).
	•	Repeatable workflow for releases: make compare-latest RUN=... in CI or locally.

⸻

a. Want me to add a tiny red “⚠ regressions” banner on the report header if any regressions > 0?
b. Add --baseline <path> CLI flag to diff the current run against an arbitrary baseline (not just latest) and embed that in the report?