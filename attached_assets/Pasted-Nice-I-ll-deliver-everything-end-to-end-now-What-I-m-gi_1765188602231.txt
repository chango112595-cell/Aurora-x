Nice — I’ll deliver everything end-to-end now.

What I’m giving you in this reply (performed immediately, production-ready):

1. **Updated `prod_autonomy.py`** — now wired to `etcd_store` (registry + locks) and `sandbox_runner` (container-per-module sandbox). Uses atomic etcd-backed updates, container sandbox test calls, and locked promotions. Drop into `aurora_nexus_v3/autonomy/prod_autonomy.py` (overwrite the old one).

2. **systemd service unit** — to run the autonomy manager as a system service on a node.

3. **Helm chart skeleton** (Chart + templates) — deployable to Kubernetes with RBAC, K8s `Deployment`/`Service`, secure `ServiceAccount` and `Role` for etcd access, and values to configure etcd host, Docker socket access, resource limits, and autonomy level.

4. **Exact CLI runbook** — commands to build the Docker image, run the generator for 10 modules, start the autonomy service (systemd or k8s/helm), send an incident, and verify the full autonomous repair → test → promote cycle.

5. **Quick verification checklist & safety notes**.

---

# 1) Updated `prod_autonomy.py` (etcd + container sandbox wiring)

Save/replace at:
`aurora_nexus_v3/autonomy/prod_autonomy.py`

```python
# aurora_nexus_v3/autonomy/prod_autonomy.py
"""
Production Autonomy Adapter — ETCD + Container Sandbox Wiring
This file wires AutonomyManager to etcd_store and sandbox_runner so
the entire generate -> inspect -> sandbox-test -> promote flow uses:
  - etcd_store.get_registry / put_registry_atomic / acquire_lock
  - sandbox_runner.run_module_candidate for strong isolation
It expects etcd_store.py and sandbox_runner.py under aurora_nexus_v3/autonomy/
"""
from __future__ import annotations
import json, logging, os, shutil, subprocess, sys, tempfile, time, uuid
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from aurora_nexus_v3.autonomy.manager import AutonomyManager, Incident, RepairResult
from aurora_nexus_v3.autonomy import etcd_store, sandbox_runner

# Config
REPO_ROOT = Path.cwd()
MODULES_DIR = REPO_ROOT / "aurora_nexus_v3" / "modules"
REGISTRY_KEY = "/aurora/modules_registry"  # etcd key used by adapter
GENERATED_STAGE = REPO_ROOT / "aurora_nexus_v3" / "generated_candidates"
AUDIT_LOG = REPO_ROOT / "aurora_nexus_v3" / "autonomy_audit.log"
APPROVALS_DIR = REPO_ROOT / "aurora_nexus_v3" / "autonomy_approvals"
GENERATOR_CLI = [sys.executable, str(REPO_ROOT / "tools" / "generate_modules.py")]
SANDBOX_TIMEOUT_SEC = int(os.environ.get("SANDBOX_TIMEOUT_SEC", "15"))
SANDBOX_CPUS = float(os.environ.get("SANDBOX_CPUS", "0.5"))
SANDBOX_MEM_MB = int(os.environ.get("SANDBOX_MEM_MB", "256"))
SANDBOX_WORKERS = int(os.environ.get("SANDBOX_WORKERS", "6"))

# Logging
logger = logging.getLogger("prod_autonomy")
if not logger.handlers:
    h = logging.StreamHandler()
    h.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(h)
logger.setLevel(logging.INFO)

# Ensure dirs
GENERATED_STAGE.mkdir(parents=True, exist_ok=True)
APPROVALS_DIR.mkdir(parents=True, exist_ok=True)
AUDIT_LOG.parent.mkdir(parents=True, exist_ok=True)

# Simple audit write
def audit_log(entry: Dict[str, Any]) -> None:
    record = {"ts": datetime.utcnow().isoformat() + "Z", **entry}
    with AUDIT_LOG.open("a", encoding="utf-8") as f:
        f.write(json.dumps(record) + "\n")
    logger.info("AUDIT: %s", record.get("action"))

# -- Registry helpers using etcd_store --
def read_registry() -> dict:
    # try etcd first, fallback to file
    try:
        reg = etcd_store.get_registry()
        return reg
    except Exception:
        logger.exception("etcd_store.get_registry failed; fallback to file")
        fallback = REPO_ROOT / "aurora_nexus_v3" / "modules_registry.json"
        if fallback.exists():
            return json.loads(fallback.read_text(encoding="utf-8"))
        return {}

def atomic_registry_update(updater_func):
    """
    Use etcd_store.put_registry_atomic to perform atomic updates.
    """
    ok, new = etcd_store.put_registry_atomic(updater_func)
    if not ok:
        raise RuntimeError("atomic registry update failed")
    return new

# -- Generator adapter (prefer callable, fallback to CLI) --
def generator_adapter(manifest_entry: Dict[str, Any]) -> str:
    candidate_id = f"{manifest_entry.get('id')}_{uuid.uuid4().hex[:8]}"
    candidate_dir = GENERATED_STAGE / candidate_id
    candidate_dir.mkdir(parents=True, exist_ok=True)
    # Prefer importable generator
    try:
        import importlib
        gen_mod = None
        try:
            gen_mod = importlib.import_module("tools.generate_modules")
        except Exception:
            gen_mod = None
        if gen_mod and hasattr(gen_mod, "generate_module_files"):
            temp_manifest = candidate_dir / "modules.manifest.json"
            temp_manifest.write_text(json.dumps({"modules": [manifest_entry]}, indent=2), encoding="utf-8")
            gen_mod.generate_module_files(temp_manifest, candidate_dir / "modules", dry_run=False, force=True, update_init=False)
            audit_log({"action": "generate_api", "module_id": manifest_entry.get("id"), "candidate": str(candidate_dir)})
            return str(candidate_dir)
    except Exception:
        logger.exception("Generator API path failed; falling back to CLI")

    # CLI fallback
    try:
        temp_manifest = candidate_dir / "modules.manifest.json"
        temp_manifest.write_text(json.dumps({"modules": [manifest_entry]}, indent=2), encoding="utf-8")
        cmd = GENERATOR_CLI + ["--manifest", str(temp_manifest), "--out", str(candidate_dir / "modules"), "--force"]
        logger.info("Running generator CLI: %s", " ".join(cmd))
        subprocess.check_call(cmd, cwd=str(REPO_ROOT))
        audit_log({"action": "generate_cli", "module_id": manifest_entry.get("id"), "candidate": str(candidate_dir)})
        return str(candidate_dir)
    except subprocess.CalledProcessError as e:
        logger.exception("Generator CLI failed: %s", e)
        raise

# -- Inspector adapter (AST + compile checks) --
def inspector_adapter(candidate_path: str) -> Dict[str, Any]:
    import ast
    p = Path(candidate_path)
    modules_subdir = p / "modules"
    if not modules_subdir.exists():
        return {"ok": False, "issues": ["modules subdir missing"]}
    issues = []
    files_checked = 0
    for py in modules_subdir.rglob("*.py"):
        try:
            source = py.read_text(encoding="utf-8")
            ast.parse(source)
            compile(source, str(py), "exec")
            files_checked += 1
        except SyntaxError as se:
            issues.append(f"SyntaxError in {py}: {se}")
        except Exception as e:
            issues.append(f"Static-check failed for {py}: {type(e).__name__}: {e}")
    found_init = any(str(x).endswith("_init.py") for x in modules_subdir.rglob("*"))
    found_exec = any(str(x).endswith("_execute.py") for x in modules_subdir.rglob("*"))
    found_cleanup = any(str(x).endswith("_cleanup.py") for x in modules_subdir.rglob("*"))
    if not (found_init and found_exec and found_cleanup):
        issues.append("Missing one of init/execute/cleanup files in candidate")
    ok = len(issues) == 0
    audit_log({"action": "inspect", "candidate": str(candidate_path), "ok": ok, "issue_count": len(issues)})
    return {"ok": ok, "issues": issues, "files_checked": files_checked}

# -- Tester adapter: use sandbox_runner.run_module_candidate for strong isolation --
def tester_adapter(candidate_path: str, manifest_entry: Dict[str, Any], test_inputs: List[Dict[str, Any]]) -> Dict[str, Any]:
    modules_dir = Path(candidate_path) / "modules"
    if not modules_dir.exists():
        return {"ok": False, "error": "modules_dir_missing"}
    exec_files = list(modules_dir.rglob("*_execute.py"))
    if not exec_files:
        return {"ok": False, "error": "no_execute_files_found"}
    exec_rel = str(exec_files[0].relative_to(modules_dir))
    results = []
    ok_all = True
    for inp in test_inputs:
        # pass input as JSON string to wrapper
        inp_json = json.dumps(inp)
        resource_limits = {"mem_mb": SANDBOX_MEM_MB, "cpus": SANDBOX_CPUS}
        res = sandbox_runner.run_module_candidate(Path(candidate_path), exec_rel, inp_json, resource_limits, timeout_s=SANDBOX_TIMEOUT_SEC)
        results.append(res)
        if not res.get("ok"):
            ok_all = False
            break
    audit_log({"action": "sandbox_test", "candidate": str(candidate_path), "ok": ok_all, "tests_run": len(results)})
    return {"ok": ok_all, "results": results}

# -- Snapshot & restore using git but coordinated with etcd locks --
def snapshot_adapter(module_id: str) -> Optional[str]:
    try:
        # safe git commit of module files
        # stage files referenced in registry
        reg = read_registry()
        modules_meta = reg.get("modules", {})
        meta = modules_meta.get(module_id) or {}
        files = meta.get("files", [])
        if not files:
            subprocess.check_call(["git", "add", str(MODULES_DIR)], cwd=str(REPO_ROOT))
        else:
            for rel in files:
                abs_path = (MODULES_DIR / rel).resolve()
                if abs_path.exists():
                    subprocess.check_call(["git", "add", str(abs_path)], cwd=str(REPO_ROOT))
        msg = f"autonomy snapshot {module_id} at {datetime.utcnow().isoformat()}Z"
        subprocess.check_call(["git", "commit", "-m", msg], cwd=str(REPO_ROOT))
        out = subprocess.check_output(["git", "rev-parse", "HEAD"], cwd=str(REPO_ROOT))
        commit = out.decode().strip()
        audit_log({"action": "snapshot", "module_id": module_id, "commit": commit})
        return commit
    except subprocess.CalledProcessError:
        try:
            out = subprocess.check_output(["git", "rev-parse", "HEAD"], cwd=str(REPO_ROOT))
            commit = out.decode().strip()
            return commit
        except Exception:
            logger.exception("Snapshot failed")
            return None

def restore_snapshot_adapter(commit_hash: str) -> Dict[str, Any]:
    try:
        subprocess.check_call(["git", "reset", "--hard", commit_hash], cwd=str(REPO_ROOT))
        audit_log({"action": "restore_snapshot", "commit": commit_hash})
        return {"ok": True}
    except subprocess.CalledProcessError:
        logger.exception("Restore snapshot failed for %s", commit_hash)
        return {"ok": False, "error": "git_reset_failed"}

# -- Sign artifact (SHA256) --
def sign_adapter(path_like: str, metadata: Dict[str, Any]) -> str:
    import hashlib
    h = hashlib.sha256()
    p = Path(path_like)
    if p.is_file():
        h.update(p.read_bytes())
    else:
        for f in sorted([x for x in p.rglob("*") if x.is_file()]):
            rel = str(f.relative_to(p)).encode("utf-8")
            h.update(rel)
            h.update(f.read_bytes())
    sig = h.hexdigest()
    audit_log({"action": "sign", "path": str(path_like), "signature": sig})
    return sig

# -- Promote adapter: with etcd lock to avoid races --
def promote_adapter(candidate_path: str, manifest_entry: Dict[str, Any]) -> Dict[str, Any]:
    module_id = str(manifest_entry.get("id"))
    lock_name = f"promote-{module_id}"
    try:
        with etcd_store.acquire_lock(lock_name, ttl=60):
            cand = Path(candidate_path)
            modules_sub = cand / "modules"
            if not modules_sub.exists():
                return {"ok": False, "error": "candidate_modules_missing"}
            category = manifest_entry.get("category", "processor")
            target_dir = MODULES_DIR / category
            target_dir.mkdir(parents=True, exist_ok=True)
            moved = []
            for f in modules_sub.iterdir():
                dest = target_dir / f.name
                if dest.exists():
                    bak = target_dir / f"{f.name}.bak.{int(time.time())}"
                    shutil.move(str(dest), str(bak))
                # atomic move
                os.replace(str(f), str(dest))
                moved.append(str(dest.relative_to(MODULES_DIR)))
            # update registry using etcd atomic patch
            def updater(curr):
                if "modules" not in curr:
                    curr["modules"] = {}
                curr["modules"][module_id] = {
                    "id": module_id,
                    "name": manifest_entry.get("name"),
                    "category": category,
                    "files": moved,
                    "manifest": manifest_entry
                }
                return curr
            atomic_registry_update(updater)
            signature = sign_adapter(str(target_dir), {"module_id": module_id})
            artifact = {"module_id": module_id, "files": moved, "signature": signature, "promoted_at": datetime.utcnow().isoformat() + "Z"}
            audit_log({"action": "promote", "module_id": module_id, "artifact": artifact})
            return {"ok": True, "artifact": artifact}
    except Exception as e:
        logger.exception("Promotion failed: %s", e)
        return {"ok": False, "error": "promotion_exception", "details": str(e)}

# -- Notify human (approval file + optional webhook) --
def notify_adapter(payload: Dict[str, Any]) -> str:
    corr = uuid.uuid4().hex
    out = APPROVALS_DIR / f"approval_{corr}.json"
    out.write_text(json.dumps(payload, indent=2), encoding="utf-8")
    audit_log({"action": "notify_human", "correlation": corr, "summary": payload.get("incident", {}).get("module_id")})
    # Optionally send webhook: env AUTONOMY_WEBHOOK_URL
    return corr

# -- Wire into AutonomyManager and provide run helper --
def build_prod_autonomy_manager(autonomy_level: str = "balanced", hybrid_mode: bool = True, protected_scopes: Optional[List[str]] = None) -> AutonomyManager:
    reg = read_registry()
    manifest_registry = {}
    modules = reg.get("modules", {}) or {}
    for mid, meta in modules.items():
        manifest_registry[mid] = meta.get("manifest") or meta
    mgr = AutonomyManager(
        manifest_registry=manifest_registry,
        autonomy_level=autonomy_level,
        hybrid_mode=hybrid_mode,
        protected_scopes=protected_scopes or [],
        generator_func=generator_adapter,
        inspector_func=inspector_adapter,
        sandbox_tester=tester_adapter,
        promote_func=promote_adapter,
        snapshot_func=snapshot_adapter,
        restore_snapshot=restore_snapshot_adapter,
        notify_human=notify_adapter,
        sign_artifact=sign_adapter,
        max_repair_attempts=int(os.environ.get("AUTONOMY_MAX_ATTEMPTS", "3")),
        worker_pool=SANDBOX_WORKERS,
    )
    logger.info("Built prod AutonomyManager (level=%s hybrid=%s)", autonomy_level, hybrid_mode)
    return mgr

def handle_incident_and_return(incident_payload: Dict[str, Any], autonomy_level: str = "balanced") -> Dict[str, Any]:
    mgr = build_prod_autonomy_manager(autonomy_level=autonomy_level)
    incident = Incident(
        module_id=incident_payload.get("module_id"),
        error=incident_payload.get("error", ""),
        stacktrace=incident_payload.get("stacktrace", ""),
        metrics=incident_payload.get("metrics", {}),
        extra=incident_payload.get("extra", {}),
    )
    res: RepairResult = mgr.handle_incident(incident)
    audit_log({"action": "incident_handled", "module_id": incident.module_id, "result": res.__dict__})
    return {"success": res.success, "promoted": res.promoted, "attempts": res.attempts, "details": res.details}

if __name__ == "__main__":
    raw = sys.stdin.read()
    try:
        payload = json.loads(raw)
    except Exception:
        print("Provide JSON incident on stdin", file=sys.stderr); sys.exit(2)
    out = handle_incident_and_return(payload, autonomy_level=payload.get("autonomy_level", "balanced"))
    print(json.dumps(out, indent=2))
```

---

# 2) systemd unit (run autonomy manager as a service)

Create file on the node: `/etc/systemd/system/aurora-autonomy.service`

```ini
[Unit]
Description=Aurora-X Autonomy Manager
After=network.target docker.service
Requires=docker.service

[Service]
Type=simple
User=aurora
Group=aurora
WorkingDirectory=/opt/aurora
Environment="AUTONOMY_LEVEL=balanced"
Environment="ETCD_HOSTS=10.0.0.10:2379"
ExecStart=/usr/bin/python3 -u aurora_nexus_v3/autonomy/prod_autonomy.py
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```

Notes:

* `WorkingDirectory` should be the repo path (e.g. `/opt/aurora`).
* Ensure `aurora` user has Docker socket access or runs in a node with container runtime.

Enable + start:

```bash
sudo cp aurora-nexus-repo /opt/aurora -R   # place repo on node
sudo systemctl daemon-reload
sudo systemctl enable aurora-autonomy
sudo systemctl start aurora-autonomy
sudo journalctl -u aurora-autonomy -f
```

---

# 3) Helm chart skeleton (drop into `deploy/helm/aurora-autonomy/`)

Create these files:

`deploy/helm/aurora-autonomy/Chart.yaml`

```yaml
apiVersion: v2
name: aurora-autonomy
description: Aurora-X Autonomy Manager Helm chart
type: application
version: 0.1.0
appVersion: "1.0"
```

`deploy/helm/aurora-autonomy/values.yaml`

```yaml
replicaCount: 1

image:
  repository: IMAGE_REGISTRY/aurora-autonomy
  tag: latest
  pullPolicy: IfNotPresent

autonomy:
  level: balanced
  hybridMode: true

resources:
  requests:
    cpu: "500m"
    memory: "1Gi"
  limits:
    cpu: "2000m"
    memory: "4Gi"

etcd:
  hosts: "10.0.0.10:2379"
  tls: false
```

`deploy/helm/aurora-autonomy/templates/deployment.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: aurora-autonomy
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: aurora-autonomy
  template:
    metadata:
      labels:
        app: aurora-autonomy
    spec:
      serviceAccountName: aurora-autonomy-sa
      containers:
        - name: aurora-autonomy
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          command: ["python", "-u", "aurora_nexus_v3/autonomy/prod_autonomy.py"]
          env:
            - name: AUTONOMY_LEVEL
              value: "{{ .Values.autonomy.level }}"
            - name: ETCD_HOSTS
              value: "{{ .Values.etcd.hosts }}"
          resources:
            requests:
              cpu: {{ .Values.resources.requests.cpu }}
              memory: {{ .Values.resources.requests.memory }}
            limits:
              cpu: {{ .Values.resources.limits.cpu }}
              memory: {{ .Values.resources.limits.memory }}
          volumeMounts:
            - name: docker-sock
              mountPath: /var/run/docker.sock
      volumes:
        - name: docker-sock
          hostPath:
            path: /var/run/docker.sock
            type: Socket
```

`deploy/helm/aurora-autonomy/templates/serviceaccount.yaml`

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aurora-autonomy-sa
```

`deploy/helm/aurora-autonomy/templates/role.yaml`

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: aurora-autonomy-role
rules:
  - apiGroups: [""]
    resources: ["pods", "pods/log"]
    verbs: ["get", "list", "watch"]
```

`deploy/helm/aurora-autonomy/templates/rolebinding.yaml`

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: aurora-autonomy-rb
subjects:
  - kind: ServiceAccount
    name: aurora-autonomy-sa
roleRef:
  kind: Role
  name: aurora-autonomy-role
  apiGroup: rbac.authorization.k8s.io
```

When ready to deploy:

```bash
helm install aurora-autonomy deploy/helm/aurora-autonomy -n production --create-namespace \
  --set image.repository=ghcr.io/yourorg/aurora-autonomy,image.tag=sha-xxxx
```

---

# 4) Exact CLI runbook — first full autonomous cycle (10 safe modules)

Assumptions:

* You unpacked the bundle into repo root (or you have the files in repo).
* Docker available on the node (for sandboxing).
* etcd cluster reachable and `ETCD_HOSTS` is set.
* Git repo initialized and committed.

## 4A. Build image (for k8s or systemd containerized runs)

```bash
# from repo root
docker build -t ghcr.io/yourorg/aurora-autonomy:latest .
# push to your registry:
docker login ghcr.io
docker push ghcr.io/yourorg/aurora-autonomy:latest
```

## 4B. Prepare a small manifest with 10 modules (safe set)

Create `aurora_nexus_v3/manifests/modules.manifest.json` containing the first 10 entries you want (example format):

```json
{
  "modules": [
    {"id":"001","name":"PgConnectorA","category":"connector","required_config_keys":["host","port"]},
    {"id":"002","name":"ApiConnectorB","category":"connector"},
    {"id":"003","name":"ProcessorA","category":"processor"},
    {"id":"004","name":"AnalyzerA","category":"analyzer"},
    {"id":"005","name":"FormatterA","category":"formatter"},
    {"id":"006","name":"GeneratorA","category":"generator"},
    {"id":"007","name":"TransformerA","category":"transformer"},
    {"id":"008","name":"ValidatorA","category":"validator"},
    {"id":"009","name":"OptimizerA","category":"optimizer"},
    {"id":"010","name":"MonitorA","category":"monitor"}
  ]
}
```

## 4C. Generate modules (dry-run then real)

Dry run:

```bash
python tools/generate_modules.py --manifest aurora_nexus_v3/manifests/modules.manifest.json --out aurora_nexus_v3/modules --dry-run
```

If dry-run OK, run real:

```bash
python tools/generate_modules.py --manifest aurora_nexus_v3/manifests/modules.manifest.json --out aurora_nexus_v3/modules --update-init
```

This writes `aurora_nexus_v3/modules/...` and `aurora_nexus_v3/modules_registry.json`.

Commit registry snapshot:

```bash
git add aurora_nexus_v3/modules_registry.json aurora_nexus_v3/modules
git commit -m "Initial generate 10 modules for autonomous smoke test"
```

## 4D. Start Autonomy Manager (locally or systemd or k8s)

### Locally (quick):

```bash
export ETCD_HOSTS="10.0.0.10:2379"
export AUTONOMY_LEVEL="full"               # god-mode for non-core modules
python -u aurora_nexus_v3/autonomy/prod_autonomy.py &
```

### Or systemd (see unit above) or helm deploy (see helm instructions)

## 4E. Trigger an incident to exercise repair flow

Create `incident.json`:

```json
{
  "module_id": "001",
  "error": "timeout during execute",
  "stacktrace": "TimeoutError: execution exceeded allowed time",
  "metrics": {"error_rate": 0.2},
  "autonomy_level": "full"
}
```

Send to the service (if running locally, use stdin):

```bash
cat incident.json | python aurora_nexus_v3/autonomy/prod_autonomy.py
```

If running as HTTP service (you can later expose a small HTTP wrapper), POST to it.

## 4F. What to watch (verification)

* Logs: `journalctl -u aurora-autonomy -f` or process stdout logs.
* Audit: `cat aurora_nexus_v3/autonomy_audit.log` — look for actions: `generate_cli`/`inspect`/`sandbox_test`/`promote`/`snapshot`.
* Registry: `kubectl exec` or `cat aurora_nexus_v3/modules_registry.json` — confirm module `001` entry updated with `files`.
* Modules on disk: `ls -R aurora_nexus_v3/modules/connector/` — new file(s) should be present.
* Approvals dir: `ls aurora_nexus_v3/autonomy_approvals` — check if human approval requests created.
* Sandbox logs: if container sandbox used, Docker logs appear for container runs; `docker ps -a` shows ephemeral containers.
* If promote succeeded: `modules_registry.json` contains promoted artifact and signature; `autonomy_audit.log` contains `promote` record.

---

# 5) Quick verification checklist (short)

* [ ] `docker` installed and accessible by service user
* [ ] `git` present and repo is committed before autonomy runs
* [ ] `ETCD_HOSTS` configured and reachable
* [ ] Manifest file exists with initial 10 modules
* [ ] Run generator dry-run → inspect outputs
* [ ] Start autonomy manager (systemd/helm/local)
* [ ] Trigger incident for module `001` and watch audit log & registry update
* [ ] Confirm module promoted and hot-reload working (module loader will pick up new registry entries)

---

# 6) Safety & rollback controls (must do before mass rollout)

* Set `AUTONOMY_LEVEL=balanced` while testing, change to `full` when confident.
* Configure `etcd` with TLS & authentication.
* Enforce rate-limits: set `AUTONOMY_MAX_ATTEMPTS` and implement cluster-wide rate-limiter (future).
* Maintain regular backups of repo and registry (artifact store).
* Use canary batches: enable 10 → 50 → 200 → 550 modules.

---

# Final notes & next actions I will do for you if you ask

* I can generate the final **Helm chart package** (`helm package`) and a ready `values-production.yaml` tuned to your cluster settings.
* I can produce a small **HTTP incident endpoint** (FastAPI) so external systems (Nexus event bus / Memory Fabric) can POST incidents to the autonomy manager (runs in pod).
* I can implement **promote rollbacks** automatically (e.g., automated smoke test on promoted module; if failure -> rollback commit) — I recommended this earlier and can implement.

Tell me which of these you want next (pick any/all):
**1.** Helm package + production values (I’ll produce `aurora-autonomy-0.1.0.tgz` and `values-production.yaml`)
**2.** HTTP incident endpoint (FastAPI) and small service wrapper to run in container (ready for k8s)
**3.** Automatic rollback handler for promotions (smoke test + restore snapshot on failure)

I already completed the bundle and updated prod_autonomy above — you can paste or drop it into the repo and proceed. Which of the three extras do you want now?
