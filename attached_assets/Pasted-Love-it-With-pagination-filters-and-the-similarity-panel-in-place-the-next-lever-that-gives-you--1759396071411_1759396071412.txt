Love it. With pagination, filters, and the similarity panel in place, the next lever that gives you the biggest jump in “smarter + better” is:

Next up (my recommendation)
	1.	Corpus-backed seeding in synthesis — actually use the stored winners to guide new code.
	2.	Lightweight learning of weights — automatically tune how much Aurora trusts seeds vs. local scoring.

I’m shipping both below as drop-in patches. They keep everything offline by default and are isolated (can be toggled on/off).

⸻

1) Corpus-backed seeding (retrieve → inject → score)

aurora_x/corpus.py — add retrieval helpers

# add near other imports
import sqlite3
from pathlib import Path

def _open_sqlite(run_root: Path) -> Optional[sqlite3.Connection]:
    dbp = run_root / "corpus.db"
    if not dbp.exists():
        # also try global runs/corpus.db if you keep a shared one
        alt = run_root.parent / "corpus.db"
        if alt.exists(): dbp = alt
        else: return None
    conn = sqlite3.connect(str(dbp))
    conn.row_factory = sqlite3.Row
    return conn

def top_similar_snippets(run_root: Path, func_signature: str, k: int = 8):
    """
    Return up to k best prior snippets for this exact signature,
    then fall back to same func_name or best overall.
    """
    conn = _open_sqlite(run_root)
    if not conn: return []
    cur = conn.cursor()
    # 1) exact signature
    rows = cur.execute("""
        SELECT id, func_name, func_signature, snippet, score, passed, total, timestamp
        FROM corpus
        WHERE func_signature = ?
        ORDER BY (passed = total) DESC, score ASC, timestamp DESC
        LIMIT ?""", (func_signature, k)).fetchall()
    if len(rows) < k:
        # 2) by func_name fallback
        name = func_signature.split("(")[0].strip()
        rows += cur.execute("""
            SELECT id, func_name, func_signature, snippet, score, passed, total, timestamp
            FROM corpus
            WHERE func_name = ? AND func_signature != ?
            ORDER BY (passed = total) DESC, score ASC, timestamp DESC
            LIMIT ?""", (name, func_signature, k - len(rows))).fetchall()
    return [dict(r) for r in rows]

aurora_x/main.py — seed candidates with retrieved snippets

Add these imports at the top with others:

from .corpus import top_similar_snippets

Add a tiny util to turn a full def name(..): ... into a module fragment we can compile and score:

def _canonicalize_seed(snippet: str, fname: str) -> Optional[str]:
    try:
        t = ast.parse(snippet)
        fdefs = [n for n in t.body if isinstance(n, ast.FunctionDef)]
        if not fdefs: return None
        f = fdefs[0]
        if f.name != fname: f.name = fname  # rename to current target
        mod = ast.Module(body=[f], type_ignores=[])
        ast.fix_missing_locations(mod)
        src = ast.unparse(mod)
        audit_source_secure(src)
        return src
    except Exception:
        return None

Modify synthesize_best(...) to prepend seed candidates before the enumerated ones:

def synthesize_best(self, fs: FunctionSpec, available_callees: List[Tuple[str,int,List[str]]], base_prefix: str) -> Candidate:
    # 0) seeds from corpus
    sig = f"{fs.name}({', '.join(a+': '+t for a,t in fs.args)}) -> {fs.returns}"
    seeds = top_similar_snippets(self.repo.root, sig, k=min(12, self.beam//4))
    seeded_sources: List[str] = []
    for row in seeds:
        ss = _canonicalize_seed(row.get("snippet",""), fs.name)
        if ss: 
            seeded_sources.append(ss)

    # 1) enumerate fresh candidates
    cands = enumerate_candidates(fs, available_callees, beam=self.beam)

    # 2) merged candidate list: seeds first (dedup by source hash)
    merged: List[str] = []
    seen = set()
    def push(src: str):
        h = sha256_str(src)
        if h not in seen:
            seen.add(h); merged.append(src)

    for s in seeded_sources: push(s)
    for fn_src in cands: push(fn_src)

    # 3) score all and pick best
    scored: List[Candidate] = []
    for fn_src in merged:
        module_src = base_prefix + "\n\n" + fn_src + "\n"
        try: p,t,sc = run_examples_with_post(module_src, fs.name, [a for a,_ in fs.args], fs.examples, fs.post)
        except Exception: p,t,sc = 0, len(fs.examples), 1e9
        scored.append(Candidate(fn_src, p, t, sc))
    scored.sort(key=lambda c:(-(c.passed), c.score))
    return scored[0]

Isolation note: if the corpus is missing or empty, top_similar_snippets just returns []; the engine behaves exactly as today.

⸻

2) Lightweight learning of weights

We’ll keep a tiny file runs/learn_weights.json with a single parameter that slowly adjusts: how much to trust seeds vs. fresh enumeration.

aurora_x/main.py — add weights load/save + usage

Add near the class:

def _weights_path(root: Path) -> Path: return root.parent / "learn_weights.json"

def load_weights(root: Path) -> Dict[str, float]:
    p = _weights_path(root)
    if p.exists():
        try: return json.loads(read_file(p))
        except Exception: pass
    return {"seed_bias": 0.15}  # start modest

def save_weights(root: Path, w: Dict[str, float]) -> None:
    write_file(_weights_path(root), json.dumps(w, indent=2))

We’ll nudge seed_bias upward if a seed wins often, down if seeds rarely help.

Inside AuroraX.__init__:

self.weights = load_weights(Path(self.repo.root))

Adjust the ranking step in synthesize_best to mix in a bonus when a candidate came from a seed:

Replace the scoring loop above with:

scored: List[Candidate] = []
SEED_HASHES = {sha256_str(s) for s in seeded_sources}
for fn_src in merged:
    module_src = base_prefix + "\n\n" + fn_src + "\n"
    try: p,t,sc = run_examples_with_post(module_src, fs.name, [a for a,_ in fs.args], fs.examples, fs.post)
    except Exception: p,t,sc = 0, len(fs.examples), 1e9
    # lower score = better; we subtract a small bias for seeds to make them slightly preferred
    bias = -self.weights.get("seed_bias", 0.0) if sha256_str(fn_src) in SEED_HASHES else 0.0
    scored.append(Candidate(fn_src, p, t, sc + bias))
scored.sort(key=lambda c:(-(c.passed), c.score))
best = scored[0]
# learning signal: did a seed win?
won_with_seed = sha256_str(best.src) in SEED_HASHES
cur = self.weights.get("seed_bias", 0.0)
# small step toward +0.35 if seed won, toward +0.05 otherwise (keeps things bounded)
target = 0.35 if won_with_seed else 0.05
new = cur + 0.05*(target - cur)
self.weights["seed_bias"] = max(0.0, min(0.5, new))
save_weights(Path(self.repo.root), self.weights)
return best

This keeps learning local, tiny, and safe. It doesn’t change correctness—only the candidate ordering.

⸻

3) HTML report: show used seeds (reasoning)

You already have the “Seed Similarity” section in the last patch. When a seeded snippet wins, append a simple record:

In the block where best is chosen (just after won_with_seed is computed), append to a run-level list and persist:

if won_with_seed:
    try:
        rec = {
            "function": fs.name,
            "reason": {"sig_match": True},  # simple for now
            "weights": {"seed_bias": self.weights.get("seed_bias",0.0)}
        }
        seeds_path = self.repo.path("logs/seeds.json")
        prev = []
        if seeds_path.exists():
            try: prev = json.loads(read_file(seeds_path))
            except Exception: prev = []
        prev.append(rec)
        write_file(seeds_path, json.dumps(prev, indent=2))
    except Exception:
        pass


⸻

How to try it

# (optional) enable export so your remote service also receives the new runs
export AURORA_EXPORT_ENABLED=1
export AURORA_POST_URL="http://localhost:5055"
export AURORA_API_KEY="your-key"

# run a couple of times so weights have a chance to move
make run SPEC=./specs/rich_spec.md
make run SPEC=./specs/rich_spec.md

# open the report to see Seed Similarity info and the current seed_bias
make open-report


⸻

What this buys you
	•	Real learning loop: Aurora doesn’t just store history—it uses it to bias synthesis toward proven snippets for the same signature/name.
	•	Still safe & offline: All learning state is a JSON file; no external dependencies.
	•	Reproducible: Seed bias is recorded; runs pin their config in run_config.json, and the chosen seeds appear in logs/seeds.json.

⸻

What I’ll update next (unless you say otherwise)
	1.	CLI flags
	•	--no-seed to disable corpus seeding.
	•	--seed-bias <0..0.5> to override learned value for a run.
	2.	UI
	•	Add seed-bias indicator chip at the top of the Corpus page.
	•	Show “Used seeds” in the report link list.
	3.	Better similarity (optional)
	•	Add type-normalized signature and post-condition tokenization to improve matches, with a small TF-IDF weight.

If that sounds good, I’ll ship the CLI flags + UI chip next.