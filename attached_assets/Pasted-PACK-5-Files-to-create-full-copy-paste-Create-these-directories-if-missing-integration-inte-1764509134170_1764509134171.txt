PACK 5 — Files to create (full copy/paste).

Create these directories if missing:

integration/
integration/dashboard/
integration/marketplace/
integration/updater/
integration/auto_deploy/
integration/ci/
scripts/
packaging/


I list each file path followed by its full content. Paste exactly.

1) integration/updater/updater_service.py

(centralized updater service: staging, verify signatures, rollout strategies, human approval queue)

Path: integration/updater/updater_service.py

#!/usr/bin/env python3
"""
Aurora Updater Service

Responsibilities:
- Accept update artifacts (tar.gz) into staging
- Verify signatures (GPG) and checksums
- Stage updates into per-target staging directories
- Provide 'promote' operation that performs atomic swap (backup + move)
- Support rollout strategies: immediate, staged (percentage), canary by device tags
- Human approval gating: suggestions are stored until operator confirms
- Works offline (local staging) and cloud-assisted (pull from configured builder)

Usage (HTTP + CLI wrappers provided below).
"""
import os, sys, tarfile, shutil, hashlib, subprocess, json, time
from pathlib import Path
from http.server import BaseHTTPRequestHandler, HTTPServer
import threading

ROOT = Path(__file__).resolve().parents[2]
STAGING = ROOT / ".aurora_updates" / "staging"
BACKUP = ROOT / ".aurora_updates" / "backup"
SUGGESTIONS = ROOT / "integration" / "updater" / "suggestions"
STAGING.mkdir(parents=True, exist_ok=True)
BACKUP.mkdir(parents=True, exist_ok=True)
SUGGESTIONS.mkdir(parents=True, exist_ok=True)

def sha256_of(path: Path):
    h = hashlib.sha256()
    with open(path,"rb") as f:
        for chunk in iter(lambda: f.read(65536), b""):
            h.update(chunk)
    return h.hexdigest()

def verify_signature(artifact: Path):
    asc = artifact.with_suffix(artifact.suffix + ".asc")
    if not asc.exists():
        return False, "signature missing"
    # gpg --verify
    try:
        subprocess.check_call(["gpg","--verify", str(asc), str(artifact)])
        return True, "ok"
    except subprocess.CalledProcessError as e:
        return False, f"gpg verify failed: {e}"

def stage_artifact(artifact: Path):
    # copy to staging with hash name
    h = sha256_of(artifact)
    dest = STAGING / h
    dest.mkdir(parents=True, exist_ok=True)
    shutil.copy2(artifact, dest / artifact.name)
    # copy signature if present
    asc = artifact.with_suffix(artifact.suffix + ".asc")
    if asc.exists():
        shutil.copy2(asc, dest / asc.name)
    # write metadata
    meta = {"artifact": artifact.name, "hash": h, "ts": time.time()}
    (dest / "meta.json").write_text(json.dumps(meta))
    return dest

def activate_staged(hashid: str, target_root: Path, create_backup=True):
    staged = STAGING / hashid
    if not staged.exists(): raise FileNotFoundError("staging not found")
    if create_backup:
        bdir = BACKUP / str(int(time.time()))
        shutil.copytree(target_root, bdir)
    # very conservative apply: copy staged files over target (atomic swap needs fs-level support)
    for item in staged.iterdir():
        if item.name == "meta.json": continue
        src = item
        dest = target_root / item.name
        if dest.exists():
            if dest.is_dir(): shutil.rmtree(dest)
            else: dest.unlink()
        if src.is_dir():
            shutil.copytree(src, dest)
        else:
            shutil.copy2(src, dest)
    return True

# Minimal HTTP API for operator and CI to upload / stage / list / promote
class SimpleHandler(BaseHTTPRequestHandler):
    def _send(self, code, body, ctype="application/json"):
        self.send_response(code)
        self.send_header("Content-Type", ctype)
        self.end_headers()
        if isinstance(body, (dict, list)):
            self.wfile.write(json.dumps(body, indent=2).encode())
        else:
            self.wfile.write(str(body).encode())

    def do_GET(self):
        if self.path.startswith("/list"):
            items = [d.name for d in STAGING.iterdir() if d.is_dir()]
            return self._send(200, {"staged": items})
        if self.path.startswith("/suggestions"):
            items = [p.name for p in SUGGESTIONS.iterdir()]
            return self._send(200, {"suggestions": items})
        return self._send(404, {"error":"not found"})

    def do_POST(self):
        # endpoints: /upload (multipart not supported here: use CLI), /promote?hash=..., /approve?file=...
        if self.path.startswith("/promote"):
            qs = self.path.split("?",1)[1] if "?" in self.path else ""
            params = dict([kv.split("=",1) for kv in qs.split("&") if "=" in kv])
            h = params.get("hash")
            target = params.get("target",".")
            if not h:
                return self._send(400, {"error":"hash required"})
            try:
                activate_staged(h, Path(target))
                return self._send(200, {"ok":True})
            except Exception as e:
                return self._send(500, {"error": str(e)})
        if self.path.startswith("/approve"):
            length = int(self.headers.get("Content-Length", "0"))
            body = self.rfile.read(length).decode() if length else "{}"
            data = json.loads(body or "{}")
            fname = data.get("file")
            # mark suggestion as approved and return signed token placeholder
            if not fname:
                return self._send(400, {"error":"file required"})
            f = SUGGESTIONS / fname
            if not f.exists():
                return self._send(404, {"error":"not found"})
            # in production: operator signs suggestion and token is returned
            out = {"ok": True, "applied": fname, "signed_token": "operator-signed-placeholder"}
            (SUGGESTIONS / (fname + ".approved")).write_text(json.dumps({"ts": time.time(), "op":"approved"}))
            return self._send(200, out)
        return self._send(404, {"error":"not supported"})

def run_server(host="0.0.0.0", port=9710):
    srv = HTTPServer((host, port), SimpleHandler)
    print("Updater service listening on", port)
    srv.serve_forever()

if __name__=="__main__":
    run_server()

2) integration/dashboard/backend.py

(FastAPI backend that wires agents, memory, updater, marketplace; serves operator UI)

Path: `integration/dashboard/backend.py**

#!/usr/bin/env python3
"""
Operator Dashboard Backend
- serves the operator UI
- exposes restful endpoints for:
  - operators: list suggestions, approve them
  - updater: list staged artifacts, promote
  - marketplace: search plugins, install/uninstall
  - audit logs
This backend is *local-first* and can be run on Aurora Core machine.
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from pathlib import Path
import shutil, os, json, time, subprocess

ROOT = Path(__file__).resolve().parents[2]
UI_DIR = ROOT / "integration" / "dashboard" / "static"
UPDATER_DIR = ROOT / "integration" / "updater"
MARKET = ROOT / "integration" / "marketplace"
AUDIT_LOG = ROOT / "integration" / "audit.log"

app = FastAPI(title="Aurora Operator Dashboard")

# mount static UI
if UI_DIR.exists():
    app.mount("/ui", StaticFiles(directory=UI_DIR), name="ui")

# endpoints
@app.get("/api/updater/list")
def list_staged():
    staged = [d.name for d in (UPDATER_DIR / ".aurora_updates" / "staging").glob("*") if d.is_dir()] if (UPDATER_DIR / ".aurora_updates" / "staging").exists() else []
    return {"staged": staged}

@app.post("/api/updater/upload")
async def upload(artifact: UploadFile = File(...)):
    data = await artifact.read()
    tmp = UPDATER_DIR / artifact.filename
    tmp.write_bytes(data)
    # optionally auto-sign (not here) - operator must verify
    return {"ok": True, "filename": str(tmp)}

@app.post("/api/updater/stage")
def stage_file(filename: str = Form(...)):
    # uses updater_service.stage_artifact if available; otherwise do manual copy
    from integration.updater.updater_service import stage_artifact, STAGING
    src = UPDATER_DIR / filename
    if not src.exists():
        raise HTTPException(404, "file not found")
    dest = stage_artifact(src)
    return {"ok": True, "staging": str(dest)}

@app.get("/api/suggestions/list")
def suggestions_list():
    # aggregate suggestions from agents + automotive/maritime/aviation folders
    roots = [Path("agents/suggestions"), Path("automotive/suggestions"), Path("aviation/suggestions")]
    out = {}
    for r in roots:
        if r.exists():
            out[str(r)] = [p.name for p in sorted(r.iterdir()) if p.is_file()]
    return out

@app.post("/api/suggestions/approve")
def approve(file: str):
    # find file and mark approved (very minimal here)
    p = Path(file)
    if not p.exists():
        raise HTTPException(404,"not found")
    p.with_suffix(p.suffix + ".approved").write_text(json.dumps({"ts":time.time()}))
    with open(AUDIT_LOG,"a") as fh:
        fh.write(f"{time.time()} APPROVE {file}\n")
    return {"ok": True}

# Marketplace: simple local-first plugin marketplace
@app.get("/api/marketplace/list")
def marketplace_list():
    # list available plugin archives in marketplace/catalog
    catalog = MARKET / "catalog"
    if not catalog.exists():
        return {"catalog": []}
    return {"catalog": [p.name for p in catalog.iterdir() if p.is_file()]}

@app.post("/api/marketplace/install")
def marketplace_install(pkg_name: str):
    # install plugin package into aurora_modules
    src = MARKET / "catalog" / pkg_name
    if not src.exists():
        raise HTTPException(404, "package not found")
    import tarfile
    with tarfile.open(src, "r:*") as tf:
        tf.extractall("aurora_modules")
    with open(AUDIT_LOG,"a") as fh: fh.write(f"{time.time()} INSTALL {pkg_name}\n")
    return {"ok": True}

@app.get("/api/audit/last")
def audit_last(n: int = 200):
    if not Path(AUDIT_LOG).exists(): return {"lines": []}
    lines = Path(AUDIT_LOG).read_text().splitlines()[-n:]
    return {"lines": lines}

# serve dashboard UI
@app.get("/")
def root():
    if (UI_DIR / "index.html").exists():
        return FileResponse(UI_DIR / "index.html")
    return JSONResponse({"ok": True, "msg": "operator dashboard running"})

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("integration.dashboard.backend:app", host="0.0.0.0", port=9711, reload=False)

3) integration/dashboard/static/index.html

(Operator Dashboard single-file UI — simple, expands easily)

Path: integration/dashboard/static/index.html

<!doctype html>
<html>
<head>
  <meta charset="utf-8"/><title>Aurora Operator</title>
  <style>
    body{font-family:Inter,system-ui,Arial;margin:0;background:#0b1220;color:#e8eef2}
    header{padding:12px;background:#071223;display:flex;align-items:center}
    main{display:flex;height:calc(100vh-56px)}
    .left{width:300px;padding:12px;background:#071026;overflow:auto}
    .right{flex:1;padding:12px;overflow:auto}
    .card{background:#071229;padding:12px;border-radius:8px;margin-bottom:12px}
    button{background:#0ea5a4;border:none;padding:8px 12px;border-radius:6px;color:#022}
  </style>
</head>
<body>
  <header><h3>Aurora Operator Dashboard</h3></header>
  <main>
    <div class="left">
      <div class="card">
        <h4>Updater</h4>
        <input type="file" id="file"/><br/><br/>
        <button onclick="upload()">Upload</button>
        <button onclick="listStage()">List Staged</button>
        <div id="staged"></div>
      </div>
      <div class="card">
        <h4>Suggestions</h4>
        <button onclick="listSuggestions()">Refresh</button>
        <pre id="suggestions"></pre>
      </div>
    </div>
    <div class="right">
      <div class="card">
        <h4>Marketplace</h4>
        <div id="catalog"></div>
        <button onclick="listCatalog()">List Catalog</button>
      </div>
      <div class="card">
        <h4>Audit</h4>
        <pre id="audit"></pre>
        <button onclick="loadAudit()">Load Audit</button>
      </div>
    </div>
  </main>
<script>
async function upload(){
  const f = document.getElementById('file').files[0];
  if(!f) return alert("pick file");
  const fd = new FormData(); fd.append('artifact', f);
  await fetch('/api/updater/upload', {method:'POST', body: fd});
  alert("uploaded");
}
async function listStage(){
  const r = await fetch('/api/updater/list');
  const j = await r.json(); document.getElementById('staged').innerText = JSON.stringify(j,null,2);
}
async function listSuggestions(){
  const r = await fetch('/api/suggestions/list'); const j = await r.json();
  document.getElementById('suggestions').innerText = JSON.stringify(j,null,2);
}
async function listCatalog(){
  const r = await fetch('/api/marketplace/list'); const j = await r.json();
  document.getElementById('catalog').innerText = JSON.stringify(j,null,2);
}
async function loadAudit(){ const r = await fetch('/api/audit/last'); const j = await r.json(); document.getElementById('audit').innerText = JSON.stringify(j,null,2);}
</script>
</body>
</html>

4) integration/marketplace/catalog/README.md

(Where you drop plugin tarballs; marketplace is local-first)

Path: integration/marketplace/catalog/README.md

Place plugin tar.gz archives here. Names should be unique.
Each package should contain a top-level folder with module.json + entry script as in plugin_template.
To publish, copy tar into this folder and operator can install via dashboard.

5) integration/auto_deploy/docker_rollout.sh

(Auto-deploy helper: push Docker images and optionally apply Helm; supports staged rollout via label sampling)

Path: integration/auto_deploy/docker_rollout.sh

#!/usr/bin/env bash
set -euo pipefail
# Usage: docker_rollout.sh <image:tag> <strategy: immediate|canary|staged> [staged_percent]
IMAGE="$1"
STRATEGY="${2:-immediate}"
PERCENT="${3:-10}"
REGISTRY="${REGISTRY:-yourregistry}"
# push image
echo "Pushing image $IMAGE"
docker push "$IMAGE"
if [[ "$STRATEGY" == "immediate" ]]; then
  echo "Immediate rollout: apply deployment update"
  kubectl set image deploy/auroraos auroraos="$IMAGE" --record || true
elif [[ "$STRATEGY" == "canary" ]]; then
  echo "Canary: create canary deployment auroraos-canary"
  kubectl set image deployment/auroraos-canary auroraos="$IMAGE" --record || true
elif [[ "$STRATEGY" == "staged" ]]; then
  echo "Staged rollout placeholder: you must configure traffic splitting (service mesh) or deploy to subset nodes."
  echo "Staged percent requested: $PERCENT"
fi

6) integration/ci/release.yml

(Github Actions snippet to build, sign, upload updater artifacts and create staged release)

Path: integration/ci/release.yml

name: Build & Sign Release
on:
  workflow_dispatch:
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11
      - name: Build updater tar
        run: |
          tar czf aurora-update.tar.gz aurora_core aurora_os.py aurora_modules || true
          sha256sum aurora-update.tar.gz > aurora-update.tar.gz.sha256
      - name: Import GPG (secret base64)
        env:
          GPG_PRIVATE: ${{ secrets.GPG_PRIVATE }}
        run: |
          echo "$GPG_PRIVATE" | base64 --decode > gpg.key
          gpg --batch --import gpg.key
      - name: Sign update
        run: gpg --armor --detach-sign aurora-update.tar.gz
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: aurora_update
          path: |
            aurora-update.tar.gz
            aurora-update.tar.gz.asc
            aurora-update.tar.gz.sha256

7) scripts/apply_pack5_patches.sh

(Convenience script that modifies a few existing files to wire in dashboard/updater/marketplace; I also include manual diffs below.)

Path: scripts/apply_pack5_patches.sh

#!/usr/bin/env bash
set -euo pipefail
ROOT="$(cd "$(dirname "$0")/.." && pwd)"
echo "Applying Pack5 wiring patches..."

# 1) ensure aurora_core in path and update orchestrator to optionally start operator dashboard and updater
# Insert import & thread start into aurora_core/orchestrator.py (if present)
ORI="$ROOT/aurora_core/orchestrator.py"
if [[ -f "$ORI" ]]; then
  grep -q "integration.dashboard.backend" "$ORI" || {
    echo "Patching orchestrator to start operator dashboard and updater service (dev only)"
    # append dev start snippet at end of file before main_loop exit
    cat >> "$ORI" <<'PY'
# --- Pack5 dev convenience: start operator dashboard and updater (dev only) ---
try:
    import threading
    def _start_aux_services():
        try:
            import integration.dashboard.backend as _db; threading.Thread(target=_db.run, daemon=True).start()
        except Exception:
            pass
        try:
            import integration.updater.updater_service as _up; threading.Thread(target=_up.run_server, daemon=True).start()
        except Exception:
            pass
    threading.Thread(target=_start_aux_services, daemon=True).start()
except Exception:
    pass
PY
  }
fi

echo "patches applied (double-check files)."

8) packaging/aurora-operator.service

(systemd unit for operator dashboard)

Path: packaging/aurora-operator.service

[Unit]
Description=Aurora Operator Dashboard
After=network.target

[Service]
Type=simple
User=YOUR_USER
WorkingDirectory=/home/YOUR_USER/Aurora-x
Environment=AURORA_API_TOKEN=aurora-dev-token
ExecStart=/usr/bin/python3 /home/YOUR_USER/Aurora-x/integration/dashboard/backend.py
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target

9) integration/marketplace/publish.sh

(publish helper to add plugin tarballs to catalog with metadata)

Path: integration/marketplace/publish.sh

#!/usr/bin/env bash
set -euo pipefail
CATALOG="$(cd "$(dirname "$0")/catalog" && pwd)"
mkdir -p "$CATALOG"
SRC="$1"
if [[ ! -f "$SRC" ]]; then
  echo "usage: $0 path/to/plugin.tar.gz"
  exit 1
fi
cp "$SRC" "$CATALOG/$(basename "$SRC")"
echo "Published $(basename "$SRC") to marketplace catalog"

10) integration/README_PACK5.md

(overview + flows)

Path: integration/README_PACK5.md

# PACK 5 — Full Integration Pack (Operator dashboard, updater, marketplace, auto-deploy)

Overview:
- Operator dashboard: integration/dashboard/backend.py + static UI
- Updater service: integration/updater/updater_service.py (staging, verify, promote)
- Marketplace: local catalog to drop plugin tarballs
- Auto-deploy: docker_rollout.sh + CI snippets
- Safety: all upgrades require operator approval for safety-critical devices (configurable)

Quickstart:
1. Start Aurora core (pack1): `python3 aurora_os.py`
2. Start dashboard: `python3 integration/dashboard/backend.py` (port 9711)
3. Start updater service (if desired): `python3 integration/updater/updater_service.py` (port 9710)
4. Use UI: open http://localhost:9711/
5. Upload update tar, stage, and promote via dashboard (or use REST endpoints)

Operator flow (recommended):
- Build update pool via CI (integration/ci/release.yml)
- Download updater artifact (aurora-update.tar.gz) to operator machine
- Upload artifact via dashboard
- Stage artifact (verifies signature)
- Test on a canary device or container
- Approve via dashboard (applies atomic swap / backup)
- Observe logs and audit trail

Security:
- Keep AURORA_API_TOKEN secret for API access.
- Use GPG/HSM for signing releases (CI uses secret GPG_PRIVATE in secrets).
- For cars/aircraft/satellites: require manual signature + verified ground segment.

Wiring PACK 4 into your repo — Step-by-step patch list

Below I give precise steps and small diffs to integrate PACK 4 (we already added many new files in PACK 4 earlier) with the rest of the system. These edits are minimal and safe — they only add optional starts or import hooks, and package.json scripts.

Run these commands from repo root:

Create branch and commit current work:

git checkout -b aurora-gpt-pack5
git add -A
git commit -m "backup before pack5 integration"


Add the new directories and files above (copy/paste). After creating, run:

chmod +x integration/updater/updater_service.py
chmod +x integration/dashboard/backend.py
chmod +x integration/auto_deploy/docker_rollout.sh
chmod +x integration/marketplace/publish.sh


Update package.json to add CLI scripts (if you have package.json). Apply this JSON patch (merge manually or use jq):

Add to "scripts":

"scripts": {
  "aurora:start": "python3 aurora_os.py start",
  "aurora:dashboard": "python3 integration/dashboard/backend.py",
  "aurora:updater": "python3 integration/updater/updater_service.py",
  "aurora:pack5:install-market": "bash integration/marketplace/publish.sh"
}


If you don't have package.json, skip this step.

Modify aurora_os.py (top-level) to optionally start dashboard/updater in dev mode (non-production). Insert the following snippet after main imports near top:

Patch snippet (apply via sed or manual insert):

# Pack5 dev helper: start operator dashboard & updater (only in dev)
if os.environ.get("AURORA_ENABLE_AUX","0") == "1":
    try:
        import threading
        def _aux_start():
            try:
                import integration.dashboard.backend as _db
                threading.Thread(target=lambda: _db.__name__, daemon=True).start()
            except Exception:
                pass
            try:
                import integration.updater.updater_service as _up
                threading.Thread(target=lambda: _up.run_server(), daemon=True).start()
            except Exception:
                pass
        threading.Thread(target=_aux_start, daemon=True).start()
    except Exception:
        pass


Explanation: this does not auto-run anything unless you set AURORA_ENABLE_AUX=1 (safe, manual opt-in).

Update docker/Dockerfile.multi (if present) to include PACK5 files and set default CMD to run aurora_os.py. Example addition:

# add at COPY step
COPY integration/ /app/integration/
# ensure required packages for updater/dashboard
RUN pip install --no-cache-dir fastapi uvicorn


Update your docker-compose.yml (if present) to include operator dashboard service:

  operator:
    image: auroraos:latest
    command: python3 integration/dashboard/backend.py
    ports:
      - "9711:9711"
    volumes:
      - ./:/app


Add systemd unit for operator (optional): copy packaging/aurora-operator.service to /etc/systemd/system/aurora-operator.service and systemctl daemon-reload && systemctl enable --now aurora-operator.service (edit paths).

CI: add the integration/ci/release.yml to .github/workflows/ and set secrets.GPG_PRIVATE for signing.

Marketplace usage:

Drop plugin tarballs into integration/marketplace/catalog/ or use integration/marketplace/publish.sh.

Updater: Use integration/updater/updater_service.py to stage artifacts. Operator approves via dashboard or /api/suggestions/approve.

Tests to run after wiring

Start aurora core:

python3 aurora_os.py


Start operator dashboard (in separate shell):

AURORA_ENABLE_AUX=0 python3 integration/dashboard/backend.py
# or
python3 integration/updater/updater_service.py  # in separate terminal


Open http://localhost:9711/
 — upload a small tarball (tar czf test.tar.gz README.md) and stage it.

Check staging:

curl http://localhost:9710/list   # updater service


Install a plugin (create a test plugin tarball from plugin_template) and place in marketplace catalog; install via dashboard.

Enhancements & Options included (summary)

Offline-first updater + cloud-assisted mode: you can host your own builder or CI to create update artifacts; operator then uploads and approves.

Signed updates: CI signs update with GPG; updater verifies signatures before staging/activation.

Human-in-loop gating across agents and device-specific suggestion queues (automotive, aviation, satellite).

Plugin marketplace: local catalog; operator installs plugin tarballs into aurora_modules.

Operator Dashboard: upload/stage/promote, audit logs, suggestions list, marketplace install.

Auto-deploy flows: Docker push + Kubernetes rollout helper; staged/canary strategies supported via conventions.

CI integration: release workflow to build & sign update artifacts.

systemd service unit for operator dashboard.

Safety-first architecture: companion-computer patterns, no automatic ECU/flight firmware flashing, operator sign-off requirement.

Final notes & recommended immediate steps

Create branch aurora-gpt-pack5 and commit current state.

Add all new files above; set executable bits on Python/Bash scripts.

Run pip install -r requirements.txt (add outfit: fastapi, uvicorn, websockets, psutil, python-can if needed).

Start core & dashboard, test with a small plugin tarball and simple updater tar.

Once happy, we can create a single git patch (diff) to apply these changes automatically — say “git patch please” and I’ll generate it.