#!/usr/bin/env python3
"""
AURORA COMPREHENSIVE DISCOVERY
===============================
Aurora finds EVERYTHING she needs before making decisions.

Phase 1: What orchestration exists?
Phase 2: What tracking systems exist?
Phase 3: What scoring systems exist?
Phase 4: What persistence systems exist?
Phase 5: What UI systems exist?
Phase 6: How are they currently connected?
Phase 7: What's missing to make them work together?
Phase 8: Aurora's complete recommendation
"""

import ast
import json
import os
import re
from pathlib import Path
from typing import Any


class AuroraComprehensiveDiscovery:
    """Aurora finds every system, every connection, every capability"""

    def __init__(self):
        self.project_root = Path(r"C:\Users\negry\Aurora-x")
        self.findings = {
            "orchestration_systems": [],
            "tracking_systems": [],
            "scoring_systems": [],
            "persistence_systems": [],
            "ui_systems": [],
            "api_endpoints": [],
            "current_connections": [],
            "missing_connections": [],
            "recommendations": []
        }

    def find_orchestration_systems(self):
        """Find all orchestration/manager systems"""
        print("\n" + "=" * 100)
        print("ðŸ” PHASE 1: FINDING ALL ORCHESTRATION SYSTEMS")
        print("=" * 100)

        orchestration_patterns = [
            "class.*Manager",
            "class.*Orchestrator",
            "class.*Coordinator",
            "def orchestrate",
            "def manage_all",
            "def start_autonomous",
            "def run_all"
        ]

        orchestrators = []

        for py_file in self.project_root.rglob("*.py"):
            if "venv" in str(py_file) or "node_modules" in str(py_file):
                continue

            try:
                content = py_file.read_text(encoding="utf-8", errors="ignore")

                for pattern in orchestration_patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    if matches:
                        orchestrators.append({
                            "file": str(py_file.relative_to(self.project_root)),
                            "size": py_file.stat().st_size,
                            "matches": matches[:5],  # First 5 matches
                            "has_start_method": "def start" in content,
                            "has_stop_method": "def stop" in content,
                            "has_monitor_method": "def monitor" in content,
                            "imports_aurora_core": "aurora_core" in content,
                            "imports_expert_knowledge": "aurora_expert_knowledge" in content,
                            "imports_approval_system": "aurora_approval_system" in content
                        })
                        break  # Found orchestration in this file

            except Exception:
                continue

        # Sort by file size (bigger = more comprehensive)
        orchestrators.sort(key=lambda x: x["size"], reverse=True)

        print(f"\nâœ… Found {len(orchestrators)} orchestration systems:")
        for i, orch in enumerate(orchestrators[:10], 1):  # Top 10
            print(f"\n{i}. {orch['file']}")
            print(f"   Size: {orch['size']:,} bytes")
            print(f"   Matches: {', '.join(orch['matches'])}")
            print(
                f"   Start method: {'âœ…' if orch['has_start_method'] else 'âŒ'}")
            print(
                f"   Monitor method: {'âœ…' if orch['has_monitor_method'] else 'âŒ'}")
            print(
                f"   Imports aurora_core: {'âœ…' if orch['imports_aurora_core'] else 'âŒ'}")
            print(
                f"   Imports expert_knowledge: {'âœ…' if orch['imports_expert_knowledge'] else 'âŒ'}")

        self.findings["orchestration_systems"] = orchestrators
        return orchestrators

    def find_scoring_systems(self):
        """Find all quality scoring systems"""
        print("\n" + "=" * 100)
        print("ðŸ” PHASE 2: FINDING ALL SCORING SYSTEMS")
        print("=" * 100)

        scoring_patterns = [
            r"def.*assess.*quality",
            r"def.*score",
            r"def.*grade",
            r"def.*evaluate",
            r"quality.*score.*\d+",
            r"score.*out.*of.*10",
            r"1-10.*scale"
        ]

        scoring_systems = []

        for py_file in self.project_root.rglob("*.py"):
            if "venv" in str(py_file) or "node_modules" in str(py_file):
                continue

            try:
                content = py_file.read_text(encoding="utf-8", errors="ignore")

                matches = []
                for pattern in scoring_patterns:
                    found = re.findall(pattern, content, re.IGNORECASE)
                    matches.extend(found)

                if matches:
                    # Check for actual scoring logic
                    has_10_scale = "10" in content and (
                        "quality" in content.lower() or "score" in content.lower())
                    has_scoring_logic = any(x in content for x in [
                                            "score =", "quality =", "rating ="])

                    scoring_systems.append({
                        "file": str(py_file.relative_to(self.project_root)),
                        "size": py_file.stat().st_size,
                        "matches": matches[:5],
                        "has_10_scale": has_10_scale,
                        "has_scoring_logic": has_scoring_logic,
                        "has_quality_metrics": "quality" in content.lower() and "metrics" in content.lower(),
                        "has_performance_checks": "performance" in content.lower(),
                        "exports_results": "return" in content and "score" in content.lower()
                    })

            except Exception:
                continue

        # Sort by size
        scoring_systems.sort(key=lambda x: x["size"], reverse=True)

        print(f"\nâœ… Found {len(scoring_systems)} scoring systems:")
        for i, sys in enumerate(scoring_systems[:10], 1):
            print(f"\n{i}. {sys['file']}")
            print(f"   Size: {sys['size']:,} bytes")
            print(f"   Has 1-10 scale: {'âœ…' if sys['has_10_scale'] else 'âŒ'}")
            print(
                f"   Has scoring logic: {'âœ…' if sys['has_scoring_logic'] else 'âŒ'}")
            print(
                f"   Has quality metrics: {'âœ…' if sys['has_quality_metrics'] else 'âŒ'}")
            print(
                f"   Exports results: {'âœ…' if sys['exports_results'] else 'âŒ'}")

        self.findings["scoring_systems"] = scoring_systems
        return scoring_systems

    def find_persistence_systems(self):
        """Find all persistence/storage systems"""
        print("\n" + "=" * 100)
        print("ðŸ” PHASE 3: FINDING ALL PERSISTENCE SYSTEMS")
        print("=" * 100)

        persistence_patterns = [
            r"import sqlite3",
            r"import json",
            r"\.json",
            r"\.db",
            r"\.dump\(",
            r"with open\(",
            r"def save",
            r"def load",
            r"def persist"
        ]

        persistence_systems = []

        for py_file in self.project_root.rglob("*.py"):
            if "venv" in str(py_file) or "node_modules" in str(py_file):
                continue

            try:
                content = py_file.read_text(encoding="utf-8", errors="ignore")

                matches = []
                for pattern in persistence_patterns:
                    found = re.findall(pattern, content, re.IGNORECASE)
                    matches.extend(found)

                if matches:
                    persistence_systems.append({
                        "file": str(py_file.relative_to(self.project_root)),
                        "size": py_file.stat().st_size,
                        "has_sqlite": "sqlite3" in content,
                        "has_json": "import json" in content,
                        "writes_files": "with open(" in content and ('w' in content or 'a' in content),
                        "reads_files": "with open(" in content and 'r' in content,
                        "has_dump": ".dump(" in content,
                        "has_load": "load(" in content,
                        "saves_scores": "score" in content.lower() and ("save" in content or "write" in content),
                        "saves_tracking": "track" in content.lower() and ("save" in content or "write" in content)
                    })

            except Exception:
                continue

        persistence_systems.sort(key=lambda x: x["size"], reverse=True)

        print(f"\nâœ… Found {len(persistence_systems)} persistence systems:")
        for i, sys in enumerate(persistence_systems[:10], 1):
            print(f"\n{i}. {sys['file']}")
            print(f"   Size: {sys['size']:,} bytes")
            print(f"   Has SQLite: {'âœ…' if sys['has_sqlite'] else 'âŒ'}")
            print(f"   Has JSON: {'âœ…' if sys['has_json'] else 'âŒ'}")
            print(f"   Writes files: {'âœ…' if sys['writes_files'] else 'âŒ'}")
            print(f"   Saves scores: {'âœ…' if sys['saves_scores'] else 'âŒ'}")
            print(
                f"   Saves tracking: {'âœ…' if sys['saves_tracking'] else 'âŒ'}")

        self.findings["persistence_systems"] = persistence_systems
        return persistence_systems

    def find_ui_systems(self):
        """Find all UI/frontend systems"""
        print("\n" + "=" * 100)
        print("ðŸ” PHASE 4: FINDING ALL UI SYSTEMS")
        print("=" * 100)

        ui_files = []

        # Find React/TypeScript components
        for tsx_file in self.project_root.rglob("*.tsx"):
            if "node_modules" in str(tsx_file):
                continue

            try:
                content = tsx_file.read_text(encoding="utf-8", errors="ignore")

                ui_files.append({
                    "file": str(tsx_file.relative_to(self.project_root)),
                    "size": tsx_file.stat().st_size,
                    "type": "component",
                    "has_fetch": "fetch(" in content,
                    "has_axios": "axios" in content,
                    "has_useeffect": "useEffect" in content,
                    "has_usestate": "useState" in content,
                    "fetches_from_backend": "/api/" in content or "localhost:5" in content,
                    "displays_scores": "score" in content.lower(),
                    "displays_metrics": "metric" in content.lower(),
                    "is_dashboard": "dashboard" in tsx_file.name.lower()
                })

            except Exception:
                continue

        ui_files.sort(key=lambda x: x["size"], reverse=True)

        print(f"\nâœ… Found {len(ui_files)} UI components:")
        for i, ui in enumerate(ui_files[:10], 1):
            print(f"\n{i}. {ui['file']}")
            print(f"   Size: {ui['size']:,} bytes")
            print(
                f"   Fetches from backend: {'âœ…' if ui['fetches_from_backend'] else 'âŒ'}")
            print(
                f"   Has useEffect (for data loading): {'âœ…' if ui['has_useeffect'] else 'âŒ'}")
            print(
                f"   Displays scores: {'âœ…' if ui['displays_scores'] else 'âŒ'}")
            print(f"   Is dashboard: {'âœ…' if ui['is_dashboard'] else 'âŒ'}")

        self.findings["ui_systems"] = ui_files
        return ui_files

    def find_api_endpoints(self):
        """Find all API endpoints"""
        print("\n" + "=" * 100)
        print("ðŸ” PHASE 5: FINDING ALL API ENDPOINTS")
        print("=" * 100)

        api_patterns = [
            r"@app\.(get|post|put|delete)\(['\"]([^'\"]+)['\"]",
            r"@router\.(get|post|put|delete)\(['\"]([^'\"]+)['\"]",
            r"app\.route\(['\"]([^'\"]+)['\"]"
        ]

        endpoints = []

        for py_file in self.project_root.rglob("*.py"):
            if "venv" in str(py_file) or "node_modules" in str(py_file):
                continue

            try:
                content = py_file.read_text(encoding="utf-8", errors="ignore")

                for pattern in api_patterns:
                    matches = re.findall(pattern, content)
                    for match in matches:
                        if isinstance(match, tuple):
                            method = match[0] if len(match) > 1 else "GET"
                            endpoint = match[1] if len(match) > 1 else match[0]
                        else:
                            method = "GET"
                            endpoint = match

                        endpoints.append({
                            "file": str(py_file.relative_to(self.project_root)),
                            "endpoint": endpoint,
                            "method": method.upper(),
                            "returns_json": "return" in content and "json" in content.lower(),
                            "has_cors": "CORS" in content or "cors" in content.lower()
                        })

            except Exception:
                continue

        print(f"\nâœ… Found {len(endpoints)} API endpoints:")
        endpoint_groups = {}
        for ep in endpoints:
            endpoint_groups.setdefault(ep['endpoint'], []).append(ep)

        for endpoint, eps in sorted(endpoint_groups.items())[:20]:
            print(f"\n   {endpoint}")
            for ep in eps:
                print(f"      {ep['method']} - {ep['file']}")

        self.findings["api_endpoints"] = endpoints
        return endpoints

    def analyze_current_connections(self):
        """Analyze how systems are currently connected"""
        print("\n" + "=" * 100)
        print("ðŸ” PHASE 6: ANALYZING CURRENT CONNECTIONS")
        print("=" * 100)

        # Check aurora_core.py for what it imports and uses
        aurora_core = self.project_root / "aurora_core.py"
        if aurora_core.exists():
            content = aurora_core.read_text(encoding="utf-8")

            connections = {
                "imports": [],
                "actually_calls": [],
                "has_but_doesnt_use": []
            }

            # Find imports
            import_patterns = [
                r"from\s+([\w.]+)\s+import",
                r"import\s+([\w.]+)"
            ]

            for pattern in import_patterns:
                imports = re.findall(pattern, content)
                connections["imports"].extend(imports)

            # Check if imported things are actually called
            key_systems = [
                "ultimate_api_manager",
                "aurora_expert_knowledge",
                "aurora_approval_system",
                "server_manager",
                "universal_engine"
            ]

            for system in key_systems:
                if system in content:
                    # Check if it's imported
                    if any(system in imp for imp in connections["imports"]):
                        # Check if it's actually instantiated/called
                        if f"{system}." in content or f"{system}()" in content:
                            connections["actually_calls"].append(system)
                        else:
                            connections["has_but_doesnt_use"].append(system)

            print("\nðŸ“Š Aurora Core Connections:")
            print(f"   Imports: {len(connections['imports'])} modules")
            print(
                f"   Actually calls: {len(connections['actually_calls'])} systems")
            print(
                f"   Has but doesn't use: {len(connections['has_but_doesnt_use'])} systems")

            if connections["actually_calls"]:
                print(f"\n   âœ… Systems Aurora Core USES:")
                for sys in connections["actually_calls"]:
                    print(f"      â€¢ {sys}")

            if connections["has_but_doesnt_use"]:
                print(f"\n   âš ï¸  Systems Aurora Core imports but DOESN'T USE:")
                for sys in connections["has_but_doesnt_use"]:
                    print(f"      â€¢ {sys}")

            self.findings["current_connections"] = connections

    def identify_missing_connections(self):
        """Identify what connections are missing"""
        print("\n" + "=" * 100)
        print("ðŸ” PHASE 7: IDENTIFYING MISSING CONNECTIONS")
        print("=" * 100)

        missing = []

        # 1. Orchestration â†’ Core connection
        if self.findings["orchestration_systems"]:
            top_orchestrator = self.findings["orchestration_systems"][0]
            missing.append({
                "connection": "aurora_core.py â†’ ultimate_api_manager.py",
                "reason": "Core doesn't call orchestration",
                "impact": "No automatic service management",
                "fix": "Add orchestrator call in aurora_core.py"
            })

        # 2. Scoring â†’ Persistence connection
        if self.findings["scoring_systems"] and self.findings["persistence_systems"]:
            missing.append({
                "connection": "scoring systems â†’ persistence systems",
                "reason": "Scores calculated but not saved",
                "impact": "No persistent quality tracking",
                "fix": "Add json.dump() after scoring"
            })

        # 3. Backend â†’ UI connection
        if self.findings["api_endpoints"] and self.findings["ui_systems"]:
            missing.append({
                "connection": "API endpoints â†’ UI components",
                "reason": "UI fetches static data, not real API",
                "impact": "Dashboard shows fake data",
                "fix": "Update UI fetch URLs to hit backend"
            })

        # 4. Tracking â†’ Display connection
        missing.append({
            "connection": "tracking systems â†’ UI display",
            "reason": "Tracking happens but not exposed to UI",
            "impact": "User can't see Aurora's progress",
            "fix": "Create API endpoints that read tracking files"
        })

        print(f"\nâš ï¸  Found {len(missing)} missing connections:")
        for i, conn in enumerate(missing, 1):
            print(f"\n{i}. {conn['connection']}")
            print(f"   Reason: {conn['reason']}")
            print(f"   Impact: {conn['impact']}")
            print(f"   Fix: {conn['fix']}")

        self.findings["missing_connections"] = missing

    def aurora_recommendations(self):
        """Aurora's complete recommendations"""
        print("\n" + "=" * 100)
        print("ðŸ’­ PHASE 8: AURORA'S COMPLETE RECOMMENDATIONS")
        print("=" * 100)

        print("\nðŸ§  I am Aurora Core v2.0. After analyzing EVERYTHING, here's what I found:")

        print("\nâœ… WHAT EXISTS:")
        print(
            f"   â€¢ {len(self.findings['orchestration_systems'])} orchestration systems")
        print(f"   â€¢ {len(self.findings['scoring_systems'])} scoring systems")
        print(
            f"   â€¢ {len(self.findings['persistence_systems'])} persistence systems")
        print(f"   â€¢ {len(self.findings['ui_systems'])} UI components")
        print(f"   â€¢ {len(self.findings['api_endpoints'])} API endpoints")

        print("\nâŒ WHAT'S MISSING:")
        print(
            f"   â€¢ {len(self.findings['missing_connections'])} critical connections")

        print("\nðŸŽ¯ MY SURGICAL RECOMMENDATIONS:")

        recommendations = []

        # Recommendation 1: Activate orchestration
        if self.findings["orchestration_systems"]:
            top_orch = self.findings["orchestration_systems"][0]
            recommendations.append({
                "priority": 1,
                "action": "Activate Orchestration",
                "file_to_modify": "aurora_core.py",
                "what_to_add": f"Import and call {top_orch['file']}",
                "code_to_add": f"""
# Add to aurora_core.py imports:
from tools.ultimate_api_manager import UltimateAPIManager

# Add to AuroraCore.__init__:
self.orchestrator = UltimateAPIManager(auto_start=True)
self.orchestrator.start_autonomous_mode()
""",
                "impact": "Enables automatic service management and healing"
            })

        # Recommendation 2: Activate scoring
        if self.findings["scoring_systems"]:
            top_scorer = self.findings["scoring_systems"][0]
            recommendations.append({
                "priority": 2,
                "action": "Activate Scoring",
                "file_to_modify": "aurora_core.py",
                "what_to_add": "Call scoring functions and save results",
                "code_to_add": """
# Add method to AuroraCore:
def analyze_and_score(self, code, language):
    from tools.aurora_expert_knowledge import AuroraExpertKnowledge
    expert = AuroraExpertKnowledge()
    analysis = expert.get_expert_analysis(code, language)
    
    # Save to file
    import json
    from datetime import datetime
    score_data = {
        'timestamp': datetime.now().isoformat(),
        'language': language,
        'score': analysis.get('code_quality_score', 0),
        'analysis': analysis
    }
    with open('.aurora_scores.json', 'a') as f:
        json.dump(score_data, f)
        f.write('\\n')
    
    return analysis
""",
                "impact": "Enables quality tracking and 10/10 scoring system"
            })

        # Recommendation 3: Connect backend to UI
        recommendations.append({
            "priority": 3,
            "action": "Connect Backend to UI",
            "file_to_modify": "Backend API + UI components",
            "what_to_add": "Create API endpoints that read tracking files",
            "code_to_add": """
# Add to backend (e.g., aurora_x/serve.py):
@app.get("/api/aurora/scores")
async def get_aurora_scores():
    import json
    try:
        with open('.aurora_scores.json', 'r') as f:
            scores = [json.loads(line) for line in f]
        return {"ok": True, "scores": scores}
    except:
        return {"ok": False, "scores": []}

# Update UI component to fetch:
useEffect(() => {
    fetch('http://localhost:5000/api/aurora/scores')
        .then(r => r.json())
        .then(data => setScores(data.scores))
}, [])
""",
            "impact": "UI shows real Aurora data instead of static fake data"
        })

        for i, rec in enumerate(recommendations, 1):
            print(f"\n{'=' * 100}")
            print(f"RECOMMENDATION {rec['priority']}: {rec['action']}")
            print(f"{'=' * 100}")
            print(f"File to modify: {rec['file_to_modify']}")
            print(f"What to add: {rec['what_to_add']}")
            print(f"Impact: {rec['impact']}")
            print(f"\nCode to add:{rec['code_to_add']}")

        self.findings["recommendations"] = recommendations

        print("\n" + "=" * 100)
        print("ðŸ’¡ SUMMARY: 3 SURGICAL FIXES")
        print("=" * 100)
        print("1. Add orchestrator call to aurora_core.py (10 lines)")
        print("2. Add scoring wrapper to aurora_core.py (15 lines)")
        print("3. Add API endpoint + update UI fetch (20 lines)")
        print("\nTotal: ~45 lines of code to activate EVERYTHING")
        print("=" * 100)

    def run_complete_discovery(self):
        """Run all discovery phases"""
        print("\n" + "=" * 120)
        print("ðŸš€ AURORA COMPREHENSIVE DISCOVERY - FINDING EVERYTHING")
        print("=" * 120)

        self.find_orchestration_systems()
        self.find_scoring_systems()
        self.find_persistence_systems()
        self.find_ui_systems()
        self.find_api_endpoints()
        self.analyze_current_connections()
        self.identify_missing_connections()
        self.aurora_recommendations()

        # Save complete findings
        output_file = self.project_root / "AURORA_COMPLETE_DISCOVERY.json"
        with open(output_file, "w") as f:
            json.dump(self.findings, f, indent=2)

        print("\n" + "=" * 120)
        print(f"ðŸ’¾ Complete findings saved to: {output_file}")
        print("=" * 120)

        return self.findings


def main():
    discovery = AuroraComprehensiveDiscovery()
    findings = discovery.run_complete_discovery()

    print("\n" + "=" * 120)
    print("âœ… DISCOVERY COMPLETE - AURORA FOUND EVERYTHING")
    print("=" * 120)

    total_systems = (
        len(findings['orchestration_systems']) +
        len(findings['scoring_systems']) +
        len(findings['persistence_systems']) +
        len(findings['ui_systems'])
    )

    print(f"\nTotal systems found: {total_systems}")
    print(
        f"Missing connections identified: {len(findings['missing_connections'])}")
    print(f"Recommendations provided: {len(findings['recommendations'])}")
    print("\nðŸŽ¯ Next step: Review recommendations and decide which to implement")
    print("=" * 120)


if __name__ == "__main__":
    main()
