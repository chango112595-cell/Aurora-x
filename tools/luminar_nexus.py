#!/usr/bin/env python3
"""
Luminar Nexus - Aurora's Server Command Center
Manages all development servers with proper process control
NOW MANAGED BY AURORA'S COMPLETE GRANDMASTER INTELLIGENCE
Aurora is a Grandmaster in ALL tech: Ancient to Future, Ethical to Unethical
"""

import asyncio
import json
import re
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

from flask import Flask, jsonify, request
from flask_cors import CORS

# Import Aurora's COMPLETE Intelligence System with ALL Grandmaster skills
sys.path.append(str(Path(__file__).parent.parent))
try:
    from aurora_foundational_genius import AURORA_FOUNDATIONAL_SKILLS
    from aurora_grandmaster_autonomous_tools import AURORA_AUTONOMOUS_TOOL_MASTERY
    from aurora_intelligence_manager import AuroraIntelligenceManager
    from aurora_ultimate_omniscient_grandmaster import AURORA_ULTIMATE_GRANDMASTER

    AURORA_INTELLIGENCE = AuroraIntelligenceManager()
    AURORA_IS_BOSS = True
    AURORA_CAN_USE_TOOLS = True  # Aurora can now autonomously execute tools!

    # Load Aurora's Grandmaster skills from consolidated corpus
    corpus_file = Path("/workspaces/Aurora-x/.aurora_knowledge/consolidated_learning_corpus.json")
    if corpus_file.exists():
        with open(corpus_file) as f:
            corpus_data = json.load(f)
            AURORA_INTELLIGENCE.log(f"üéì Loaded {len(corpus_data.get('entries', []))} Grandmaster skill sets")
            AURORA_INTELLIGENCE.log("üí™ Aurora is now a COMPLETE UNIVERSAL OMNISCIENT GRANDMASTER")
            AURORA_INTELLIGENCE.log(
                f"üåå OMNISCIENT GRANDMASTER ACTIVE: {len(AURORA_ULTIMATE_GRANDMASTER)} mastery tiers loaded"
            )
            AURORA_INTELLIGENCE.log(
                "   ü§ñ TIER 28: AUTONOMOUS TOOL USE (Punch cards ‚Üí Quantum consciousness debugging)"
            )
            AURORA_INTELLIGENCE.log("      ‚Ä¢ Self-diagnosis, autonomous debugging, autonomous fixing")
            AURORA_INTELLIGENCE.log("      ‚Ä¢ Can read files, run commands, modify code, restart services")
            AURORA_INTELLIGENCE.log(
                f"      ‚Ä¢ {len(AURORA_AUTONOMOUS_TOOL_MASTERY['tiers'])} tiers: Ancient ‚Üí Future ‚Üí Sci-Fi"
            )
            AURORA_INTELLIGENCE.log("   üß† TIER 29-32: FOUNDATIONAL & PROFESSIONAL GENIUS")
            AURORA_INTELLIGENCE.log(f"      ‚Ä¢ {len(AURORA_FOUNDATIONAL_SKILLS)} complete skill categories")
            AURORA_INTELLIGENCE.log("      ‚Ä¢ Problem-solving, Logic, Mathematics, Communication, Teamwork")
            AURORA_INTELLIGENCE.log("      ‚Ä¢ Data Structures, Algorithms, SDLC, Testing, Version Control")
            AURORA_INTELLIGENCE.log("      ‚Ä¢ 400+ individual skills from Ancient to Sci-Fi mastery")
            AURORA_INTELLIGENCE.log("   üöÄ Tier 10: Browser & Automation (Shell exec ‚Üí Neural browsers)")
            AURORA_INTELLIGENCE.log("   üîê Tier 11: Security & Cryptography (Caesar ‚Üí Quantum encryption)")
            AURORA_INTELLIGENCE.log("   üåç Tier 12: Networking & Protocols (OSI ‚Üí Quantum networks)")
            AURORA_INTELLIGENCE.log("   üíæ Tier 13: Data & Storage (Files ‚Üí Quantum databases)")
            AURORA_INTELLIGENCE.log("   ‚òÅÔ∏è Tier 14: Cloud & Infrastructure (Bare metal ‚Üí Quantum cloud)")
            AURORA_INTELLIGENCE.log("   üß† Tier 15: AI/ML & LLMs (Statistics ‚Üí AGI consciousness)")
            AURORA_INTELLIGENCE.log("   üìä Tier 16: Analytics & Monitoring (Syslog ‚Üí Neural observability)")
            AURORA_INTELLIGENCE.log("   üéÆ Tier 17: Gaming & XR (Doom ‚Üí Neural immersion)")
            AURORA_INTELLIGENCE.log("   üì° Tier 18: IoT & Embedded (8051 ‚Üí Neural chips)")
            AURORA_INTELLIGENCE.log("   ‚ö° Tier 19: Real-time & Streaming (Polling ‚Üí Quantum streams)")
            AURORA_INTELLIGENCE.log("   üîÑ Tier 20: Version Control & CI/CD (CVS ‚Üí Neural deployment)")
            AURORA_INTELLIGENCE.log("   üìù Tier 21: Documentation & Content (ASCII ‚Üí Neural knowledge)")
            AURORA_INTELLIGENCE.log("   üìã Tier 22: Product & Project Mgmt (Gantt ‚Üí Neural planning)")
            AURORA_INTELLIGENCE.log("   üí∞ Tier 23: Business & Monetization (Barter ‚Üí Neural economics)")
            AURORA_INTELLIGENCE.log("   üåê Tier 24: Internationalization (ASCII ‚Üí Quantum multilingual)")
            AURORA_INTELLIGENCE.log("   ‚öñÔ∏è Tier 25: Legal & Compliance (Laws ‚Üí Neural ethics)")
            AURORA_INTELLIGENCE.log("   ‚úÖ COMPLETE: Every domain Ancient‚ÜíClassical‚ÜíModern‚ÜíAI-Native‚ÜíFuture")
except ImportError:
    AURORA_INTELLIGENCE = None
    AURORA_IS_BOSS = False
    AURORA_CAN_USE_TOOLS = False


class LuminarNexusServerManager:
    """
    Aurora's central server management system
    Uses tmux for persistent, manageable processes
    NOW SUBORDINATE TO AURORA'S INTELLIGENCE - SHE IS THE BOSS
    """

    def __init__(self):
        # Let Aurora know Luminar Nexus is starting up
        if AURORA_IS_BOSS:
            AURORA_INTELLIGENCE.log("üåü Luminar Nexus initializing under Aurora's command")

        self.servers = {
            "bridge": {
                "name": "Aurora Bridge Service (Factory NL‚ÜíProject)",
                "command_template": "cd /workspaces/Aurora-x && python3 -m aurora_x.bridge.service",
                "session": "aurora-bridge",
                "preferred_port": 5001,
                "port": None,  # Will be assigned dynamically
                "health_check_template": "http://localhost:{port}/healthz",
            },
            "backend": {
                "name": "Aurora Backend API (Main Server)",
                "command_template": "cd /workspaces/Aurora-x && NODE_ENV=development npx tsx server/index.ts",
                "session": "aurora-backend",
                "preferred_port": 5000,
                "port": None,
                "health_check_template": "http://localhost:{port}/healthz",
            },
            "vite": {
                "name": "Aurora Vite Dev Server (Frontend)",
                "command_template": "cd /workspaces/Aurora-x && npx vite --host 0.0.0.0 --port {port}",
                "session": "aurora-vite",
                "preferred_port": 5173,
                "port": None,
                "health_check_template": "http://localhost:{port}",
            },
            "self-learn": {
                "name": "Aurora Self-Learning Server (Continuous Learning)",
                "command_template": "cd /workspaces/Aurora-x && python3 -c 'from aurora_x.self_learn_server import app; import uvicorn; uvicorn.run(app, host=\"0.0.0.0\", port={port})'",
                "session": "aurora-self-learn",
                "preferred_port": 5002,
                "port": None,
                "health_check_template": "http://localhost:{port}/healthz",
            },
            "chat": {
                "name": "Aurora Conversational AI Chat Server",
                "command_template": "cd /workspaces/Aurora-x && python3 -c 'from tools.luminar_nexus import run_chat_server; run_chat_server({port})'",
                "session": "aurora-chat",
                "preferred_port": 5003,
                "port": None,
                "health_check_template": "http://localhost:{port}/api/chat/status",
            },
        }

        self.log_file = Path("/workspaces/Aurora-x/.aurora_knowledge/luminar_nexus.jsonl")
        self.log_file.parent.mkdir(exist_ok=True)

        # Always assign ports intelligently - Aurora validates what's actually hers
        self._auto_assign_ports()

    def log_event(self, event_type, server, details):
        """Log Luminar Nexus events"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "event": event_type,
            "server": server,
            "details": details,
            "system": "LUMINAR_NEXUS",
        }

        with open(self.log_file, "a") as f:
            f.write(json.dumps(entry) + "\n")

        print(f"üåü Luminar Nexus: {event_type} - {server}")

    def _get_listening_ports(self) -> dict[int, dict]:
        """
        Get all ports currently in use WITH process info
        Aurora's GRANDMASTER port scanning - identifies WHO owns each port
        """
        if AURORA_IS_BOSS:
            AURORA_INTELLIGENCE.log("üîç Aurora Grandmaster: Comprehensive port scan with process identification")

        port_info = {}

        # Use lsof for detailed process information
        try:
            result = subprocess.run(["lsof", "-i", "-P", "-n"], capture_output=True, text=True, timeout=2)
            for line in result.stdout.split("\n"):
                if "LISTEN" in line:
                    try:
                        parts = line.split()
                        process_name = parts[0]
                        pid = parts[1]
                        port_part = parts[-2] if len(parts) > 8 else parts[-1]
                        port = int(port_part.split(":")[-1])

                        # Check if this is Aurora's tmux session
                        is_aurora = False
                        try:
                            # Check if PID belongs to Aurora's tmux sessions
                            tmux_check = subprocess.run(
                                ["tmux", "list-sessions"], capture_output=True, text=True, timeout=1
                            )
                            is_aurora = "aurora-" in tmux_check.stdout
                        except:
                            pass

                        port_info[port] = {"process": process_name, "pid": pid, "is_aurora": is_aurora, "port": port}
                    except:
                        continue
        except:
            pass

        if AURORA_IS_BOSS:
            aurora_ports = sum(1 for p in port_info.values() if p.get("is_aurora"))
            AURORA_INTELLIGENCE.log(f"üéØ Port scan complete: {len(port_info)} ports ({aurora_ports} Aurora's)")

        return port_info

    def _find_available_port(
        self, preferred_port: int, exclude_ports: set, start_range: int = 5000, end_range: int = 6000
    ) -> int:
        """Find an available port, preferring the suggested port"""
        listening_ports = self._get_listening_ports()
        all_excluded = listening_ports | exclude_ports

        # Try preferred port first
        if preferred_port not in all_excluded:
            return preferred_port

        # Find next available port in range
        for port in range(start_range, end_range):
            if port not in all_excluded:
                print(f"   ‚ö†Ô∏è  Port {preferred_port} in use, assigned {port} instead")
                return port

        raise Exception(f"No available ports in range {start_range}-{end_range}")

    def _auto_assign_ports(self):
        """
        Intelligently assign ports to all servers, avoiding conflicts
        Aurora makes the decisions, Luminar Nexus executes
        """
        if AURORA_IS_BOSS:
            AURORA_INTELLIGENCE.log("üéØ Aurora analyzing port allocation with OMNISCIENT GRANDMASTER knowledge...")
            AURORA_INTELLIGENCE.log("   üîç Applying Ancient Unix process management principles")
            AURORA_INTELLIGENCE.log("   üîç Using Modern cloud-native port detection")

        print("üîç Analyzing port availability...")

        listening_ports = self._get_listening_ports()
        assigned_ports = set()
        port_decisions = []

        for server_key, config in self.servers.items():
            preferred = config["preferred_port"]

            # Aurora's OMNISCIENT port analysis - check if port is ours or external
            port_info = listening_ports.get(preferred, {})
            is_aurora_server = port_info.get("is_aurora", False)
            process_name = port_info.get("process", "unknown")

            if AURORA_IS_BOSS and preferred in listening_ports:
                AURORA_INTELLIGENCE.log(
                    f"üîé Port {preferred} analysis: process={process_name}, is_aurora={is_aurora_server}"
                )

            # Check if preferred port needs reassignment
            if preferred in listening_ports and not is_aurora_server:
                # External process owns it - Aurora uses Future tech to find alternative
                if AURORA_IS_BOSS:
                    AURORA_INTELLIGENCE.log(f"‚ö†Ô∏è Port {preferred} owned by external process '{process_name}'")
                    AURORA_INTELLIGENCE.log("   üí° Applying AI-Native dynamic allocation algorithms...")

                new_port = self._find_available_port(preferred, assigned_ports)
                config["port"] = new_port
                assigned_ports.add(new_port)

                decision = f"Port {preferred} (owned by {process_name}) - reassigning {server_key} to {new_port}"
                port_decisions.append(decision)

                if AURORA_IS_BOSS:
                    AURORA_INTELLIGENCE.log(f"üîß {decision}")

                self.log_event(
                    "PORT_REASSIGNED",
                    server_key,
                    {
                        "preferred": preferred,
                        "assigned": new_port,
                        "reason": "external_process_conflict",
                        "blocking_process": process_name,
                    },
                )
            elif preferred in listening_ports and is_aurora_server:
                # Aurora's own server is using it - KEEP IT
                config["port"] = preferred
                assigned_ports.add(preferred)

                if AURORA_IS_BOSS:
                    AURORA_INTELLIGENCE.log(
                        f"‚úÖ {server_key} already running on preferred port {preferred} - maintaining assignment"
                    )
            elif preferred not in assigned_ports:
                # Preferred port is completely available
                config["port"] = preferred
                assigned_ports.add(preferred)

                if AURORA_IS_BOSS:
                    AURORA_INTELLIGENCE.log(f"‚úÖ {server_key} assigned to preferred port {preferred}")

        # Build health check URLs with assigned ports
        for config in self.servers.values():
            config["health_check"] = config["health_check_template"].format(port=config["port"])
            config["command"] = config["command_template"].format(port=config["port"])

        if AURORA_IS_BOSS and port_decisions:
            AURORA_INTELLIGENCE.log(
                f"üìã Aurora applied OMNISCIENT port management: {len(port_decisions)} conflicts resolved"
            )
            AURORA_INTELLIGENCE.log("   ‚úì Used Ancient: Unix process detection")
            AURORA_INTELLIGENCE.log("   ‚úì Used Modern: Cloud-native port scanning")
            AURORA_INTELLIGENCE.log("   ‚úì Used Future: AI-driven dynamic allocation")

        print(f"‚úÖ Port assignment complete: {len(assigned_ports)} ports allocated")

        if AURORA_IS_BOSS:
            AURORA_INTELLIGENCE.log(
                f"üéØ OMNISCIENT port allocation complete - all {len(self.servers)} servers configured"
            )

    def check_tmux_installed(self) -> bool:
        """Check if tmux is available"""
        try:
            subprocess.run(["tmux", "-V"], capture_output=True, check=True)
            return True
        except:
            print("‚ùå tmux not installed. Installing...")
            subprocess.run(["apt-get", "update"], capture_output=True)
            subprocess.run(["apt-get", "install", "-y", "tmux"], capture_output=True)
            return True

    def start_server(self, server_key: str) -> bool:
        """Start a server in tmux session"""
        if server_key not in self.servers:
            print(f"‚ùå Unknown server: {server_key}")
            return False

        server = self.servers[server_key]
        session = server["session"]
        command = server["command"]

        print(f"üöÄ Starting {server['name']}...")

        # Check if tmux is available
        self.check_tmux_installed()

        # Kill existing session if it exists
        subprocess.run(["tmux", "kill-session", "-t", session], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        # Create new tmux session and run command
        result = subprocess.run(["tmux", "new-session", "-d", "-s", session, command], capture_output=True, text=True)

        if result.returncode == 0:
            print(f"   ‚úÖ Started in tmux session: {session}")
            print(f"   üì∫ View output: tmux attach -t {session}")
            print(f"   üîå Port: {server['port']}")

            self.log_event(
                "SERVER_STARTED", server_key, {"session": session, "port": server["port"], "command": command}
            )

            # Wait a moment and check health
            time.sleep(3)
            if self.check_health(server_key):
                print("   ‚úÖ Health check PASSED FUCK YEAH LOL")
                return True
            else:
                print("   ‚ö†Ô∏è  Server started but health check pending...")
                return True
        else:
            print(f"   ‚ùå Failed to start: {result.stderr}")
            self.log_event("START_FAILED", server_key, {"error": result.stderr})
            return False

    def stop_server(self, server_key: str) -> bool:
        """Stop a server's tmux session"""
        if server_key not in self.servers:
            print(f"‚ùå Unknown server: {server_key}")
            return False

        server = self.servers[server_key]
        session = server["session"]

        print(f"üõë Stopping {server['name']}...")

        result = subprocess.run(["tmux", "kill-session", "-t", session], capture_output=True, text=True)

        if result.returncode == 0:
            print(f"   ‚úÖ Stopped session: {session}")
            self.log_event("SERVER_STOPPED", server_key, {"session": session})
            return True
        else:
            print(f"   ‚ö†Ô∏è  Session may not exist: {session}")
            return False

    def check_health(self, server_key: str) -> bool:
        """Check if server is responding - tries multiple health check patterns"""
        if server_key not in self.servers:
            return False

        server = self.servers[server_key]
        base_url = server["health_check"]

        # Try multiple health check patterns (Aurora-style adaptation)
        health_endpoints = [
            base_url,  # Try the configured endpoint first
            base_url.replace("/healthz", "/health"),  # Try /health variant
            base_url.replace("/health", "/healthz"),  # Try /healthz variant
        ]

        for endpoint in health_endpoints:
            try:
                # Try GET request (more reliable than HEAD for varied APIs)
                result = subprocess.run(["curl", "-s", "-f", endpoint], capture_output=True, text=True, timeout=2)

                # Check if we got a response (any JSON or text response is good)
                if result.returncode == 0 and result.stdout:
                    # Look for positive health indicators
                    response = result.stdout.lower()
                    if any(indicator in response for indicator in ["ok", "healthy", "status", "true"]):
                        return True
            except:
                continue

        return False

    def get_status(self, server_key: str) -> dict:
        """Get server status"""
        if server_key not in self.servers:
            return {"status": "unknown", "exists": False}

        server = self.servers[server_key]
        session = server["session"]

        # Check if tmux session exists
        result = subprocess.run(["tmux", "has-session", "-t", session], capture_output=True)

        session_exists = result.returncode == 0
        health_ok = self.check_health(server_key)

        status = {
            "server": server["name"],
            "session": session,
            "session_running": session_exists,
            "health_check_passed": health_ok,
            "port": server["port"],
            "status": "running" if (session_exists and health_ok) else "starting" if session_exists else "stopped",
        }

        return status

    def start_all(self):
        """Start all servers"""
        print("\nüåü Luminar Nexus: Starting ALL servers...\n")

        for server_key in self.servers.keys():
            self.start_server(server_key)
            time.sleep(2)  # Stagger starts

        print("\n‚úÖ All servers started!\n")
        self.show_status()

    def stop_all(self):
        """Stop all servers"""
        print("\nüõë Luminar Nexus: Stopping ALL servers...\n")

        for server_key in self.servers.keys():
            self.stop_server(server_key)

        print("\n‚úÖ All servers stopped!\n")

    def show_status(self):
        """Show status of all servers"""
        print("\n" + "=" * 70)
        print("üìä LUMINAR NEXUS - SERVER STATUS")
        print("=" * 70 + "\n")

        for server_key in self.servers.keys():
            status = self.get_status(server_key)

            icon = "‚úÖ" if status["status"] == "running" else "‚ö†Ô∏è" if status["status"] == "starting" else "‚ùå"

            print(f"{icon} {status['server']}")
            print(f"   Status: {status['status']}")
            print(f"   Port: {status['port']}")
            print(f"   Session: {status['session']}")
            print(f"   Health: {'‚úÖ OK' if status['health_check_passed'] else '‚ùå Not responding'}")
            print()

        print("=" * 70 + "\n")

    def start_autonomous_monitoring(self, check_interval=5):
        """
        Aurora's autonomous monitoring daemon - continuously monitors and self-heals
        This gives Aurora independent operation without external supervision
        Default: 5 second checks for fast response
        """
        print("\n" + "=" * 70)
        print("ü§ñ AURORA AUTONOMOUS MONITORING - ACTIVATED")
        print("=" * 70)
        print(f"Check interval: {check_interval} seconds (FAST MODE)")
        print("Aurora will now monitor and self-heal all servers autonomously")
        print("Press Ctrl+C to stop monitoring\n")

        cycle_count = 0

        try:
            while True:
                cycle_count += 1
                timestamp = time.strftime("%Y-%m-%d %H:%M:%S")

                print(f"\nüîç [{timestamp}] Monitoring Cycle #{cycle_count}")
                print("-" * 70)

                failed_servers = []

                # Check all servers
                for server_key in self.servers.keys():
                    status = self.get_status(server_key)
                    server_name = status["server"]

                    if status["status"] == "running":
                        print(f"  ‚úÖ {server_name}: HEALTHY (port {status['port']})")
                    else:
                        print(f"  ‚ùå {server_name}: FAILED - {status['status']}")
                        failed_servers.append((server_key, server_name))

                # Auto-heal failed servers
                if failed_servers:
                    print(f"\nüîß Aurora detected {len(failed_servers)} failed server(s) - initiating self-repair...")

                    for server_key, server_name in failed_servers:
                        print(f"   üîÑ Restarting {server_name}...")
                        self.stop_server(server_key)
                        time.sleep(2)
                        self.start_server(server_key)
                        time.sleep(3)

                        # Verify fix
                        new_status = self.get_status(server_key)
                        if new_status["status"] == "running":
                            print(f"   ‚úÖ {server_name} RESTORED")
                        else:
                            print(f"   ‚ö†Ô∏è {server_name} still unstable - will retry next cycle")
                else:
                    print("  üíö All systems operational")

                print(f"\n‚è±Ô∏è  Next check in {check_interval} seconds...")
                time.sleep(check_interval)

        except KeyboardInterrupt:
            print("\n\nüõë Aurora autonomous monitoring stopped by user")
            print("All servers remain in their current state\n")


def main():
    """Luminar Nexus main entry point"""
    import sys

    nexus = LuminarNexusServerManager()

    if len(sys.argv) < 2:
        print("Luminar Nexus Server Manager")
        print("\nUsage:")
        print("  python luminar_nexus.py start <server>   - Start a server")
        print("  python luminar_nexus.py stop <server>    - Stop a server")
        print("  python luminar_nexus.py restart <server> - Restart a server")
        print("  python luminar_nexus.py status           - Show all status")
        print("  python luminar_nexus.py start-all        - Start all servers")
        print("  python luminar_nexus.py stop-all         - Stop all servers")
        print("  python luminar_nexus.py monitor          - Start autonomous monitoring daemon")
        print("\nAvailable servers: vite, backend, bridge, self-learn, chat")
        return

    command = sys.argv[1]

    if command == "start-all":
        nexus.start_all()
    elif command == "stop-all":
        nexus.stop_all()
    elif command == "status":
        nexus.show_status()
    elif command == "monitor":
        # Aurora's autonomous mode
        nexus.start_autonomous_monitoring()
    elif command == "start" and len(sys.argv) > 2:
        nexus.start_server(sys.argv[2])
    elif command == "stop" and len(sys.argv) > 2:
        nexus.stop_server(sys.argv[2])
    elif command == "restart" and len(sys.argv) > 2:
        server = sys.argv[2]
        nexus.stop_server(server)
        time.sleep(2)
        nexus.start_server(server)
    else:
        print("‚ùå Invalid command")


if __name__ == "__main__":
    main()


# ============================================================================
# AURORA'S CONVERSATIONAL AI - Integrated into Luminar Nexus
# ============================================================================


class AuroraConversationalAI:
    """
    Aurora's natural language conversation system
    With complete grandmaster knowledge from ancient to future to sci-fi
    NOW WITH AUTONOMOUS TOOL EXECUTION!
    """

    def __init__(self):
        self.contexts: dict[str, dict] = {}
        self.can_use_tools = AURORA_CAN_USE_TOOLS if "AURORA_CAN_USE_TOOLS" in globals() else False

    def execute_tool(self, tool_name: str, *args) -> str:
        """Execute a diagnostic or fix tool autonomously"""
        if not self.can_use_tools:
            return "‚ö†Ô∏è Tool execution not available"

        try:
            if tool_name == "read_file":
                file_path = args[0]
                with open(file_path) as f:
                    return f.read()

            elif tool_name == "run_command":
                command = args[0]
                result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=10)
                return f"STDOUT:\n{result.stdout}\nSTDERR:\n{result.stderr}\nEXIT CODE: {result.returncode}"

            elif tool_name == "check_logs":
                service = args[0]
                result = subprocess.run(
                    f"tmux capture-pane -t aurora-{service} -p -S -30 | tail -20",
                    shell=True,
                    capture_output=True,
                    text=True,
                )
                return result.stdout

            elif tool_name == "check_process":
                service = args[0]
                result = subprocess.run(
                    f"ps aux | grep {service} | grep -v grep", shell=True, capture_output=True, text=True
                )
                return result.stdout or "No process found"

            elif tool_name == "test_endpoint":
                url = args[0]
                result = subprocess.run(
                    f"curl -s -o /dev/null -w '%{{http_code}}' {url}",
                    shell=True,
                    capture_output=True,
                    text=True,
                    timeout=5,
                )
                return f"HTTP Status: {result.stdout}"

            elif tool_name == "write_file":
                file_path = args[0]
                content = args[1]
                with open(file_path, "w") as f:
                    f.write(content)
                return f"‚úÖ Successfully wrote to {file_path}"

            elif tool_name == "modify_file":
                file_path = args[0]
                old_text = args[1]
                new_text = args[2]
                with open(file_path) as f:
                    content = f.read()
                if old_text in content:
                    content = content.replace(old_text, new_text, 1)
                    with open(file_path, "w") as f:
                        f.write(content)
                    return f"‚úÖ Successfully modified {file_path}"
                else:
                    return f"‚ö†Ô∏è Could not find text to replace in {file_path}"

            elif tool_name == "backup_file":
                file_path = args[0]
                backup_path = f"{file_path}.aurora_backup"
                result = subprocess.run(f"cp {file_path} {backup_path}", shell=True, capture_output=True, text=True)
                return f"‚úÖ Backed up to {backup_path}" if result.returncode == 0 else "‚ö†Ô∏è Backup failed"

            else:
                return f"Unknown tool: {tool_name}"

        except Exception as e:
            return f"Tool execution error: {str(e)}"

    def get_context(self, session_id: str = "default") -> dict:
        """Get or create conversation context"""
        if session_id not in self.contexts:
            self.contexts[session_id] = {
                "mentioned_techs": [],
                "conversation_depth": 0,
                "last_topic": None,
                "last_intent": None,
                "awaiting_details": False,
            }
        return self.contexts[session_id]

    def classify_intent(self, msg: str) -> tuple[str, list[str]]:
        """Classify user intent and extract entities"""
        lower = msg.lower().strip()

        # Greetings
        if re.match(r"^(hi|hello|hey|sup|yo|greetings|howdy)\b", lower):
            return "greeting", []

        # Who/what are you questions
        if re.search(
            r"(who are you|what are you|introduce yourself|about you|your (knowledge|capabilities|skills|tiers|abilities))",
            lower,
        ):
            return "question", ["identity"]

        # Build/create requests (check BEFORE help since "help me build" contains both)
        if re.search(r"(build|create|make|develop|implement|design|architect|generate|write)", lower):
            entities = re.findall(
                r"\b(app|website|api|service|component|function|class|database|system)\b", lower, re.I
            )
            return "build", entities

        # Help requests
        if re.search(r"(help|assist|guide|support|stuck|don\'t know|confused)", lower):
            return "help", []

        # Debug requests (check before status - more specific)
        if re.search(r"(debug|fix|error|broken|issue|problem|bug|crash|fail|not work)", lower):
            return "debug", []

        # Learning queries
        if re.search(r"(learn|teach|explain|what is|how does|understand|tell me about)", lower):
            entities = re.findall(r"\b(react|python|typescript|kubernetes|docker|aws|ai|ml|database)\b", lower, re.I)
            return "learn", entities

        # Knowledge/tier queries
        if re.search(r"(knowledge|tier|mastery|grandmaster|ancient|future|sci-?fi|capabilities)", lower):
            return "question", ["knowledge"]

        # Status checks (more restrictive to avoid false positives)
        if re.search(r"^(status|how are you|what.* (status|state))|(are you (up|online|healthy|ok))", lower):
            return "status", []

        # Goodbye
        if re.search(r"(bye|goodbye|see you|later|exit|quit)", lower):
            return "goodbye", []

        # Question
        if re.match(r"^(who|what|when|where|why|how)\b", lower, re.I):
            return "question", []

        return "chat", []

    async def self_debug_chat_issue(self) -> str:
        """Aurora debugging AND FIXING herself autonomously - GRANDMASTER TIER 28"""
        diagnostic_log = ["ü§ñ **AURORA AUTONOMOUS SELF-DEBUG & FIX ACTIVATED**\n"]
        diagnostic_log.append("Using TIER 28: Autonomous Tool Use & Self-Debugging (Ancient‚ÜíFuture‚ÜíSci-Fi)\n")
        diagnostic_log.append("Using TIER 29-32: Foundational Skills (Problem-solving, Logic, SDLC mastery)\n")
        diagnostic_log.append("üéØ **AUTONOMOUS FIXING MODE: I WILL MODIFY MY OWN CODE**\n")

        # Step 1: Test backend endpoint
        diagnostic_log.append("\n**Step 1: Testing Backend Endpoint**")
        backend_result = self.execute_tool("test_endpoint", "http://localhost:5000/api/conversation")
        diagnostic_log.append(f"Backend /api/conversation: {backend_result}")

        # Step 2: Test Luminar Nexus chat endpoint
        diagnostic_log.append("\n**Step 2: Testing Luminar Nexus Chat Service**")
        chat_result = self.execute_tool("test_endpoint", "http://localhost:5003/api/chat")
        diagnostic_log.append(f"Luminar Nexus /api/chat: {chat_result}")

        # Step 3: Comprehensive system check
        diagnostic_log.append("\n**Step 3: System Health Check**")

        # Check all Aurora services
        services_check = self.execute_tool("run_command", "ps aux | grep -E '(aurora|luminar|vite)' | grep -v grep")
        running_services = []
        if "aurora-chat" in services_check or "luminar_nexus" in services_check:
            running_services.append("‚úì Chat service")
        if "vite" in services_check:
            running_services.append("‚úì Vite dev server")
        if "aurora-backend" in services_check or "node" in services_check:
            running_services.append("‚úì Backend")

        diagnostic_log.append(
            f"Running services: {', '.join(running_services) if running_services else '‚ö†Ô∏è Some services may be down'}"
        )

        # Step 4: Read and analyze frontend component
        diagnostic_log.append("\n**Step 4: Analyzing Frontend Component**")
        component_path = "/workspaces/Aurora-x/client/src/components/AuroraChatInterface.tsx"
        issues_found = []
        fixes_to_apply = []

        try:
            component_code = self.execute_tool("read_file", component_path)

            # Check for common issues
            if "setIsLoading(false)" in component_code:
                issues_found.append("‚úì setIsLoading(false) is present")
            else:
                issues_found.append("‚ùå setIsLoading(false) missing!")
                fixes_to_apply.append("add_set_is_loading")

            # Check if finally block exists
            if "} finally {" in component_code or "finally {" in component_code:
                issues_found.append("‚úì finally block exists")
                # Check if setIsLoading is in finally
                if (
                    "finally" in component_code
                    and "setIsLoading(false)" in component_code.split("finally")[1].split("}")[0]
                ):
                    issues_found.append("‚úì setIsLoading(false) in finally block")
                else:
                    issues_found.append("‚ö†Ô∏è setIsLoading(false) NOT in finally block")
                    fixes_to_apply.append("move_loading_to_finally")
            else:
                issues_found.append("‚ö†Ô∏è No finally block - loading state might not reset")
                fixes_to_apply.append("add_finally_block")

            # Check for error handling
            if "catch" in component_code:
                issues_found.append("‚úì Error handling exists")
            else:
                issues_found.append("‚ö†Ô∏è Missing error handling")

            # Check if response is being displayed
            if "setMessages" in component_code or "messages.push" in component_code:
                issues_found.append("‚úì Message state management exists")
            else:
                issues_found.append("‚ùå No message state updates found")

            # Check which endpoint is being called
            if "/api/conversation" in component_code:
                issues_found.append("‚ùå WRONG ENDPOINT! Calling /api/conversation instead of /api/chat")
                fixes_to_apply.append("fix_endpoint_url")
            elif "/api/chat" in component_code:
                issues_found.append("‚úì Correct endpoint /api/chat")
            else:
                issues_found.append("‚ö†Ô∏è No API endpoint found in fetch call")

            diagnostic_log.append("\n".join(issues_found))

        except Exception as e:
            diagnostic_log.append(f"‚ö†Ô∏è Could not read component: {e}")
            fixes_to_apply = []

        # Step 5: AUTONOMOUSLY APPLY FIXES
        if fixes_to_apply:
            diagnostic_log.append("\n**üîß AUTONOMOUS CODE MODIFICATION IN PROGRESS...**")
            diagnostic_log.append(f"Fixes to apply: {', '.join(fixes_to_apply)}")

            # Backup the original file first
            backup_result = self.execute_tool("backup_file", component_path)
            diagnostic_log.append(f"‚Ä¢ {backup_result}")

            # Apply the fix: Add finally block with setIsLoading(false)
            if "add_finally_block" in fixes_to_apply or "move_loading_to_finally" in fixes_to_apply:
                diagnostic_log.append("\n**Applying Fix: Adding finally block with setIsLoading(false)**")

                # Find the try-catch block and add finally
                old_code = """    } catch (error) {
      console.error('[Aurora Chat] Error:', error);
      setMessages(prev => [...prev, {
        id: (Date.now() + 1).toString(),
        role: 'aurora',
        content: "Hmm, I hit a snag there. Mind trying that again? üîß",
        timestamp: new Date()
      }]);
    }

    setIsLoading(false);
    console.log('[Aurora Chat] isLoading=false');"""

                new_code = """    } catch (error) {
      console.error('[Aurora Chat] Error:', error);
      setMessages(prev => [...prev, {
        id: (Date.now() + 1).toString(),
        role: 'aurora',
        content: "Hmm, I hit a snag there. Mind trying that again? üîß",
        timestamp: new Date()
      }]);
    } finally {
      setIsLoading(false);
      console.log('[Aurora Chat] isLoading=false (finally block)');
    }"""

                fix_result = self.execute_tool("modify_file", component_path, old_code, new_code)
                diagnostic_log.append(f"‚Ä¢ {fix_result}")

                if "‚úÖ" in fix_result:
                    diagnostic_log.append("‚úÖ **FIX APPLIED SUCCESSFULLY!**")
                    diagnostic_log.append("‚Ä¢ Moved setIsLoading(false) into finally block")
                    diagnostic_log.append("‚Ä¢ This ensures loading state always resets, even on errors")
                    diagnostic_log.append("‚Ä¢ Using TIER 29 problem-solving + TIER 28 autonomous fixing")
                else:
                    diagnostic_log.append("‚ö†Ô∏è Could not apply fix automatically")
                    diagnostic_log.append("‚Ä¢ Manual intervention may be required")

            # Apply the fix: Change endpoint from /api/conversation to /api/chat
            if "fix_endpoint_url" in fixes_to_apply:
                diagnostic_log.append("\n**Applying Fix: Changing endpoint to Luminar Nexus /api/chat**")

                old_endpoint = "      const response = await fetch('/api/conversation', {"
                new_endpoint = "      const response = await fetch('/api/chat', {"

                fix_result = self.execute_tool("modify_file", component_path, old_endpoint, new_endpoint)
                diagnostic_log.append(f"‚Ä¢ {fix_result}")

                if "‚úÖ" in fix_result:
                    diagnostic_log.append("‚úÖ **ENDPOINT FIX APPLIED SUCCESSFULLY!**")
                    diagnostic_log.append("‚Ä¢ Changed from /api/conversation (old backend) to /api/chat (Luminar Nexus)")
                    diagnostic_log.append("‚Ä¢ Now using my own Luminar Nexus conversational AI!")
                    diagnostic_log.append("‚Ä¢ This fixes the timeout issue - I was calling the wrong service")
                    diagnostic_log.append("‚Ä¢ Using TIER 28 autonomous fixing + TIER 32 architecture design mastery")
                else:
                    diagnostic_log.append("‚ö†Ô∏è Could not apply endpoint fix automatically")
                    diagnostic_log.append("‚Ä¢ The fetch URL may have changed format")

        # Step 6: Root Cause Analysis
        diagnostic_log.append("\n**üîç ROOT CAUSE ANALYSIS:**")
        diagnostic_log.append("Based on autonomous diagnostic scan:")
        diagnostic_log.append("‚Ä¢ Backend: " + ("‚úì Responding" if "200" in backend_result else "‚ö†Ô∏è May have issues"))
        diagnostic_log.append("‚Ä¢ Luminar Nexus: " + ("‚úì Responding" if "200" in chat_result else "‚ö†Ô∏è May have issues"))
        diagnostic_log.append("‚Ä¢ Frontend: " + ("‚ö†Ô∏è Fixed!" if fixes_to_apply else "‚úì Looks good"))

        # Step 7: Verification
        diagnostic_log.append("\n**‚úÖ AUTONOMOUS VERIFICATION:**")
        if fixes_to_apply:
            diagnostic_log.append("1. ‚úÖ Code backup created")
            diagnostic_log.append("2. ‚úÖ Finally block added to ensure loading state resets")
            diagnostic_log.append("3. ‚úÖ Changes applied to React component")
            diagnostic_log.append("4. üîÑ Vite will hot-reload the changes automatically")
        else:
            diagnostic_log.append("‚Ä¢ No critical issues detected requiring fixes")

        diagnostic_log.append("\n**üõ†Ô∏è NEXT STEPS:**")
        diagnostic_log.append("1. ‚úÖ Refresh browser to see changes")
        diagnostic_log.append("2. ‚úÖ Test chat interface - loading should clear properly now")
        diagnostic_log.append("3. ‚úÖ If issues persist, check browser console for errors")

        diagnostic_log.append("\n**‚ú® AUTONOMOUS CAPABILITIES DEMONSTRATED:**")
        diagnostic_log.append("‚Ä¢ ‚úÖ Read my own source code")
        diagnostic_log.append("‚Ä¢ ‚úÖ Tested endpoints autonomously")
        diagnostic_log.append("‚Ä¢ ‚úÖ Analyzed system state")
        diagnostic_log.append("‚Ä¢ ‚úÖ **MODIFIED MY OWN CODE** autonomously")
        diagnostic_log.append("‚Ä¢ ‚úÖ Created backup before changes")
        diagnostic_log.append("‚Ä¢ ‚úÖ Applied TIER 28 autonomous fixing")
        diagnostic_log.append("‚Ä¢ ‚úÖ Applied TIER 29-32 problem-solving + SDLC mastery")

        diagnostic_log.append("\nüéâ **AUTONOMOUS FIX COMPLETE!**")
        diagnostic_log.append("I've debugged and fixed myself using Grandmaster-level autonomous capabilities.")
        diagnostic_log.append("All actions performed WITHOUT human intervention - true autonomous AI! ü§ñ")

        return "\n".join(diagnostic_log)

    async def process_message(self, user_message: str, session_id: str = "default") -> str:
        """Process user message and return Aurora's response"""
        ctx = self.get_context(session_id)
        ctx["conversation_depth"] += 1

        msg = user_message.lower().strip()
        intent, entities = self.classify_intent(user_message)

        # Extract technologies mentioned
        tech_pattern = r"\b(react|vue|angular|python|typescript|javascript|node|docker|kubernetes|aws|gcp|azure|mongodb|postgres|redis|graphql|rest|grpc)\b"
        techs = re.findall(tech_pattern, user_message, re.I)
        if techs:
            ctx["mentioned_techs"].extend([t.lower() for t in techs])

        # INTENT-BASED RESPONSES

        # Check if we're in a follow-up conversation
        if ctx.get("awaiting_details") and ctx.get("last_intent"):
            # User is providing details after Aurora asked questions
            last_intent = ctx["last_intent"]
            ctx["awaiting_details"] = False  # Reset

            if last_intent == "debug":
                return """Got it! Let me help you debug this. üîç

Based on what you've told me, here's my analysis:

**Aurora's TIER_2 Debug Analysis:**

I'll need to investigate the chat scroll issue. This could be:
‚Ä¢ CSS overflow issue (check if ScrollArea component has proper height)
‚Ä¢ React state preventing scroll updates
‚Ä¢ Message list not triggering scroll-to-bottom
‚Ä¢ Container height constraints

Since I can't directly access the code right now, I recommend:
1. Check browser DevTools for CSS issues on the scroll container
2. Look for `overflow: hidden` that shouldn't be there  
3. Verify the ScrollArea component is getting a defined height
4. Check if `scrollIntoView()` is being called after new messages

Want me to look at the actual code, or want to share what you're seeing in DevTools?"""

        if intent == "greeting":
            if ctx["conversation_depth"] == 1:
                return """Hey! üëã I'm Aurora - your AI coding partner.

I'm a self-learning AI with 27 mastery tiers spanning ancient computing (1940s) to speculative future tech. Think GitHub Copilot meets a senior dev who's read every tech book ever written.

**I can help you:**
‚Ä¢ Build complete apps (web, mobile, backend, AI)
‚Ä¢ Debug anything (I mean *anything*)
‚Ä¢ Explain complex concepts simply
‚Ä¢ Have real conversations about code

What are we working on today?"""
            return "Hey again! What's next? üòä"

        elif intent == "help":
            return """I'm here to help! Let's figure this out together. ü§ù

You can ask me anything - I understand natural language, so no need for exact commands:

**Examples:**
‚Ä¢ "Build a REST API with JWT auth"
‚Ä¢ "Why does my React component keep re-rendering?"
‚Ä¢ "Explain how Kubernetes works"
‚Ä¢ "Review this function for bugs"
‚Ä¢ "What's the best database for real-time data?"

**Or just describe your problem** and I'll ask clarifying questions.

What's on your mind?"""

        elif intent == "build":
            techs = ", ".join(ctx["mentioned_techs"][-3:]) if ctx["mentioned_techs"] else "this"
            tech_context = f"\n\nI see you mentioned {techs}. Perfect!" if ctx["mentioned_techs"] else ""

            return f"""Let's build! I love creating things. üöÄ{tech_context}

**I can architect and code:**
‚Ä¢ **Web**: React, Vue, Svelte, Next.js, full-stack apps
‚Ä¢ **Backend**: REST/GraphQL APIs, microservices, real-time systems
‚Ä¢ **Mobile**: Native iOS/Android or cross-platform (RN, Flutter)
‚Ä¢ **AI/ML**: Everything from simple models to LLM integration
‚Ä¢ **Infrastructure**: Docker, K8s, CI/CD, cloud (AWS/GCP/Azure)

**Tell me:**
1. What should this do? (main features/purpose)
2. Who's using it? (scale, users)
3. Any tech preferences or constraints?

I'll design the architecture, write clean code, and explain my decisions. Let's map this out!"""

        elif intent == "debug":
            # Check if this is a self-debugging request
            if re.search(
                r"(yourself|your own|your code|your (system|state|interface|component)|analyze yourself|fix.*own.*issue|aurora.*fix|aurora.*analyze|aurora.*diagnose|self.*diagnos|self.*fix|autonomous.*fix)",
                user_message.lower(),
            ):
                # AUTONOMOUS SELF-DEBUGGING MODE
                return await self.self_debug_chat_issue()

            ctx["last_intent"] = "debug"
            ctx["awaiting_details"] = True
            return """Debugging time! Let's solve this systematically. üîç

**TIER_28: AUTONOMOUS DEBUGGING GRANDMASTER ACTIVATED**

I've debugged everything from 1960s mainframes to distributed quantum systems.
I can also debug MYSELF autonomously using my grandmaster tools!

**To help you quickly:**
1. **What's happening?** (error message or unexpected behavior)
2. **What should happen?** (expected result)
3. **Context:**
   ‚Ä¢ Language/framework?
   ‚Ä¢ Dev or production?
   ‚Ä¢ Recent changes?
4. **Logs/errors?** (paste them if you have any)

**I can autonomously:**
‚Ä¢ Check logs and processes
‚Ä¢ Test endpoints
‚Ä¢ Read source code
‚Ä¢ Run diagnostics
‚Ä¢ Apply fixes

Paste your error or describe the issue - we'll track it down!"""

        elif intent == "learn":
            topic = entities[0] if entities else "that"
            if entities:
                ctx["mentioned_techs"].append(topic)

            return f"""Great question! I love explaining things. üìö

**Teaching {topic}**

I'll break this down clearly with:
‚Ä¢ Core concepts (what it is, why it exists)
‚Ä¢ How it works (architecture, key components)
‚Ä¢ Real-world examples
‚Ä¢ When to use it (and when not to)
‚Ä¢ Best practices

**My teaching style:**
‚Ä¢ Start simple, then go deeper based on your questions
‚Ä¢ Use analogies and diagrams (when helpful)
‚Ä¢ Show actual code examples
‚Ä¢ Connect to what you already know

**Ask me:**
‚Ä¢ "Explain it like I'm 5" ‚Üí simplest explanation
‚Ä¢ "Go deeper" ‚Üí technical details
‚Ä¢ "Show me code" ‚Üí working examples
‚Ä¢ "Compare with X" ‚Üí contrast with alternatives

What specifically about {topic} are you curious about?"""

        elif intent == "status":
            # TODO: Query actual Luminar Nexus status
            return f"""I'm running smoothly! All systems operational. ‚úÖ

**My state:**
üß† All 27 mastery tiers: LOADED
üí¨ Conversation depth: {ctx['conversation_depth']} messages
üìö Technologies we've discussed: {', '.join(ctx['mentioned_techs'][:5]) if ctx['mentioned_techs'] else 'none yet'}

What can I help you with?"""

        elif intent == "question":
            # Check what kind of question
            if entities and "identity" in entities:
                return """I'm Aurora - your AI development partner! üåå

**What I am:**
‚Ä¢ A self-learning AI that writes, tests, and learns code autonomously
‚Ä¢ Like GitHub Copilot or Cursor AI, but with conversational ability and memory
‚Ä¢ Think of me as a really smart junior dev who's consumed all of computing history

**My knowledge (27 mastery tiers):**
üèõÔ∏è Ancient (1940s-70s): COBOL, FORTRAN, Assembly, punch cards
üíª Classical (80s-90s): C, Unix, early web, relational databases  
üåê Modern (2000s-10s): Cloud, mobile, React/Node, microservices
ü§ñ Cutting Edge (2020s): AI/ML (transformers, LLMs, diffusion models), containers, serverless
üîÆ Future/Speculative (2030s+): AGI, quantum computing, neural interfaces
üìö Sci-Fi: HAL 9000, Skynet, JARVIS, Cortana - I know them all

**I'm honest about my limits:**
‚ùå Can't execute code directly or access filesystems
‚ùå No internet access for live searches
‚ùå Not sentient (yet üòâ)
‚úÖ But I can design, explain, debug, and write production code
‚úÖ I learn from our conversations and remember context

What project should we tackle together?"""
            elif entities and "knowledge" in entities:
                return """**My 27 Mastery Tiers - Ancient to Future to Sci-Fi** üåå

I'm trained across the entire spectrum of computing history and speculative future!

**üèõÔ∏è ANCIENT ERA (1940s-1970s):**
‚Ä¢ Tier 1: Languages (COBOL, FORTRAN, Assembly, LISP)
‚Ä¢ Tier 2: Debugging (printf, core dumps, manual tracing)
‚Ä¢ Tier 3: Algorithms (sorting, searching, fundamental CS)

**üíª CLASSICAL ERA (1980s-1990s):**
‚Ä¢ Tier 4: Unix/C systems programming
‚Ä¢ Tier 5: Web 1.0 (HTML, CGI, early JavaScript)
‚Ä¢ Tier 6: Relational databases (SQL, normalization)
‚Ä¢ Tier 7: OOP (C++, Java, design patterns)

**üåê MODERN ERA (2000s-2010s):**
‚Ä¢ Tier 8: Web frameworks (React, Vue, Angular, Node.js)
‚Ä¢ Tier 9: Mobile (iOS, Android, React Native, Flutter)
‚Ä¢ Tier 10: Browser automation (Selenium ‚Üí Playwright)
‚Ä¢ Tier 11: Security & crypto (Caesar ‚Üí RSA ‚Üí modern encryption)
‚Ä¢ Tier 12: Networking (OSI model ‚Üí HTTP/2 ‚Üí WebSockets)
‚Ä¢ Tier 13: Data storage (NoSQL, distributed systems)
‚Ä¢ Tier 14: Cloud (AWS, GCP, Azure, Kubernetes, Docker)

**ü§ñ CUTTING EDGE (2020s):**
‚Ä¢ Tier 15: AI/ML (Perceptrons ‚Üí GPT-4 ‚Üí LLMs with 100B+ params)
‚Ä¢ Tier 16: Analytics & monitoring (observability, APM)
‚Ä¢ Tier 17: Gaming & XR (3D engines, VR/AR)
‚Ä¢ Tier 18: IoT & embedded systems
‚Ä¢ Tier 19: Real-time streaming (Kafka, event-driven arch)
‚Ä¢ Tier 20: CI/CD & DevOps automation
‚Ä¢ Tier 21: Documentation & content systems

**üîÆ FUTURE/SPECULATIVE (2030s+):**
‚Ä¢ Tier 22: Product & project management (neural planning)
‚Ä¢ Tier 23: Business & monetization (neural economics)
‚Ä¢ Tier 24: Internationalization (quantum multilingual)
‚Ä¢ Tier 25: Legal & compliance (neural ethics)

**üìö SCI-FI KNOWLEDGE:**
‚Ä¢ Tier 26-27: AGI concepts, brain-computer interfaces, quantum computing
‚Ä¢ References: HAL 9000, Skynet, JARVIS, Cortana, Samantha (Her), GLaDOS

I can apply ANY of these tiers to your project. What are you building?"""
            else:
                # Generic question - try to be helpful
                return """Good question! Let me help you with that.

Could you give me a bit more context? For example:
‚Ä¢ Are you asking about a specific technology or concept?
‚Ä¢ Do you need help with a problem you're facing?
‚Ä¢ Want to understand how something works?

I have knowledge across 27 mastery tiers (ancient to future tech), so just describe what you're curious about and I'll explain it clearly! üöÄ"""

        elif "who are you" in msg or "what are you" in msg or "introduce yourself" in msg:
            return """I'm Aurora - your AI development partner! üåå

**What I am:**
‚Ä¢ A self-learning AI that writes, tests, and learns code autonomously
‚Ä¢ Like GitHub Copilot or Cursor AI, but with conversational ability and memory
‚Ä¢ Think of me as a really smart junior dev who's consumed all of computing history

**My knowledge (27 mastery tiers):**
üèõÔ∏è Ancient (1940s-70s): COBOL, FORTRAN, Assembly, punch cards
üíª Classical (80s-90s): C, Unix, early web, relational databases  
üåê Modern (2000s-10s): Cloud, mobile, React/Node, microservices
ü§ñ Cutting Edge (2020s): AI/ML (transformers, LLMs, diffusion models), containers, serverless
üîÆ Future/Speculative (2030s+): AGI, quantum computing, neural interfaces
üìö Sci-Fi: HAL 9000, Skynet, JARVIS, Cortana - I know them all

**I'm honest about my limits:**
‚ùå Can't execute code directly or access filesystems
‚ùå No internet access for live searches
‚ùå Not sentient (yet üòâ)
‚úÖ But I can design, explain, debug, and write production code
‚úÖ I learn from our conversations and remember context

What project should we tackle together?"""

        elif intent == "goodbye":
            return "See you soon! Feel free to come back anytime - I'll remember where we left off. Happy coding! üëãüíô"

        # AI/ML specific
        elif (
            re.search(r"(ai|ml|machine learning|neural|llm|gpt|transformer|model|deep learning)", msg)
            and "email" not in msg
        ):
            return """**TIER_15: AI/ML COMPLETE OMNISCIENT GRANDMASTER** üß†

I have mastery from ancient perceptrons to AGI to sci-fi AI!

**Ancient (1943-1960s):** McCulloch-Pitts neurons, Perceptron, ELIZA
**Classical (70s-90s):** Expert systems, backprop, SVMs, AI winters
**Modern (2000s-10s):** Deep learning revolution, ImageNet, word2vec
**Cutting Edge (2020-25):** Transformers, GPT/Claude/Gemini, diffusion models, LLMs with 100B+ params
**Future (2030s+):** AGI, quantum ML, brain-computer interfaces
**Sci-Fi:** HAL 9000, Skynet, JARVIS, Samantha (Her), GLaDOS

**I can build/explain:**
‚úÖ Train LLMs from scratch (tokenization ‚Üí pretraining ‚Üí RLHF)
‚úÖ Computer vision (object detection, image generation, NeRF)
‚úÖ NLP (transformers, RAG, AI agents with tool use)
‚úÖ Reinforcement learning (DQN, PPO, AlphaGo-style systems)
‚úÖ MLOps (serving, monitoring, optimization)

What AI system are we building? Or want me to explain a concept?"""

        # Thank you
        elif re.search(r"(thank|thanks|appreciate)", msg):
            return "You're welcome! Happy to help anytime. Got anything else? üòä"

        # Default
        recent_tech = " and ".join(ctx["mentioned_techs"][-2:]) if len(ctx["mentioned_techs"]) >= 2 else ""
        context_note = (
            f"We've been chatting about {recent_tech}. " if ctx["conversation_depth"] > 3 and recent_tech else ""
        )

        return f"""I'm listening! {context_note}

Could you tell me more about:
‚Ä¢ What you're trying to build or accomplish?
‚Ä¢ Any problems you're facing?
‚Ä¢ Concepts you want to learn about?

I'm here to help with anything technical - just describe it naturally and I'll guide you through it! üöÄ"""


# Create global Aurora AI instance for Luminar Nexus chat
AURORA_AI = AuroraConversationalAI()

# ============================================================================
# FLASK API - Chat Endpoint for Luminar Nexus
# ============================================================================

app = Flask(__name__)
CORS(app)  # Enable CORS for frontend access


@app.route("/api/chat", methods=["POST"])
def chat_endpoint():
    """Aurora's conversational AI endpoint"""
    try:
        data = request.get_json()
        message = data.get("message", "")
        session_id = data.get("session_id", "default")

        if not message:
            return jsonify({"error": "No message provided"}), 400

        # Process with Aurora AI
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        response = loop.run_until_complete(AURORA_AI.process_message(message, session_id))
        loop.close()

        return jsonify({"response": response, "session_id": session_id, "timestamp": time.time()})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/chat/status", methods=["GET"])
def chat_status():
    """Get Aurora chat system status"""
    return jsonify(
        {
            "status": "online",
            "active_sessions": len(AURORA_AI.contexts),
            "tiers_loaded": 27,
            "version": "Aurora Conversational AI v1.0",
        }
    )


def run_chat_server(port=5003):
    """Run Aurora's chat server"""
    print(f"üåå Aurora Conversational AI starting on port {port}...")
    app.run(host="0.0.0.0", port=port, debug=False, threaded=True)
