#!/usr/bin/env python3
"""
Aurora + Copilot Ultra-Deep Comprehensive Scan
Scan EVERYTHING - files, scripts, ports, servers, capabilities
Find what exists but isn't being used properly
"""

from aurora_core import AuroraCoreIntelligence
from pathlib import Path
import json
import re
import os

print("=" * 120)
print("üîç ULTRA-DEEP COMPREHENSIVE SCAN - AURORA + COPILOT")
print("=" * 120)

core = AuroraCoreIntelligence()
kt = core.knowledge_tiers

print(f"\n‚ö° Scanning with {kt.total_power} power...")

# ============================================================================
# PHASE 1: SCAN ALL PYTHON FILES FOR UNUSED CAPABILITIES
# ============================================================================

print("\n" + "=" * 120)
print("üìÅ PHASE 1: SCANNING ALL PYTHON FILES FOR ADVANCED FEATURES")
print("=" * 120)

all_py_files = list(Path('.').rglob('*.py'))
print(f"\nüìä Total Python files: {len(all_py_files)}")

# Look for advanced patterns that might not be used
advanced_patterns = {
    "AI/ML Integration": [
        r"import openai",
        r"import anthropic",
        r"from anthropic import",
        r"import torch",
        r"import tensorflow",
        r"import transformers"
    ],
    "Database Systems": [
        r"import sqlite3",
        r"import psycopg2",
        r"import pymongo",
        r"import redis",
        r"import sqlalchemy"
    ],
    "API Servers": [
        r"from fastapi import",
        r"from flask import",
        r"app\.route",
        r"@app\.post",
        r"@app\.get"
    ],
    "WebSocket/Real-time": [
        r"import websocket",
        r"import socketio",
        r"WebSocket",
        r"socket\.io"
    ],
    "Advanced Analytics": [
        r"import pandas",
        r"import numpy",
        r"import matplotlib",
        r"import seaborn"
    ],
    "Async/Concurrent": [
        r"import asyncio",
        r"async def",
        r"await ",
        r"ThreadPoolExecutor",
        r"ProcessPoolExecutor"
    ],
    "Code Analysis": [
        r"import ast",
        r"import pylint",
        r"import mypy",
        r"import black"
    ],
    "Testing Systems": [
        r"import pytest",
        r"import unittest",
        r"from pytest import",
        r"@pytest\."
    ],
    "Logging/Monitoring": [
        r"import logging",
        r"logger\.",
        r"import prometheus",
        r"import grafana"
    ],
    "Security": [
        r"import cryptography",
        r"import jwt",
        r"import bcrypt",
        r"import hashlib"
    ]
}

found_capabilities = {}
files_with_capabilities = {}

print("\nüîç Scanning for advanced capabilities in Python files...")

for category, patterns in advanced_patterns.items():
    found_capabilities[category] = []
    files_with_capabilities[category] = []

    for py_file in all_py_files:
        try:
            if py_file.stat().st_size > 10_000_000:  # Skip files > 10MB
                continue
            content = py_file.read_text(encoding='utf-8', errors='ignore')

            for pattern in patterns:
                if re.search(pattern, content):
                    if py_file not in files_with_capabilities[category]:
                        files_with_capabilities[category].append(py_file)
                        found_capabilities[category].append({
                            'file': str(py_file),
                            'pattern': pattern,
                            'size': py_file.stat().st_size
                        })
        except Exception:
            continue

print("\nüìã FOUND ADVANCED CAPABILITIES:\n")

total_advanced_files = 0
for category, files in files_with_capabilities.items():
    if files:
        total_advanced_files += len(files)
        print(f"‚úÖ {category}: {len(files)} files")
        for f in files[:3]:  # Show first 3
            print(f"   ‚Ä¢ {f.name}")
        if len(files) > 3:
            print(f"   ... and {len(files) - 3} more")

# ============================================================================
# PHASE 2: SCAN FOR UNUSED SERVERS AND PORTS
# ============================================================================

print("\n" + "=" * 120)
print("üåê PHASE 2: SCANNING FOR SERVER CONFIGURATIONS AND PORTS")
print("=" * 120)

server_patterns = {
    "Port Definitions": r"(?:port|PORT)\s*[=:]\s*(\d+)",
    "Server Starts": r"(?:app\.run|server\.listen|listen)\(.*port.*?(\d+)",
    "API Endpoints": r"@(?:app|router)\.(?:get|post|put|delete|patch)\(['\"]([^'\"]+)",
    "WebSocket Endpoints": r"socketio|WebSocket.*?(\d+)",
}

servers_found = {}
ports_found = set()
api_endpoints = []

print("\nüîç Scanning for server configurations...")

for py_file in all_py_files:
    try:
        if py_file.stat().st_size > 10_000_000:
            continue
        content = py_file.read_text(encoding='utf-8', errors='ignore')

        # Find ports
        for match in re.finditer(server_patterns["Port Definitions"], content):
            port = match.group(1)
            if port.isdigit():
                ports_found.add(int(port))
                if py_file not in servers_found:
                    servers_found[str(py_file)] = []
                servers_found[str(py_file)].append(f"Port {port}")

        # Find API endpoints
        for match in re.finditer(server_patterns["API Endpoints"], content):
            endpoint = match.group(1)
            api_endpoints.append({
                'file': str(py_file.name),
                'endpoint': endpoint
            })

    except Exception:
        continue

print(f"\nüìä FOUND SERVER INFRASTRUCTURE:\n")
print(f"   Total Ports Configured: {len(ports_found)}")
print(f"   Ports: {sorted(ports_found)}")
print(f"   Files with Server Config: {len(servers_found)}")
print(f"   API Endpoints: {len(api_endpoints)}")

# ============================================================================
# PHASE 3: SCAN FOR ADVANCED AI/AUTONOMOUS FEATURES
# ============================================================================

print("\n" + "=" * 120)
print("ü§ñ PHASE 3: SCANNING FOR AI/AUTONOMOUS FEATURES")
print("=" * 120)

ai_features = {
    "Self-Learning": [
        r"self.*learn",
        r"improve.*self",
        r"evolution",
        r"self.*improve"
    ],
    "Autonomous Decision": [
        r"autonomous",
        r"auto.*decide",
        r"self.*decide",
        r"strategic.*plan"
    ],
    "Code Generation": [
        r"generate.*code",
        r"code.*generation",
        r"synthesize.*code",
        r"create.*function"
    ],
    "Quality Scoring": [
        r"score.*quality",
        r"quality.*score",
        r"rate.*code",
        r"assess.*quality",
        r"10/10",
        r"rating.*system"
    ],
    "Error Recovery": [
        r"auto.*recover",
        r"self.*heal",
        r"error.*recovery",
        r"auto.*fix"
    ],
    "Task Planning": [
        r"task.*plan",
        r"plan.*execution",
        r"strategy.*execution",
        r"orchestrat"
    ]
}

ai_files = {}

print("\nüîç Scanning for AI/Autonomous features...")

for category, patterns in ai_features.items():
    ai_files[category] = []

    for py_file in all_py_files:
        try:
            if py_file.stat().st_size > 10_000_000:
                continue
            content = py_file.read_text(encoding='utf-8', errors='ignore')

            for pattern in patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    if py_file not in ai_files[category]:
                        ai_files[category].append(py_file)
                    break
        except Exception:
            continue

print("\nüìã FOUND AI/AUTONOMOUS FEATURES:\n")

for category, files in ai_files.items():
    if files:
        print(f"‚úÖ {category}: {len(files)} files")
        for f in files[:3]:
            print(f"   ‚Ä¢ {f.name}")
        if len(files) > 3:
            print(f"   ... and {len(files) - 3} more")

# ============================================================================
# PHASE 4: SCAN FOR UNUSED TOOLS IN tools/ DIRECTORY
# ============================================================================

print("\n" + "=" * 120)
print("üîß PHASE 4: SCANNING tools/ DIRECTORY FOR UNUSED CAPABILITIES")
print("=" * 120)

tools_dir = Path("tools")
if tools_dir.exists():
    tool_files = list(tools_dir.glob("*.py"))
    print(f"\nüìä Total tools: {len(tool_files)}")

    # Check which tools are imported elsewhere
    tools_imported = set()
    tools_not_imported = []

    for tool in tool_files:
        tool_name = tool.stem
        if tool_name == "__init__":
            continue

        # Check if imported in any file
        imported = False
        for py_file in all_py_files:
            try:
                if py_file == tool:
                    continue
                content = py_file.read_text(encoding='utf-8', errors='ignore')
                if f"from tools.{tool_name} import" in content or f"import tools.{tool_name}" in content:
                    imported = True
                    tools_imported.add(tool_name)
                    break
            except Exception:
                continue

        if not imported:
            tools_not_imported.append({
                'name': tool_name,
                'file': str(tool),
                'size': tool.stat().st_size
            })

    print(f"\nüìã TOOL USAGE ANALYSIS:")
    print(f"   Tools Imported: {len(tools_imported)}")
    print(f"   Tools NOT Imported: {len(tools_not_imported)}")

    if tools_not_imported:
        print(f"\n‚ö†Ô∏è  UNUSED TOOLS IN tools/ DIRECTORY:")
        for tool in tools_not_imported[:20]:
            print(f"   ‚Ä¢ {tool['name']}.py ({tool['size']:,} bytes)")

# ============================================================================
# PHASE 5: SCAN FOR CONFIGURATION FILES
# ============================================================================

print("\n" + "=" * 120)
print("‚öôÔ∏è  PHASE 5: SCANNING CONFIGURATION FILES")
print("=" * 120)

config_files = {
    "JSON Configs": list(Path('.').glob('*.json')),
    "YAML Configs": list(Path('.').glob('*.yaml')) + list(Path('.').glob('*.yml')),
    "ENV Files": list(Path('.').glob('.env*')),
    "INI Files": list(Path('.').glob('*.ini')),
    "Config Dirs": list(Path('.').glob('config/*')) if Path('config').exists() else []
}

print("\nüìã CONFIGURATION FILES:\n")

for config_type, files in config_files.items():
    if files:
        print(f"‚úÖ {config_type}: {len(files)} files")
        for f in files[:5]:
            if f.is_file():
                print(f"   ‚Ä¢ {f.name}")

# ============================================================================
# PHASE 6: SCAN FOR FRONTEND COMPONENTS
# ============================================================================

print("\n" + "=" * 120)
print("üé® PHASE 6: SCANNING FRONTEND COMPONENTS")
print("=" * 120)

frontend_files = {
    "TSX Components": list(Path('client/src/components').rglob('*.tsx')) if Path('client/src/components').exists() else [],
    "Pages": list(Path('client/src/pages').rglob('*.tsx')) if Path('client/src/pages').exists() else [],
    "TypeScript": list(Path('client').rglob('*.ts')) if Path('client').exists() else [],
}

print("\nüìã FRONTEND FILES:\n")

for file_type, files in frontend_files.items():
    if files:
        print(f"‚úÖ {file_type}: {len(files)} files")

# Check for unused components
if frontend_files["TSX Components"]:
    print(f"\nüîç Checking component usage...")

    all_tsx = []
    for ftype in frontend_files.values():
        all_tsx.extend(ftype)

    unused_components = []
    for component in frontend_files["TSX Components"]:
        component_name = component.stem

        # Check if imported
        imported = False
        for tsx_file in all_tsx:
            if tsx_file == component:
                continue
            try:
                content = tsx_file.read_text(encoding='utf-8', errors='ignore')
                if component_name in content:
                    imported = True
                    break
            except Exception:
                continue

        if not imported:
            unused_components.append(component_name)

    if unused_components:
        print(f"\n‚ö†Ô∏è  POTENTIALLY UNUSED COMPONENTS:")
        for comp in unused_components[:10]:
            print(f"   ‚Ä¢ {comp}")

# ============================================================================
# FINAL SUMMARY
# ============================================================================

print("\n" + "=" * 120)
print("üìä COMPREHENSIVE SCAN RESULTS")
print("=" * 120)

missing_or_unused = []

# 1. Tracking systems (from previous analysis)
missing_or_unused.append({
    "category": "Tracking & Display Systems",
    "status": "MISSING",
    "items": [
        "aurora_quality_tracker.py - Score tracking database",
        "aurora_code_comparison.py - Before/after comparison",
        "aurora_task_tracker.py - Task completion logger",
        "aurora_evolution_log.py - Evolution tracking",
        "aurora_performance_metrics.py - Real-time metrics"
    ]
})

# 2. Unused tools
if tools_not_imported:
    missing_or_unused.append({
        "category": "Unused Tools in tools/",
        "status": "NOT BEING USED",
        "items": [f"{t['name']}.py ({t['size']:,} bytes)" for t in tools_not_imported[:10]]
    })

# 3. Unused frontend components
if unused_components:
    missing_or_unused.append({
        "category": "Unused Frontend Components",
        "status": "NOT IMPORTED",
        "items": unused_components[:10]
    })

# 4. Advanced capabilities not integrated
missing_or_unused.append({
    "category": "Advanced Capabilities Needing Integration",
    "status": "EXISTS BUT NOT INTEGRATED",
    "items": [
        f"AI/ML files: {len(files_with_capabilities.get('AI/ML Integration', []))} found",
        f"Database systems: {len(files_with_capabilities.get('Database Systems', []))} files",
        f"API servers: {len(files_with_capabilities.get('API Servers', []))} files",
        f"WebSocket/Real-time: {len(files_with_capabilities.get('WebSocket/Real-time', []))} files",
        f"Code Analysis tools: {len(files_with_capabilities.get('Code Analysis', []))} files",
    ]
})

print("\nüéØ MISSING OR UNDERUTILIZED SYSTEMS:\n")

for i, item in enumerate(missing_or_unused, 1):
    print(f"{i}. {item['category']} - {item['status']}")
    print(f"   Items:")
    for sub_item in item['items']:
        print(f"      ‚Ä¢ {sub_item}")
    print()

print("=" * 120)
print("üí° KEY FINDINGS:")
print("=" * 120)

print(f"""
SUMMARY OF WHAT EXISTS BUT ISN'T BEING USED:

1. TRACKING SYSTEMS: 5 critical systems missing (need to be created)
   
2. UNUSED TOOLS: {len(tools_not_imported)} tools exist in tools/ but are never imported
   - These are advanced capabilities that aren't integrated
   
3. FRONTEND COMPONENTS: {len(unused_components) if unused_components else 0} components not being used
   
4. ADVANCED FEATURES PRESENT:
   - {len(files_with_capabilities.get('AI/ML Integration', []))} files with AI/ML capabilities
   - {len(files_with_capabilities.get('Database Systems', []))} files with database systems
   - {len(files_with_capabilities.get('Async/Concurrent', []))} files with async capabilities
   - {len(ai_files.get('Quality Scoring', []))} files with quality scoring
   
5. SERVER INFRASTRUCTURE:
   - {len(ports_found)} ports configured
   - {len(servers_found)} server files
   - {len(api_endpoints)} API endpoints defined

CONCLUSION:
The intelligence exists. The capabilities exist. But they're not connected.
Need to:
1. Create the 5 missing tracking systems
2. Integrate the {len(tools_not_imported)} unused tools
3. Connect frontend to backend properly
4. Activate dormant AI capabilities
""")

print("=" * 120)

# Save detailed report
report = {
    "scan_date": "2025-11-22",
    "total_python_files": len(all_py_files),
    "advanced_capabilities": {k: len(v) for k, v in files_with_capabilities.items()},
    "ai_features": {k: len(v) for k, v in ai_files.items()},
    "ports_configured": sorted(list(ports_found)),
    "api_endpoints_count": len(api_endpoints),
    "unused_tools_count": len(tools_not_imported),
    "unused_components_count": len(unused_components) if unused_components else 0,
    "missing_systems": [item for item in missing_or_unused]
}

with open("AURORA_COMPREHENSIVE_SCAN_REPORT.json", "w") as f:
    json.dump(report, f, indent=2, default=str)

print("\n‚úÖ Detailed report saved to: AURORA_COMPREHENSIVE_SCAN_REPORT.json")
