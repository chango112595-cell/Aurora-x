#!/usr/bin/env python3
"""Aurora-X main module with corpus integration and learning weights."""

from __future__ import annotations

import argparse
import json
import random
import subprocess
import sys
import time
from datetime import datetime
from glob import glob
from pathlib import Path
from typing import Any

from .corpus.pretty import filter_rows, fmt_rows, to_json
from .corpus.store import record as corpus_record
from .corpus.store import retrieve as corpus_retrieve
from .corpus.store import spec_digest
from .learn import AdaptiveBiasScheduler, AdaptiveConfig, get_seed_store
from .learn import weights as learn
from .spec.parser_v2 import parse
from .synthesis.search import synthesize

# Global adaptive scheduler for API access
_global_adaptive_scheduler: AdaptiveBiasScheduler | None = None

# Progress tracking constants
PROGRESS_JSON_DEFAULT = Path(__file__).resolve().parents[1] / "progress.json"
UPDATE_SCRIPT_DEFAULT = Path(__file__).resolve().parents[1] / "tools" / "update_progress.py"
HIST_DIR = Path(__file__).resolve().parents[1] / ".progress_history"

def iso_now() -> str:
    """Get current ISO timestamp."""
    return datetime.now().isoformat()

def fmt_duration(seconds: float) -> str:
    """Format seconds into human-readable 'Xh Ym Zs' format."""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)

    parts = []
    if hours > 0:
        parts.append(f"{hours}h")
    if minutes > 0:
        parts.append(f"{minutes}m")
    parts.append(f"{secs}s")

    return " ".join(parts)

def diff_graphs(old: dict[str, list], new: dict[str, list]) -> dict[str, Any]:
    """Compute differences between two call graphs.

    Args:
        old: Old graph as adjacency list (node -> list of connected nodes)
        new: New graph as adjacency list (node -> list of connected nodes)

    Returns:
        Dictionary with:
        - "added": sorted list of added edge tuples
        - "removed": sorted list of removed edge tuples
        - "old_edges": count of edges in old graph
        - "new_edges": count of edges in new graph
    """
    # Extract edges from both graphs as sets of tuples
    old_edges = set()
    for u, neighbors in old.items():
        for v in neighbors:
            old_edges.add((u, v))

    new_edges = set()
    for u, neighbors in new.items():
        for v in neighbors:
            new_edges.add((u, v))

    # Compute added and removed edges
    added = new_edges - old_edges
    removed = old_edges - new_edges

    # Return result dictionary
    return {
        "added": sorted(list(added)),
        "removed": sorted(list(removed)),
        "old_edges": len(old_edges),
        "new_edges": len(new_edges)
    }

def load_scores_map(run_root: Path) -> dict[str, dict[str, Any]]:
    """Load function scores from logs/scores.jsonl.

    Args:
        run_root: Root directory of the run

    Returns:
        Dict mapping function names to their latest scores:
        {'passed': int, 'total': int, 'iter': int}
    """
    scores_file = run_root / "logs" / "scores.jsonl"
    if not scores_file.exists():
        return {}

    scores_map = {}
    try:
        with open(scores_file) as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                entry = json.loads(line)
                func_name = entry.get("function")
                if not func_name:
                    continue

                # Track latest score per function (highest iteration)
                curr_iter = entry.get("iter", 0)
                if func_name not in scores_map or curr_iter > scores_map[func_name].get("iter", 0):
                    scores_map[func_name] = {
                        "passed": entry.get("passed", 0),
                        "total": entry.get("total", 0),
                        "iter": curr_iter
                    }
    except Exception:
        # Return empty dict on any error
        return {}

    return scores_map

def diff_scores(old: dict[str, dict[str, Any]], new: dict[str, dict[str, Any]]) -> dict[str, Any]:
    """Compute per-function score differences.

    Args:
        old: Old score map from load_scores_map
        new: New score map from load_scores_map

    Returns:
        Dict with:
        - "summary": {"regressions": count, "improvements": count, "count": total_functions}
        - "rows": list of dicts with "function", "old" [passed, total], "new" [passed, total], "delta_passed"
    """
    all_funcs = set(old.keys()) | set(new.keys())

    rows = []
    regressions = 0
    improvements = 0

    for func in sorted(all_funcs):
        old_score = old.get(func, {"passed": 0, "total": 0})
        new_score = new.get(func, {"passed": 0, "total": 0})

        old_passed = old_score.get("passed", 0)
        old_total = old_score.get("total", 0)
        new_passed = new_score.get("passed", 0)
        new_total = new_score.get("total", 0)

        delta_passed = new_passed - old_passed

        if delta_passed < 0:
            regressions += 1
        elif delta_passed > 0:
            improvements += 1

        rows.append({
            "function": func,
            "old": [old_passed, old_total],
            "new": [new_passed, new_total],
            "delta_passed": delta_passed
        })

    return {
        "summary": {
            "regressions": regressions,
            "improvements": improvements,
            "count": len(all_funcs)
        },
        "rows": rows
    }

# Stub imports for synthesis modules (to be implemented)
class Repo:
    @staticmethod
    def create(outdir):
        r = Repo()
        if outdir:
            # Create timestamped run directory under outdir
            import time
            timestamp = time.strftime("%Y%m%d-%H%M%S")
            r.root = outdir / f"run-{timestamp}"
            r.root.mkdir(parents=True, exist_ok=True)
        else:
            r.root = Path("/tmp/aurora-tmp")
            r.root.mkdir(parents=True, exist_ok=True)
        return r
    def __init__(self):
        self.root = Path(".")
    def path(self, p):
        return self.root / p
    def set_hash(self, p, c):
        pass
    def list_files(self):
        return [str(p.relative_to(self.root)) for p in self.root.rglob("*") if p.is_file()]

class Sandbox:
    def __init__(self, root, timeout_s): pass

class Spec:
    def __init__(self): self.functions = []

def parse_spec(text): return Spec()
def write_file(p, c):
    Path(p).parent.mkdir(parents=True, exist_ok=True)
    Path(p).write_text(c)

def run_spec(path: str):
    """Run spec compilation for the given spec file."""
    sp = Path(path)
    if not sp.exists():
        print(f"[ERR] Spec not found: {sp}")
        sys.exit(1)
    md = sp.read_text(encoding='utf-8')
    spec = parse(md)
    out = synthesize(spec, Path('runs'))
    print(f"[OK] Generated: {out}")
    print(f"Source: {out/'src'}")
    print(f"Tests: {out/'tests'}")
    print(f"Report: {out/'report.html'}")
    print(f"Run tests: python -m unittest discover -s {out/'tests'} -t {out}")

def main():
    """Main entry point for Aurora-X."""
    ap = argparse.ArgumentParser(description="AURORA-X Ultra (Offline)")

    # Mutually exclusive: spec (for spec compilation), spec-text, spec-file, dump-corpus, show-bias, progress-print, or nl (natural language)
    g = ap.add_mutually_exclusive_group(required=True)
    g.add_argument("--spec", type=str, help="Path to spec markdown to compile → code")
    g.add_argument("--spec-text", type=str, help="Inline spec text (Markdown DSL)")
    g.add_argument("--spec-file", type=str, help="Path to spec file (legacy synthesis)")
    g.add_argument("--nl", type=str, help="Natural language instruction to generate a spec")
    g.add_argument("--dump-corpus", type=str, help="Signature to query corpus instead of running synthesis")
    g.add_argument("--show-bias", action="store_true", help="Print current seed_bias and exit")
    g.add_argument("--progress-print", action="store_true", help="Print computed progress and exit")

    # Corpus dump options
    ap.add_argument("--top", type=int, default=10, help="How many corpus entries to print with --dump-corpus")
    ap.add_argument("--json", action="store_true", help="Emit JSON for --dump-corpus")
    ap.add_argument("--grep", type=str, default=None, help="Filter results by substring for --dump-corpus")

    # Synthesis options
    ap.add_argument("--seed", type=int, default=1337, help="Random seed")
    ap.add_argument("--outdir", type=str, default="./runs", help="Output directory")
    ap.add_argument("--no-seed", action="store_true", help="Disable seeding from corpus")
    ap.add_argument("--baseline", type=str, default=None, help="Path to baseline run dir for report diffs (default: runs/latest)")
    ap.add_argument("--seed-bias", type=float, default=None, help="Override learned seed bias [0.0..0.5]")
    ap.add_argument("--max-iters", type=int, default=100, help="Maximum synthesis iterations")
    ap.add_argument("--beam", type=int, default=20, help="Beam search width")
    ap.add_argument("--timeout", type=int, default=5, help="Timeout in seconds")

    # RNG config
    ap.add_argument("--temp", type=float, default=0.9, help="Temperature")
    ap.add_argument("--top-k", type=int, default=50, help="Top-K sampling")
    ap.add_argument("--top-p", type=float, default=0.95, help="Top-P (nucleus) sampling")

    # Progress tracking options
    ap.add_argument("--update-task", action="append", default=None, help="ID=NN or ID=auto (repeatable)")
    ap.add_argument("--bump", action="append", default=None, help="ID=+/-Δ (repeatable)")

    args = ap.parse_args()

    outdir = Path(args.outdir).resolve() if args.outdir else None
    rng_cfg = {
        "temperature": args.temp,
        "top_k": args.top_k,
        "top_p": args.top_p
    }

    # ----- Natural language mode -----
    if args.nl:
        # Check if this is a Flask request
        from aurora_x.spec.parser_nl import parse_english
        parsed = parse_english(args.nl)

        if parsed.get("framework") == "flask":
            # Handle Flask app synthesis
            # Import by adding tools to path
            import sys
            tools_dir = Path(__file__).parent.parent / "tools"
            sys.path.insert(0, str(tools_dir))
            from datetime import datetime

            from spec_from_flask import create_flask_app_from_text
            run_name = f"run-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
            run_dir = Path("runs") / run_name
            app_file = create_flask_app_from_text(args.nl, run_dir)
            print(f"[OK] Flask app generated at: {app_file}")

            # Update the latest symlink
            latest = Path("runs/latest")
            if latest.exists() or latest.is_symlink():
                latest.unlink()
            latest.symlink_to(run_dir.name)

            # Also print out the location for the server to read
            print(f"[Aurora-X] Latest valid run: {run_name}")
            print(f"[Aurora-X] Read generated code from: {app_file.name}")
            return 0
        else:
            # Regular function synthesis
            import sys
            tools_dir = Path(__file__).parent.parent / "tools"
            sys.path.insert(0, str(tools_dir))
            from spec_from_text import create_spec_from_text
            spec_path = create_spec_from_text(args.nl)
            print(f"[OK] Spec generated from English at: {spec_path}")
            # Now compile the generated spec
            comp = Path("tools/spec_compile_v3.py")
            if comp.exists():
                import os
                import subprocess
                subprocess.check_call([sys.executable, "tools/spec_compile_v3.py", str(spec_path)], env=os.environ.copy())
            else:
                print("No v3 compiler found (tools/spec_compile_v3.py). Add v3 pack first.")
        return 0

    # ----- Spec compilation mode -----
    if args.spec:
        run_spec(args.spec)
        return 0

    # ----- Progress print mode (no synthesis) -----
    if args.progress_print:
        data = load_progress() or {}
        print(json.dumps(data, indent=2))
        return 0

    # ----- Handle progress updates (can be used with synthesis) -----
    if args.update_task or args.bump:
        updates: dict[str, str|float] = {}
        if args.update_task:
            for item in args.update_task:
                if "=" not in item:
                    print(f"[invalid] {item}")
                    continue
                k, v = item.split("=", 1)
                v = v.strip()
                try:
                    updates[k.strip()] = float(v)
                except ValueError:
                    updates[k.strip()] = v

        done = []
        if updates:
            done += update_progress_ids(updates)

        if args.bump:
            for item in args.bump:
                if "=" not in item or item[0] == "=":
                    print(f"[invalid bump] {item}")
                    continue
                k, v = item.split("=", 1)
                k = k.strip()
                v = v.strip()
                if v[0] in "+-" and v[1:].isdigit():
                    d = float(v)
                    r = bump_progress_id(k, d)
                    if r:
                        done.append(f"{r}{v}")

        if done:
            print("[AURORA-X] updated:", ", ".join(done))

        # If no synthesis requested, exit after updates
        if not (args.spec_text or args.spec_file):
            return 0

    # ----- Show bias mode (no synthesis) -----
    if args.show_bias:
        run_root = outdir / "latest"
        weights_file = run_root / "learn_weights.json"
        if weights_file.exists():
            try:
                weights = json.loads(weights_file.read_text())
                sb = float(weights.get("seed_bias", 0.0))
                print(f"[AURORA-X] Current seed_bias: {sb:.2f}")
            except Exception as e:
                print(f"[AURORA-X] Error reading seed_bias: {e}")
        else:
            print("[AURORA-X] No weights found yet. Run a synthesis first.")
        return 0

    # ----- Corpus dump mode (no synthesis) -----
    if args.dump_corpus:
        run_root = outdir / "run-dump"
        run_root.mkdir(parents=True, exist_ok=True)
        rows = corpus_retrieve(run_root, args.dump_corpus, k=max(1, args.top))
        rows = filter_rows(rows, args.grep)
        print(to_json(rows) if args.json else fmt_rows(rows))
        return 0

    # ----- Synthesis mode -----
    spec_text = args.spec_text if args.spec_text else Path(args.spec_file).read_text()

    ax = AuroraX(
        seed=args.seed,
        max_iters=args.max_iters,
        beam=args.beam,
        timeout_s=args.timeout,
        outdir=outdir,
        rng_cfg=rng_cfg,
        disable_seed=args.no_seed,
        seed_bias_override=args.seed_bias,
        baseline=Path(args.baseline).resolve() if args.baseline else None
    )
    repo, ok = ax.run(spec_text)

    # Show seed_bias snapshot at the end of the run for quick glance
    try:
        weights = learn.load(repo.root)
        sb = float(weights.get("seed_bias", 0.0))
    except Exception:
        sb = 0.0

    print(f"[AURORA-X] Repo: {repo.root}")
    print(f"[AURORA-X] Status: {'PASS' if ok else 'INCOMPLETE'}")
    print(f"[AURORA-X] seed_bias: {sb:.2f}")
    print("[AURORA-X] Files:")
    for f in repo.list_files():
        print(" -", f)
    print(f"\nOpen HTML report: file://{repo.path('report.html')}")

    return 0 if ok else 1

class AuroraX:
    def __init__(self, seed: int, max_iters: int, beam: int, timeout_s: int, outdir: Path | None,
                 rng_cfg: dict[str, Any], disable_seed: bool = False, seed_bias_override: float | None = None,
                 baseline: Path | None = None):
        random.seed(seed)
        self._start_time = time.time()
        self.repo = Repo.create(outdir)
        self.sandbox = Sandbox(self.repo.root, timeout_s=timeout_s)
        self.beam = beam
        self.max_iters = max_iters
        self.rng_cfg = rng_cfg
        self.disable_seed = disable_seed
        self.baseline = baseline
        self.weights = learn.load(self.repo.root)
        if seed_bias_override is not None:
            self.weights["seed_bias"] = max(0.0, min(0.5, float(seed_bias_override)))

        # Initialize persistent seed store
        self.seed_store = get_seed_store()

        # Initialize adaptive scheduler
        self.adaptive_scheduler = self._attach_adaptive_scheduler()

        # Set global reference for API access
        global _global_adaptive_scheduler
        _global_adaptive_scheduler = self.adaptive_scheduler

    def run(self, spec_text: str):
        """Main orchestration loop."""
        start_ts = iso_now()  # Capture start timestamp in ISO format
        spec = parse_spec(spec_text)
        best_map: dict[str,str] = {}

        for _idx, f in enumerate(spec.functions):
            # Gather seed snippets from corpus
            seed_snippets: list[str] = []
            sig = f"{f.name}({', '.join(a+': '+t for a,t in f.args)}) -> {f.returns}"

            if not self.disable_seed:
                # Get seed bias for this function
                seed_key = self.seed_store.make_seed_key(sig, spec_text[:100])
                self.seed_store.get_bias(seed_key)

                # Apply bias to candidate enumeration
                candidates = []
                for row in corpus_retrieve(self.repo.root, sig, k=min(12, self.beam//4)):
                    seed_snippets.append(row["snippet"])
                    candidates.append(seed_key)

                # Use adaptive scheduler to choose candidate
                if candidates and self.adaptive_scheduler:
                    self.adaptive_scheduler.choose(candidates)
                    self.adaptive_scheduler.tick()

            # Synthesize (stub - would call actual synthesis)
            cand = type('obj', (object,), {'src': 'def stub(): pass'})()

            # Record to corpus
            corpus_entry = {
                "func_name": f.name,
                "func_signature": sig,
                "passed": 1,
                "total": 1,
                "score": 1.0,
                "snippet": cand.src,
                **spec_digest(spec_text)
            }
            corpus_record(self.repo.root, corpus_entry)

            best_map[f.name] = cand.src

            # Update seed store with result
            if not self.disable_seed:
                success = corpus_entry["passed"] == corpus_entry["total"]
                result = {
                    "seed_key": seed_key,
                    "score": corpus_entry["score"],
                    "success": success
                }
                self.seed_store.update(result)

                # Update adaptive scheduler
                if self.adaptive_scheduler:
                    self.adaptive_scheduler.reward(seed_key, success, magnitude=corpus_entry["score"])

            # Learning nudge (keep legacy for backward compat)
            won_with_seed = _seed_won(cand.src, seed_snippets)
            self.weights["seed_bias"] = learn.update_seed_bias(
                float(self.weights.get("seed_bias", 0.0)),
                won_with_seed
            )
            learn.save(self.repo.root, self.weights)

        # Save persistent seed store at end of loop
        self.seed_store.save()

        # Build and save module
        module_src = self.build_module(spec, best_map)
        write_file(self.repo.path("src/app.py"), module_src)
        self.repo.set_hash("src/app.py", module_src)

        # Save run config
        cfg = {
            "seed": random.getstate()[1][0],
            "max_iters": self.max_iters,
            "beam": self.beam,
            **self.rng_cfg,
            "weights": self.weights
        }
        self.save_run_config(cfg)

        # Capture end time and save run metadata
        self._end_time = time.time()
        duration_seconds = round(self._end_time - self._start_time, 3)
        run_metadata = {
            "start_ts": start_ts,
            "end_ts": iso_now(),
            "duration_seconds": duration_seconds
        }
        write_file(self.repo.path("run_meta.json"), json.dumps(run_metadata, indent=2))

        # Generate HTML report (before symlink update so it can detect previous latest run)
        write_html_report(self.repo, spec, baseline=self.baseline)

        # Update 'latest' symlink to this run (after HTML report generation)
        try:
            latest_link = self.repo.root.parent / "latest"
            if latest_link.exists() or latest_link.is_symlink():
                latest_link.unlink()
            latest_link.symlink_to(self.repo.root.resolve())
            print(f"[AURORA-X] Updated symlink: {latest_link} → {self.repo.root.name}")
        except Exception as e:
            print(f"[AURORA-X] (nonfatal) failed to update 'latest' symlink: {e}")

        return self.repo, True

    def build_module(self, spec, best_map):
        """Build final module source."""
        return "\n\n".join(best_map.values())

    def synthesize_best(self, f, callees_meta, base_prefix):
        """Stub for synthesis - returns mock candidate."""
        return type('obj', (object,), {'src': f'def {f.name}(): pass'})()

    def save_run_config(self, cfg: dict[str, Any]) -> None:
        write_file(self.repo.path("run_config.json"), json.dumps(cfg, indent=2))

    def _attach_adaptive_scheduler(self) -> AdaptiveBiasScheduler:
        """Attach adaptive bias scheduler to the engine."""
        cfg = AdaptiveConfig(epsilon=0.15, decay=0.98, cooldown_iters=5, top_k=10)
        sched = AdaptiveBiasScheduler(cfg)
        try:
            sched.load(self.seed_store.get_biases())
        except Exception:
            pass
        return sched

def load_progress() -> dict | None:
    """Load progress.json if it exists."""
    try:
        if PROGRESS_JSON_DEFAULT.exists():
            return json.loads(PROGRESS_JSON_DEFAULT.read_text(encoding="utf-8"))
    except Exception:
        return None
    return None

def save_progress(obj: dict) -> None:
    """Save progress.json."""
    PROGRESS_JSON_DEFAULT.write_text(json.dumps(obj, indent=2), encoding="utf-8")

def run_update_script() -> None:
    """Run update_progress.py if it exists."""
    if UPDATE_SCRIPT_DEFAULT.exists():
        try:
            subprocess.run([sys.executable, str(UPDATE_SCRIPT_DEFAULT)], check=False)
        except Exception:
            pass

def update_progress_ids(id_to_pct: dict[str, str|float]) -> list[str]:
    """Update progress percentages for given IDs. Handles 'auto' values."""
    data = load_progress()
    if not data:
        return []
    updated: list[str] = []

    for ph in data.get("phases", []):
        for t in ph.get("tasks", []):
            tid = str(t.get("id"))
            subs = t.get("subtasks") or []

            # Update subtasks
            for s in subs:
                sid = str(s.get("id"))
                if sid in id_to_pct and str(id_to_pct[sid]).lower() != "auto":
                    try:
                        s["progress"] = float(id_to_pct[sid])
                        updated.append(sid)
                    except Exception:
                        pass

            # Update task
            if tid in id_to_pct:
                val = id_to_pct[tid]
                if isinstance(val, str) and val.lower() == "auto":
                    if subs:
                        avg = sum(float(x.get("progress", 0)) for x in subs) / max(1, len(subs))
                        t["progress"] = avg
                        updated.append(f"{tid}=auto({int(round(avg))}%)")
                    else:
                        updated.append(f"{tid}=auto(n/a)")
                else:
                    try:
                        t["progress"] = float(val)
                        updated.append(tid)
                    except Exception:
                        pass

    data["last_updated"] = datetime.utcnow().strftime("%Y-%m-%d")
    save_progress(data)
    run_update_script()

    # Create history snapshot
    try:
        HIST_DIR.mkdir(exist_ok=True)
        ts = datetime.utcnow().strftime("%Y%m%d-%H%M%S")
        (HIST_DIR / f"progress-{ts}.json").write_text(json.dumps(data, indent=2), encoding="utf-8")
    except Exception:
        pass

    return updated

def bump_progress_id(id_: str, delta: float) -> str | None:
    """Bump progress by delta for given ID."""
    data = load_progress()
    if not data:
        return None
    done = None

    for ph in data.get("phases", []):
        for t in ph.get("tasks", []):
            if t.get("id") == id_ and not t.get("subtasks"):
                t["progress"] = max(0.0, min(100.0, float(t.get("progress", 0) or 0) + delta))
                done = id_
            for s in t.get("subtasks", []):
                if s.get("id") == id_:
                    s["progress"] = max(0.0, min(100.0, float(s.get("progress", 0) or 0) + delta))
                    done = id_

    if done:
        data["last_updated"] = datetime.utcnow().strftime("%Y-%m-%d")
        save_progress(data)
        run_update_script()
        return done
    return None

def _recent_runs(parent: Path, limit: int = 12) -> list[Path]:
    """Get recent run directories."""
    runs = sorted([Path(p) for p in glob(str(parent / "run-*"))], reverse=True)
    return runs[:limit]

def _run_pass_count(run_dir: Path) -> int | None:
    """Count passed tests from scores.jsonl."""
    fp = run_dir / "logs" / "scores.jsonl"
    if not fp.exists():
        return None

    latest: dict[str, int] = {}
    iters: dict[str, int] = {}

    for line in fp.read_text(encoding="utf-8").splitlines():
        try:
            o = json.loads(line)
            fn = o.get("function")
            it = int(o.get("iter", -1))
            if fn is None:
                continue
            if fn not in iters or it >= iters[fn]:
                iters[fn] = it
                latest[fn] = int(o.get("passed", 0))
        except Exception:
            pass

    return sum(latest.values()) if latest else None

def render_floating_hud(repo_root: Path) -> str:
    """Generate floating HUD HTML with SVG chart."""
    parent = repo_root.parent
    pts = []
    for r in reversed(_recent_runs(parent, limit=12)):
        v = _run_pass_count(r)
        pts.append(0 if v is None else int(v))

    if not pts:
        pts = [0]

    mx = max(pts) or 1
    w, h = 160, 36
    n = len(pts)
    xs = [i * (w / (max(1, n - 1))) for i in range(n)]
    ys = [h - (p / mx) * (h - 6) - 3 for p in pts]

    if n > 1:
        path = " ".join(f"L{xs[i]:.1f},{ys[i]:.1f}" for i in range(1, n))
        d = f"M{xs[0]:.1f},{ys[0]:.1f} {path}"
    else:
        d = f"M0,{ys[0]:.1f} L{w},{ys[0]:.1f}"

    return f"""
<!-- Aurora HUD -->
<div id="aurora-hud" style="position:fixed; top:16px; right:16px; z-index:9999; font-family:system-ui">
  <div style="background:#111;color:#fff;padding:8px 10px;border-radius:8px;opacity:0.85; transition:opacity .2s, transform .2s;"
       onmouseover="this.style.opacity=1; this.style.transform='scale(1.03)'"
       onmouseout="this.style.opacity=.85; this.style.transform='scale(1.0)'">
    <div style="display:flex; align-items:center; gap:8px; justify-content:space-between;">
      <strong>Aurora HUD</strong><span style="font-size:12px;opacity:.8">(hover)</span>
    </div>
    <div style="margin-top:6px;background:#222;padding:6px;border-radius:6px">
      <svg viewBox="0 0 {w} {h}" width="{w}" height="{h}" role="img" aria-label="recent pass counts">
        <path d="{d}" fill="none" stroke="#10b981" stroke-width="2"/>
      </svg>
    </div>
    <div style="font-size:12px;opacity:.85;margin-top:6px;">Recent runs (newest → right)</div>
    <div style="margin-top:8px;border-top:1px solid #333;padding-top:6px;">
      <form id="aurora-edit" onsubmit="return window._auroraSubmit(event)">
        <div style="display:flex;gap:6px;align-items:center;">
          <input id="aurora-id" placeholder="ID (e.g., T02f)" style="width:88px;border-radius:4px;border:1px solid #444;background:#000;color:#fff;padding:4px 6px">
          <input id="aurora-val" placeholder="NN or auto or +5" style="width:120px;border-radius:4px;border:1px solid #444;background:#000;color:#fff;padding:4px 6px">
          <button type="submit" style="border:0;background:#10b981;color:#000;padding:4px 8px;border-radius:4px;font-weight:700;cursor:pointer">Update</button>
        </div>
        <div id="aurora-hint" style="margin-top:4px;font-size:12px;opacity:.85;"></div>
      </form>
      <script>
      (function(){{
        const hint = document.getElementById('aurora-hint');
        const served = location.protocol.startsWith('http');
        if(!served){{
          hint.innerText = "To edit: run `aurorax-serve --run-dir <RUN_DIR>` and open http://localhost:8000";
          document.getElementById('aurora-id').disabled = true;
          document.getElementById('aurora-val').disabled = true;
        }} else {{
          hint.innerText = "POST /_aurora/update (ID=NN|auto|+/-Δ)";
        }}
        window._auroraSubmit = async (ev) => {{
          ev.preventDefault();
          if(!location.protocol.startsWith('http')) return false;
          const id = document.getElementById('aurora-id').value.trim();
          const val = document.getElementById('aurora-val').value.trim();
          if(!id || !val) {{ hint.innerText = "Provide ID and value"; return false; }}
          const body = {{updates:{{}}}};
          // allow +5/-3 bump
          if((val.startsWith('+')||val.startsWith('-')) && !isNaN(Number(val.substring(1)))){{
            body.bump = {{[id]: Number(val)}};
          }} else {{
            body.updates[id] = isNaN(Number(val)) ? val : Number(val);
          }}
          try{{
            const res = await fetch('/_aurora/update', {{method:'POST', headers:{{'Content-Type':'application/json'}}, body: JSON.stringify(body)}});
            const j = await res.json();
            hint.innerText = (j.updated && j.updated.length ? "Updated: " + j.updated.join(", ") : "No changes");
          }}catch(e){{ hint.innerText = "Error: " + e; }}
          return false;
        }};
      }})();
      </script>
    </div>
  </div>
</div>
"""

def render_progress_sidebar_html() -> str:
    """Generate progress sidebar HTML."""
    data = load_progress()
    if not data:
        return ""

    def task_pct(t):
        subs = t.get("subtasks") or []
        return (sum(float(s.get("progress", 0)) for s in subs) / len(subs)) if subs else float(t.get("progress", 0) or 0)

    def phase_pct(ph):
        pairs = [(task_pct(t), max(1, len(t.get("subtasks") or []))) for t in ph.get("tasks", [])]
        num = sum(v * w for v, w in pairs)
        den = sum(w for _, w in pairs) or 1
        return num / den

    def overall(phases):
        pairs = [(phase_pct(ph), max(1, len(ph.get("tasks") or []))) for ph in phases]
        num = sum(v * w for v, w in pairs)
        den = sum(w for _, w in pairs) or 1
        return num / den

    ov = overall(data.get("phases", []))
    rows = []
    for ph in data.get("phases", []):
        pct = int(round(phase_pct(ph)))
        rows.append(f'<div style="margin-bottom:4px">{ph.get("id")} {ph.get("name")}: <strong>{pct}%</strong></div>')

    return f"""
<aside style="position:sticky; top:16px; padding:12px; border:1px solid #e5e7eb; border-radius:8px; background:#fafafa; max-width:360px;">
  <div style="display:flex;justify-content:space-between;align-items:center;margin-bottom:8px;">
    <h3 style="margin:0;font-size:16px;">Project Progress</h3>
    <span style="font-weight:700;">{int(round(ov))}%</span>
  </div>
  <div style="display:flex;flex-direction:column;gap:6px;">{''.join(rows)}</div>
  <div style="margin-top:8px;"><a href="../MASTER_TASK_LIST.md">Open MASTER_TASK_LIST.md</a></div>
</aside>
"""

def write_html_report(repo: Repo, spec: Spec, baseline: Path | None = None) -> None:
    """Generate HTML report with latest run status."""
    # Read report markdown if exists
    report_path = repo.path("AURORA_REPORT.md")
    if report_path.exists():
        md = report_path.read_text()
    else:
        md = "# Aurora-X Synthesis Report\n\nRun completed."
        write_file(report_path, md)

    cfg = json.loads(repo.path("run_config.json").read_text())

    # Load run metadata for timestamp and duration
    run_meta_path = repo.path("run_meta.json")
    start_ts = None
    duration_seconds = None
    if run_meta_path.exists():
        try:
            run_meta = json.loads(run_meta_path.read_text())
            start_ts = run_meta.get("start_ts")
            duration_seconds = run_meta.get("duration_seconds")
        except Exception:
            pass

    # Call graph (stub for now - would load from call_graph.json in real implementation)
    graph = {"nodes": [f.name for f in spec.functions], "edges": {}}

    # Save call graph for future comparisons
    call_graph_path = repo.path("call_graph.json")
    write_file(call_graph_path, json.dumps(graph, indent=2))

    # weights + bias snapshot
    weights_path = repo.path("learn_weights.json")
    try:
        weights = json.loads(weights_path.read_text()) if weights_path.exists() else {}
    except Exception:
        weights = {}
    seed_bias = float(weights.get("seed_bias", 0.0))

    # latest symlink status
    latest_link = repo.root.parent / "latest"
    try:
        is_latest = latest_link.exists() and latest_link.resolve() == repo.root.resolve()
    except Exception:
        is_latest = False

    # Choose baseline: use baseline if provided, else use latest_link if it exists
    base_root = None
    if baseline:
        base_root = baseline
    elif latest_link.exists():
        try:
            base_root = latest_link.resolve()
        except Exception:
            base_root = None

    # Initialize regressions count
    regressions_count = 0

    # If base_root exists and is different from current run, generate graph comparison
    graph_diff_generated = False
    if base_root and base_root != repo.root.resolve():
        try:
            base_graph_path = Path(base_root) / "call_graph.json"
            if base_graph_path.exists():
                base_graph = json.loads(base_graph_path.read_text())
                # Compute diff using diff_graphs function
                diff = diff_graphs(
                    base_graph.get("edges", {}),
                    graph.get("edges", {})
                )

                # Save graph_diff.json
                graph_diff_path = repo.path("graph_diff.json")
                write_file(graph_diff_path, json.dumps(diff, indent=2))

                # Generate graph_diff.html
                diff_html = f"""<!doctype html><html><head><meta charset="utf-8"><title>Graph Comparison</title>
<style>
  body{{font-family:system-ui,Segoe UI,Roboto,sans-serif;margin:24px}}
  .added{{color:#16a34a;font-weight:600}}
  .removed{{color:#dc2626;font-weight:600}}
  pre{{background:#f6f8fa;padding:12px;overflow:auto;border-radius:6px}}
  h2{{color:#1e293b}}
  .stats{{background:#f0f9ff;padding:12px;border-radius:6px;margin:16px 0}}
</style>
</head><body>
<h1>Graph Comparison with Baseline</h1>
<div class="stats">
  <b>Statistics:</b><br>
  Previous edges: {diff['old_edges']}<br>
  Current edges: {diff['new_edges']}<br>
  Added: {len(diff['added'])}<br>
  Removed: {len(diff['removed'])}
</div>

<h2 class="added">Added Edges ({len(diff['added'])})</h2>
<pre>{json.dumps(diff['added'], indent=2) if diff['added'] else 'None'}</pre>

<h2 class="removed">Removed Edges ({len(diff['removed'])})</h2>
<pre>{json.dumps(diff['removed'], indent=2) if diff['removed'] else 'None'}</pre>

<h2>Full Diff Data</h2>
<pre>{json.dumps(diff, indent=2)}</pre>

<p><a href="report.html">← Back to Report</a></p>
</body></html>"""
                write_file(repo.path("graph_diff.html"), diff_html)
                graph_diff_generated = True
        except Exception as e:
            print(f"[AURORA-X] Could not generate graph diff: {e}")

    # Scores regression comparison
    scores_diff_generated = False
    scores_html = ""
    if base_root:
        try:
            # Load scores from both runs
            base_scores = load_scores_map(Path(base_root))
            current_scores = load_scores_map(repo.root)

            # Only generate diff if we have scores from at least one run
            if base_scores or current_scores:
                # Compute diff
                scores_diff = diff_scores(base_scores, current_scores)

                # Save scores_diff.json
                scores_diff_path = repo.path("scores_diff.json")
                write_file(scores_diff_path, json.dumps(scores_diff, indent=2))

                # Generate HTML table for scores regression
                summary = scores_diff["summary"]
                rows = scores_diff["rows"]

                # Extract regressions count from the diff summary
                regressions_count = summary.get("regressions", 0)

                # Build regression table rows
                table_rows = []
                for row in rows:
                    func = row["function"]
                    old_p, old_t = row["old"]
                    new_p, new_t = row["new"]
                    delta = row["delta_passed"]

                    # Color coding for delta
                    if delta < 0:
                        delta_color = "#dc2626"  # red for regression
                        delta_text = f"{delta:+d}"
                    elif delta > 0:
                        delta_color = "#16a34a"  # green for improvement
                        delta_text = f"{delta:+d}"
                    else:
                        delta_color = "#6b7280"  # gray for no change
                        delta_text = "0"

                    table_rows.append(
                        f'<tr>'
                        f'<td>{func}</td>'
                        f'<td>{old_p}/{old_t}</td>'
                        f'<td>{new_p}/{new_t}</td>'
                        f'<td style="color:{delta_color};font-weight:600">{delta_text}</td>'
                        f'</tr>'
                    )

                # Generate scores_diff.html standalone page
                scores_diff_html = f"""<!doctype html><html><head><meta charset="utf-8"><title>Scores Regression</title>
<style>
  body{{font-family:system-ui,Segoe UI,Roboto,sans-serif;margin:24px}}
  .summary{{background:#f0f9ff;padding:16px;border-radius:8px;margin:20px 0}}
  .summary-bar{{display:flex;gap:20px;margin-top:12px}}
  .stat{{padding:8px 12px;border-radius:6px;font-weight:600}}
  .regression{{background:#fee2e2;color:#991b1b}}
  .improvement{{background:#dcfce7;color:#14532d}}
  .neutral{{background:#f3f4f6;color:#374151}}
  table{{width:100%;border-collapse:collapse;margin:20px 0}}
  th{{background:#f8fafc;text-align:left;padding:12px;border-bottom:2px solid #e5e7eb}}
  td{{padding:10px 12px;border-bottom:1px solid #e5e7eb}}
  tr:hover{{background:#f8fafc}}
  h2{{color:#1e293b}}
  a{{color:#0969da;text-decoration:none}}
  a:hover{{text-decoration:underline}}
</style>
</head><body>
<h1>Scores Regression Analysis</h1>

<div class="summary">
  <h3 style="margin:0 0 12px 0">Summary</h3>
  <div class="summary-bar">
    <span class="stat regression">Regressions: {summary['regressions']}</span>
    <span class="stat improvement">Improvements: {summary['improvements']}</span>
    <span class="stat neutral">Total Functions: {summary['count']}</span>
  </div>
</div>

<h2>Function Scores Comparison</h2>
<table>
  <thead>
    <tr>
      <th>Function</th>
      <th>Old (pass/total)</th>
      <th>New (pass/total)</th>
      <th>Δ passed</th>
    </tr>
  </thead>
  <tbody>
    {''.join(table_rows)}
  </tbody>
</table>

<h2>Raw Data</h2>
<pre>{json.dumps(scores_diff, indent=2)}</pre>

<p><a href="report.html">← Back to Report</a></p>
</body></html>"""
                write_file(repo.path("scores_diff.html"), scores_diff_html)
                scores_diff_generated = True

                # Build HTML section for main report
                scores_html = f"""
<h3>Scores Regression</h3>
<div style="background:#f0f9ff;padding:16px;border-radius:8px;margin:12px 0">
  <div style="display:flex;gap:20px">
    <span style="padding:8px 12px;border-radius:6px;background:#fee2e2;color:#991b1b;font-weight:600">
      Regressions: {summary['regressions']}
    </span>
    <span style="padding:8px 12px;border-radius:6px;background:#dcfce7;color:#14532d;font-weight:600">
      Improvements: {summary['improvements']}
    </span>
    <span style="padding:8px 12px;border-radius:6px;background:#f3f4f6;color:#374151;font-weight:600">
      Total: {summary['count']}
    </span>
  </div>
</div>
<table style="width:100%;border-collapse:collapse;margin:12px 0">
  <thead>
    <tr style="background:#f8fafc">
      <th style="text-align:left;padding:12px;border-bottom:2px solid #e5e7eb">Function</th>
      <th style="text-align:left;padding:12px;border-bottom:2px solid #e5e7eb">Old (pass/total)</th>
      <th style="text-align:left;padding:12px;border-bottom:2px solid #e5e7eb">New (pass/total)</th>
      <th style="text-align:left;padding:12px;border-bottom:2px solid #e5e7eb">Δ passed</th>
    </tr>
  </thead>
  <tbody>
    {''.join(['<tr style="border-bottom:1px solid #e5e7eb">' + row.replace('<td', '<td style="padding:10px 12px"') + '</tr>' for row in [r.replace('<tr>', '').replace('</tr>', '') for r in table_rows]])}
  </tbody>
</table>
"""
        except Exception as e:
            print(f"[AURORA-X] Could not generate scores diff: {e}")

    latest_badge = (
        '<span style="display:inline-block;padding:4px 8px;border-radius:6px;background:#16a34a;color:#fff;font-weight:600;">LATEST RUN ✓</span>'
        if is_latest else
        '<span style="display:inline-block;padding:4px 8px;border-radius:6px;background:#f59e0b;color:#111;font-weight:600;">NOT LATEST</span> '
        '<a href="../latest/report.html" style="margin-left:8px;">Open Latest Report →</a>'
    )

    # Add regression badge
    if regressions_count > 0:
        reg_badge = f'<span style="display:inline-block;padding:4px 8px;border-radius:6px;background:#dc2626;color:#fff;font-weight:600;">REGRESSIONS ⚠ {regressions_count}</span>'
    elif base_root:
        reg_badge = '<span style="display:inline-block;padding:4px 8px;border-radius:6px;background:#16a34a;color:#fff;font-weight:600;">No regressions</span>'
    else:
        reg_badge = ""

    # Build timestamp/duration banner
    meta_parts = []
    if start_ts:
        meta_parts.append(f"<b>Started:</b> {start_ts}")
    if duration_seconds is not None:
        meta_parts.append(f"<b>Duration:</b> {fmt_duration(duration_seconds)}")
    meta_html = " | ".join(meta_parts) if meta_parts else ""

    # quick links if present
    corpus_jsonl = repo.root / "corpus.jsonl"
    corpus_db = repo.root / "corpus.db"
    links = []
    if corpus_jsonl.exists():
        links.append('<a href="corpus.jsonl">corpus.jsonl</a>')
    if corpus_db.exists():
        links.append('<a href="corpus.db">corpus.db</a>')
    if weights_path.exists():
        links.append('<a href="learn_weights.json">learn_weights.json</a>')
    if graph_diff_generated:
        links.append('<a href="graph_diff.html">Compare with baseline (diff)</a>')
    if scores_diff_generated:
        links.append('<a href="scores_diff.html">Scores regression</a>')
    links_html = " | ".join(links) if links else "No corpus files yet"

    # Generate HUD and sidebar
    hud_html = render_floating_hud(repo.root)
    sidebar_html = render_progress_sidebar_html()

    body = f"""<!doctype html><html><head><meta charset="utf-8"><title>AURORA-X Report</title>
<style>
  body{{font-family:system-ui,Segoe UI,Roboto,sans-serif;margin:24px}}
  pre,code{{background:#f6f8fa;padding:12px;overflow:auto;border-radius:6px}}
  .hdr{{display:flex;align-items:center;gap:12px;margin-bottom:8px}}
  .sub{{color:#555;margin:0 0 4px 0}}
  .meta{{color:#666;margin:0 0 16px 0;font-size:14px}}
  h3{{color:#1e293b;margin-top:24px}}
  a{{color:#0969da;text-decoration:none}}
  a:hover{{text-decoration:underline}}
  .layout{{display:grid;grid-template-columns:1fr 380px;gap:24px}}
  main{{min-width:0}}
</style>
</head><body>
{hud_html}
<div class="layout">
<main>
<div class="hdr">
  <h1 style="margin:0;">AURORA-X Ultra</h1>
  {latest_badge}
  {reg_badge}
</div>
<p class="sub"><b>Run:</b> {repo.root}</p>
{f'<p class="meta">{meta_html}</p>' if meta_html else ''}

<h3>Learning</h3>
<pre>{json.dumps({"seed_bias": round(seed_bias, 4), "weights_file": str(weights_path.name if weights_path.exists() else "none")}, indent=2)}</pre>

<h3>Quick Links</h3>
<p>{links_html}</p>

<h3>Config</h3>
<pre>{json.dumps(cfg, indent=2)}</pre>

<h3>Call Graph</h3>
<pre>{json.dumps(graph, indent=2)}</pre>

{scores_html}

<h3>Report</h3>
<pre>{md}</pre>
</main>
{sidebar_html}
</div>
</body></html>"""
    write_file(repo.path("report.html"), body)

def _seed_won(final_src: str, seeds: list[str]) -> bool:
    """Check if winning code matches any seed (whitespace-insensitive)."""
    def norm(s: str) -> str:
        return "".join(s.split())
    n_final = norm(final_src)
    for s in seeds:
        try:
            if norm(s) == n_final:
                return True
        except Exception:
            continue
    return False

if __name__ == "__main__":
    sys.exit(main())
