#!/usr/bin/env python3
"""
Adaptive Learning Engine for Aurora-X
Implements epsilon-greedy exploration with decay and cooldown
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, List, Tuple
import math
import random

# Import production config if available
try:
    from aurora_x.prod_config import CFG
    _use_prod_config = True
except ImportError:
    _use_prod_config = False

@dataclass
class BiasStat:
    value: float = 0.0
    wins: int = 0
    losses: int = 0
    last_used_iter: int = -1

@dataclass
class AdaptiveConfig:
    # Use production config if available, else defaults
    epsilon: float = CFG.EPSILON if _use_prod_config else 0.15
    decay: float = CFG.DECAY if _use_prod_config else 0.98
    cooldown_iters: int = CFG.COOLDOWN_ITERS if _use_prod_config else 5
    max_drift_per_iter: float = CFG.MAX_DRIFT if _use_prod_config else 0.10
    top_k: int = CFG.TOP_K if _use_prod_config else 10
    seed: int = 42

class AdaptiveBiasScheduler:
    """Adaptive scheduler mixing exploitation and ε-greedy exploration."""
    def __init__(self, config: AdaptiveConfig | None = None):
        self.cfg = config or AdaptiveConfig()
        self.rng = random.Random(self.cfg.seed)
        self.iteration = 0
        self.stats: Dict[str, BiasStat] = {}
        self.history: List[Tuple[int, str, float]] = []  # (iter, key, value)

    def load(self, payload: Dict[str, float] | None):
        if not payload: return
        for k, v in payload.items():
            self.stats.setdefault(k, BiasStat()).value = float(v)

    def dump(self) -> Dict[str, float]:
        return {k: round(v.value, 6) for k, v in self.stats.items()}

    def tick(self):
        self.iteration += 1
        for k, st in self.stats.items():
            st.value *= self.cfg.decay
        if len(self.stats) > self.cfg.top_k * 2:
            top = sorted(self.stats.items(), key=lambda kv: abs(kv[1].value), reverse=True)[: self.cfg.top_k]
            self.stats = dict(top)

    def choose(self, candidates: List[str]) -> str:
        if not candidates: return ""
        if self.rng.random() < self.cfg.epsilon:
            return self.rng.choice(candidates)
        best_key, best_val = "", -math.inf
        for k in candidates:
            v = self.stats.get(k, BiasStat()).value
            if v > best_val and (self.iteration - self.stats.get(k, BiasStat()).last_used_iter) >= self.cfg.cooldown_iters:
                best_key, best_val = k, v
        return best_key or self.rng.choice(candidates)

    def reward(self, key: str, success: bool, magnitude: float = 1.0):
        if not key: return
        st = self.stats.setdefault(key, BiasStat())
        st.last_used_iter = self.iteration
        delta = min(self.cfg.max_drift_per_iter, magnitude * 0.1)
        if success:
            st.wins += 1
            st.value += delta
        else:
            st.losses += 1
            st.value -= delta
        self.history.append((self.iteration, key, st.value))

    def summary(self) -> Dict[str, float]:
        return {k: round(v.value, 4) for k, v in sorted(self.stats.items(), key=lambda kv: -abs(kv[1].value))[: self.cfg.top_k]}

    def sparkline(self, key: str, width: int = 24) -> str:
        vals = [v for (it, k, v) in self.history if k == key]
        if not vals: return ""
        mn, mx = min(vals), max(vals)
        span = max(1e-9, mx - mn)
        blocks = '▁▂▃▄▅▆▇█'
        out = []
        for v in vals[-width:]:
            idx = int((v - mn) / span * (len(blocks) - 1))
            out.append(blocks[idx])
        return ''.join(out)